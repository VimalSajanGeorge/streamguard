# StreamGuard Training - Quick Start Guide

## Overview

This guide helps you train the StreamGuard vulnerability detection model with the latest stability improvements and features.

**What's New (Latest Updates):**
- âœ… WeightedRandomSampler for balanced batches (fixes collapse)
- âœ… Hardened collapse detection (2 consecutive epochs)
- âœ… Discriminative learning rates (encoder vs head)
- âœ… Code features extraction (+5-10 F1 boost)
- âœ… Focal Loss for hard negatives
- âœ… Safe LR with caps and auto-tuning
- âœ… CSV logging with metrics tracking
- ğŸ†• **LR Finder** - Auto-detect optimal learning rate
- ğŸ†• **LR Safety & Caching** - Validation, fallback, and intelligent caching
- ğŸ†• **Triple Weighting Auto-Adjustment** - Prevents overcorrection
- ğŸ†• **TensorBoard** - Real-time visualization
- ğŸ†• **Auto-plotting** - Generate training curves automatically
- ğŸ†• **Ablation Testing** - Systematic weighting strategy comparison
- ğŸ†• **Enhanced Checkpoint Metadata** - Full reproducibility tracking

---

## Quick Test (30 minutes - Verify Fixes Work)

**Recommended command for testing stability improvements:**

```bash
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --quick-test \
  --epochs 10 \
  --batch-size 8 \
  --use-weighted-sampler \
  --weight-multiplier 1.5 \
  --seed 42
```

**Expected Results:**
- âœ… **No collapse:** Vulnerable predictions > 0 all epochs
- âœ… **Epoch 1:** F1 > 0.3 (not 0.0!)
- âœ… **Epoch 5:** F1 > 0.6
- âœ… **Epoch 10:** F1 > 0.7
- âœ… **Balanced predictions:** ~40-60% each class
- âœ… **Stable training:** No wild oscillations

---

## Full Training (Recommended - 2-3 hours on T4)

**Production-ready training with all improvements:**

```bash
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --test-data data/processed/codexglue/test.jsonl \
  --epochs 10 \
  --batch-size 32 \
  --use-weighted-sampler \
  --use-code-features \
  --weight-multiplier 1.2 \
  --seed 42
```

**Expected Results:**
- âœ… **Val F1 > 0.75** after 10 epochs
- âœ… **Test F1 > 0.73**
- âœ… **No collapse** throughout training
- âœ… **Code features** provide +5-10 point boost

---

## Maximum Accuracy (3-4 hours)

**Use when you need the highest possible F1:**

```bash
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --test-data data/processed/codexglue/test.jsonl \
  --epochs 15 \
  --batch-size 32 \
  --use-weighted-sampler \
  --use-code-features \
  --focal-loss \
  --weight-multiplier 1.5 \
  --seed 42
```

**Expected Results:**
- âœ… **Val F1 > 0.80**
- âœ… **Focal Loss** helps with hard negatives
- âœ… **Best performance** on imbalanced data

---

### 4. Ablation Testing Guide

**What is Ablation Testing?**

Ablation testing systematically compares different weighting strategy combinations to find the optimal configuration for your dataset.

**Provided Script:**

The `training/test_ablations.py` script automatically tests 7 combinations:

| Name | Sampler | Weight Multiplier | Focal Loss |
|------|---------|-------------------|------------|
| baseline | âŒ | 1.0 | âŒ |
| sampler_only | âœ… | 1.0 | âŒ |
| weights_only | âŒ | 1.5 | âŒ |
| focal_only | âŒ | 1.0 | âœ… |
| sampler_weights | âœ… | 1.5 | âŒ |
| sampler_focal | âœ… | 1.0 | âœ… |
| **all_three** | âœ… | 1.5 â†’ 1.2 (auto) | âœ… (Î³ clamped) |

**How to run:**

```bash
# Run full ablation test (70-100 minutes on T4)
python training/test_ablations.py

# Results saved to: ablation_results.csv
```

**What it does:**
1. Runs 10-epoch training for each combination
2. Records F1, precision, recall, accuracy
3. Tracks prediction balance (vulnerable vs safe ratio)
4. Identifies best configuration
5. Saves comprehensive CSV report

**Example Output:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          StreamGuard Ablation Test: Weighting Strategies         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Testing 7 combinations of:
  - WeightedRandomSampler (balances batches)
  - Class weights (loss weighting)
  - Focal Loss (hard example focus)

This will take approximately 70-100 minutes (10 min Ã— 7 runs).

[1/7] Testing: baseline
  Config: Sampler=False, Weight=1.0, Focal=False
  [+] Complete: F1=0.6234, Precision=0.5821, Recall=0.6712

[2/7] Testing: sampler_only
  Config: Sampler=True, Weight=1.0, Focal=False
  [+] Complete: F1=0.7145, Precision=0.6934, Recall=0.7389

[7/7] Testing: all_three
  [!] Triple weighting detected - auto-adjusting...
  Config: Sampler=True, Weight=1.5â†’1.2, Focal=True (Î³=1.5)
  [+] Complete: F1=0.7823, Precision=0.7612, Recall=0.8054

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ABLATION TEST RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                name  sampler  weight_mult  focal  best_epoch      f1  accuracy  precision  recall  pred_vuln_ratio  actual_vuln_ratio  final_lr
            baseline    False          1.0  False           8  0.6234    0.6512     0.5821  0.6712           0.4523             0.5400  2.00e-05
        sampler_only     True          1.0  False           9  0.7145    0.7234     0.6934  0.7389           0.5134             0.5400  2.00e-05
        weights_only    False          1.5  False           7  0.6823    0.6912     0.6512  0.7156           0.5823             0.5400  2.00e-05
          focal_only    False          1.0   True           9  0.7034    0.7156     0.6734  0.7367           0.5256             0.5400  2.00e-05
     sampler_weights     True          1.5  False          10  0.7456    0.7567     0.7234  0.7712           0.5512             0.5400  2.00e-05
       sampler_focal     True          1.0   True          10  0.7623    0.7689     0.7401  0.7867           0.5489             0.5400  2.00e-05
           all_three     True          1.2   True          10  0.7823    0.7845     0.7612  0.8054           0.5523             0.5400  2.00e-05

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BEST CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Name: all_three
  Config: Sampler=True, Weight=1.2, Focal=True
  F1: 0.7823
  Precision: 0.7612
  Recall: 0.8054
  Pred vulnerable ratio: 55.23%
  Actual vulnerable ratio: 54.00%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Triple weighting (auto-adjusted):
    F1: 0.7823
    vs Baseline: +25.5%

  Prediction balance:
    âœ“ all_three          : 55.23% (actual: 54.00%)
    âœ“ sampler_focal      : 54.89% (actual: 54.00%)
    âœ“ sampler_weights    : 55.12% (actual: 54.00%)
    âœ“ sampler_only       : 51.34% (actual: 54.00%)
    âœ“ focal_only         : 52.56% (actual: 54.00%)
    ! weights_only       : 58.23% (actual: 54.00%)
    ! baseline           : 45.23% (actual: 54.00%)

[+] Ablation test complete!
```

**Interpreting Results:**

1. **Best F1**: Which config achieved highest F1?
2. **Prediction Balance**: How close is `pred_vuln_ratio` to `actual_vuln_ratio`?
   - Difference < 10% is good (âœ“)
   - Difference > 10% indicates imbalance (!)
3. **Triple Weighting**: Did auto-adjustment prevent overcorrection?

**When to use ablation testing:**

- **New dataset**: Test which weighting works best
- **Verify fixes**: Confirm triple weighting auto-adjustment helps
- **Hyperparameter tuning**: Find optimal weight_multiplier
- **Research**: Compare methods systematically

**Tips:**

- Use `--quick-test` in the script for faster testing (reduces epochs to 5)
- Results are dataset-dependent - what works for one may not work for another
- Look for both high F1 AND good balance

---

## Troubleshooting

### Issue 1: Still seeing collapse (0% vulnerable predictions)

**Symptoms:**
```
Epoch 1: Predictions: Vulnerable=0/54, Safe=100/46
Epoch 2: Predictions: Vulnerable=0/54, Safe=100/46
[!] COLLAPSE SIGNAL: Zero vulnerable predictions
```

**Solutions (try in order):**

1. âœ… **Add WeightedRandomSampler** (most important!)
   ```bash
   --use-weighted-sampler
   ```

2. âœ… **Increase LR**
   ```bash
   --lr-override 2e-5
   ```

3. âœ… **Stronger class weights**
   ```bash
   --weight-multiplier 2.0
   ```

4. âœ… **Try Focal Loss**
   ```bash
   --focal-loss
   ```

**Check the diagnostic report:**
- File: `models/transformer/collapse_report.json`
- Contains metrics, recommendations, and hyperparameters

---

### Issue 2: Low F1 (<0.6 after 10 epochs)

**Solutions:**

1. âœ… **Add code features** (+5-10 points)
   ```bash
   --use-code-features
   ```

2. âœ… **Train longer**
   ```bash
   --epochs 15
   ```

3. âœ… **Larger batch size** (if memory allows)
   ```bash
   --batch-size 64
   ```

4. âœ… **Check LR**
   ```bash
   --lr-override 2e-5
   ```

---

### Issue 3: Out of memory

**Solutions:**

1. âœ… **Reduce batch size**
   ```bash
   --batch-size 8
   ```

2. âœ… **Use gradient accumulation**
   ```bash
   --batch-size 8 --accumulation-steps 4
   # Effective batch = 8 * 4 = 32
   ```

3. âœ… **Disable mixed precision** (if causing issues)
   ```bash
   --no-mixed-precision
   ```

---

### Issue 4: LR Finder suggests unreliable learning rate

**Symptoms:**
```
[*] LR Finder Results:
    Raw suggestion: 2.30e-03
    Confidence: low
    Slope magnitude: 0.0012
    SNR: 0.65
    Final LR: 1.00e-05 (fallback_due_to_noisy_curve)
[!] WARNING: Used conservative fallback (1e-5)
```

**This is normal!** The system detected a poor-quality LR curve and safely fell back to 1e-5.

**Why it happens:**
- Dataset too small for reliable LR range test
- Loss curve is flat/noisy (no clear descent)
- Loss diverges early in the test
- Training data has high variance

**Solutions:**

1. âœ… **Use the fallback LR** (1e-5 is safe, proven default)
   ```bash
   # Just proceed with training - fallback LR works well
   python training/train_transformer.py --find-lr ...
   # System already applied safe 1e-5
   ```

2. âœ… **Override with known-good LR**
   ```bash
   --lr-override 1.5e-5
   # Skips LR Finder validation, uses your value
   ```

3. âœ… **Increase LR Finder iterations** (more data points)
   ```bash
   --find-lr --lr-finder-iterations 200
   # Default is 100, try 200 for smoother curve
   ```

4. âœ… **Use larger training subset**
   ```bash
   # If using --quick-test, try without it
   # More data â†’ better LR curve
   ```

**When to worry:**
- If fallback LR causes collapse â†’ Use `--lr-override 2e-5`
- If training is too slow â†’ Try `--lr-override 3e-5`

---

### Issue 5: LR cache won't invalidate

**Symptoms:**
```
[*] Loading cached LR Finder results...
    Cached LR: 1.50e-04
    Age: 180.5 hours (max: 168 hours)
[!] Cache expired, running fresh LR Finder...
```

Or: You changed data but cache still used.

**Solutions:**

1. âœ… **Force refresh**
   ```bash
   --find-lr --force-find-lr
   # Ignores cache, always runs fresh
   ```

2. âœ… **Adjust cache expiry**
   ```bash
   --find-lr --lr-cache-max-age 24
   # Cache valid for 24 hours only
   ```

3. âœ… **Manually delete cache**
   ```bash
   # Linux/Mac
   rm -rf ~/.cache/streamguard/lr_finder/

   # Windows
   rd /s /q %USERPROFILE%\.cache\streamguard\lr_finder
   ```

**How cache invalidation works:**
- Automatically expires after `--lr-cache-max-age` hours (default: 168)
- Cache key includes dataset mtime + size (auto-detects changes)
- Cache key includes model name, batch size, config

---

### Issue 6: Triple weighting still causing overcorrection

**Symptoms:**
```
Epoch 10:
Predictions: Vulnerable=95/54, Safe=5/46
Val Precision: 0.45 (too low!)
Val Recall: 0.98 (too high!)
```

**This means auto-adjustment wasn't enough.**

**Solutions:**

1. âœ… **Disable one weighting method**
   ```bash
   # Try sampler + weights (no focal)
   --use-weighted-sampler --weight-multiplier 1.2

   # Or sampler + focal (no extra weights)
   --use-weighted-sampler --focal-loss
   ```

2. âœ… **Use lower weight multiplier**
   ```bash
   --use-weighted-sampler --weight-multiplier 1.1 --focal-loss
   # Auto-adjusts: 1.1 â†’ 0.88 (effectively turns off class weights)
   ```

3. âœ… **Run ablation test**
   ```bash
   python training/test_ablations.py
   # Find optimal combination for your dataset
   ```

**Expected behavior after auto-adjustment:**
```
Predictions: Vulnerable=55/54, Safe=45/46
Val Precision: 0.76 (good!)
Val Recall: 0.81 (good!)
```

---

### Issue 7: Training too slow

**Solutions:**

1. âœ… **Disable code features for quick test**
   ```bash
   # Don't use --use-code-features
   ```

2. âœ… **Smaller batch** (ironically faster on some GPUs)
   ```bash
   --batch-size 16
   ```

3. âœ… **Enable mixed precision** (if disabled)
   ```bash
   --mixed-precision
   ```

---

## New Features Guide (Priority 1 - Quick Wins)

### 1. LR Finder (Auto-detect Optimal Learning Rate)

**What it does:** Automatically finds the optimal learning rate before training using Leslie Smith's method.

**How to use:**
```bash
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --find-lr \
  --lr-finder-iterations 100
```

**What happens:**
1. Runs LR range test (start: 1e-7, end: 1.0)
2. Tests ~100 different learning rates
3. Finds LR with steepest descent (fastest loss decrease)
4. Automatically applies suggested LR to training
5. Saves plot to `models/transformer/lr_finder_plot.png`

**Benefits:**
- Eliminates manual LR tuning
- Prevents too-high LR (divergence) or too-low LR (slow learning)
- Takes 5-10 minutes, saves hours of experimentation

**Example output:**
```
[*] Running LR Finder...
    Start LR: 1.00e-07
    End LR: 1.00e+00
    Iterations: 100
    ...
[*] LR Finder Results:
    Raw suggestion: 1.50e-04
    Confidence: high
    Slope magnitude: 0.0234
    SNR: 3.45
    Final LR: 1.50e-04 (accepted)
[*] Applying suggested LR: 1.50e-04
[+] Optimizer and scheduler rebuilt with suggested LR
```

**Note:** If you use `--lr-override`, LR Finder will still run but won't apply the suggested LR.

---

### 1.1 LR Finder Safety & Caching

**Safety Features (NEW):**

The LR Finder now includes intelligent safety validation to prevent applying dangerously high or unreliable learning rates:

**Conservative Safety Cap (5e-4):**
```
If suggested LR > 5e-4:
  â†’ Automatically capped to 5e-4
  â†’ Prevents divergence from too-aggressive LR
  â†’ Logs original suggestion for reference
```

**Curve Quality Analysis:**
The system analyzes the LR-loss curve and assigns a confidence score:
- **High confidence**: Clear steep descent, good signal-to-noise ratio (SNR > 2.0)
- **Medium confidence**: Moderate descent, reasonable SNR (SNR > 1.0)
- **Low confidence**: Flat/noisy curve, or divergence detected

**Automatic Fallback (1e-5):**
```
If confidence is low OR divergence detected:
  â†’ Uses conservative fallback of 1e-5
  â†’ Prevents applying unreliable suggestions
  â†’ Logs reason (e.g., "flat_curve", "divergence_after_min")
```

**Example - Good Curve:**
```
[*] LR Finder Results:
    Raw suggestion: 1.50e-04
    Confidence: high
    Slope magnitude: 0.0234
    SNR: 3.45
    Final LR: 1.50e-04 (accepted)
```

**Example - Low Confidence (Fallback Applied):**
```
[*] LR Finder Results:
    Raw suggestion: 2.30e-03
    Confidence: low
    Slope magnitude: 0.0012
    SNR: 0.65
    Final LR: 1.00e-05 (fallback_due_to_noisy_curve)
[!] WARNING: Used conservative fallback (1e-5)
    Reasons: noisy_curve
```

**Example - Divergent Curve (Fallback Applied):**
```
[*] LR Finder Results:
    Raw suggestion: 5.00e-04
    Confidence: medium
    Divergence detected after minimum
    Final LR: 1.00e-05 (fallback_due_to_divergence_after_min)
[!] WARNING: Used conservative fallback (1e-5)
    Reasons: divergence_after_min
```

**Caching System (NEW):**

To avoid re-running expensive LR Finder tests, results are automatically cached:

**How it works:**
1. **Cache key** computed from:
   - Dataset fingerprint (modification time + file size)
   - Model name (e.g., "microsoft/codebert-base")
   - Batch size
   - Additional config (max_seq_len, etc.)

2. **Cache storage**: `~/.cache/streamguard/lr_finder/`

3. **Cache expiry**: Default 168 hours (1 week)

**Usage:**
```bash
# First run: Cache miss, runs LR Finder
python training/train_transformer.py \
  --find-lr \
  --train-data data.jsonl

# Second run (same data/config): Cache hit, instant!
python training/train_transformer.py \
  --find-lr \
  --train-data data.jsonl
# Output: [*] Loading cached LR Finder results...

# Force refresh cache
python training/train_transformer.py \
  --find-lr \
  --force-find-lr \
  --train-data data.jsonl

# Customize cache expiry (e.g., 24 hours)
python training/train_transformer.py \
  --find-lr \
  --lr-cache-max-age 24
```

**Cache output example:**
```
[*] Computing LR cache key...
    Dataset: data/processed/codexglue/train.jsonl
    Fingerprint: mtime=1696723200, size=52428800
    Model: microsoft/codebert-base
    Batch size: 32
    Cache key: a3f5e9b2c4d1f8e7...

[*] Loading cached LR Finder results...
    Cached LR: 1.50e-04
    Confidence: high
    Age: 12.3 hours (max: 168 hours)
    Analysis: {'slope_mag': 0.0234, 'snr': 3.45, 'diverged': False}
[+] Using cached LR: 1.50e-04
```

**Benefits:**
- **Time savings**: Skip 5-10 minute LR Finder on repeated runs
- **Consistency**: Same data/config always uses same LR
- **Smart invalidation**: Auto-refreshes when data changes

---

### 1.2 Triple Weighting Auto-Adjustment

**What is Triple Weighting?**

When you enable all three class-balancing methods simultaneously:
1. **WeightedRandomSampler** - Balances batches
2. **Class Weights** (via `--weight-multiplier`) - Weights loss function
3. **Focal Loss** (via `--focal-loss`) - Focuses on hard examples

The combined effect can **overcorrect**, leading to:
- Model predicts vulnerable class too aggressively
- High recall but low precision
- Many false positives

**Automatic Detection & Adjustment (NEW):**

The training script now detects triple weighting and automatically reduces weights:

**Detection:**
```python
if (WeightedRandomSampler is ON)
   AND (weight_multiplier > 1.0)
   AND (focal_loss is ON):
    â†’ Triple weighting detected!
```

**Auto-Adjustment:**
```
1. Reduce weight_multiplier by 20%
   Example: 1.5 â†’ 1.2

2. Clamp focal_gamma to 1.5 (if > 1.5)
   Example: 2.0 â†’ 1.5

3. Log original values to checkpoint metadata
```

**Example Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[!] NOTICE: Triple weighting detected!
    - WeightedRandomSampler: ON
    - Class weight multiplier: 1.5
    - Focal Loss: ON

[*] Auto-adjusting to prevent overcorrection...
    weight_multiplier: 1.50 â†’ 1.20
    focal_gamma: 2.00 â†’ 1.5

[*] Recalculating class weights with adjusted multiplier...
    Vulnerable weight: 2.50 â†’ 2.00
    Safe weight: 1.00 (unchanged)

[+] Triple weighting auto-adjustment complete.
    Original values saved to checkpoint metadata for reproducibility.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why This Helps:**

**Before (No adjustment):**
```
Sampler: Balances batches       (+heavy vulnerable focus)
Weight: 1.5x vulnerable weight  (+heavy vulnerable focus)
Focal:  Focus on hard negatives (+heavy vulnerable focus)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Result: TOO MUCH focus â†’ 90% predicted vulnerable (overcorrection!)
```

**After (Auto-adjustment):**
```
Sampler: Balances batches       (+heavy vulnerable focus)
Weight: 1.2x vulnerable weight  (+moderate vulnerable focus)
Focal:  Î³=1.5 (gentler focus)   (+moderate hard negative focus)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Result: Balanced focus â†’ 55% predicted vulnerable (just right!)
```

**Verification:**

Check the ablation test results to confirm this works:
```bash
# Run ablation test comparing 7 configurations
python training/test_ablations.py

# Expected result: 'all_three' config performs well with auto-adjustment
# Compare F1, precision, recall, and prediction balance
```

**Checkpoint Metadata:**

Original values are stored for reproducibility:
```json
{
  "triple_weighting_detected": true,
  "original_weight_multiplier": 1.5,
  "original_focal_gamma": 2.0,
  "adjusted_weight_multiplier": 1.2,
  "adjusted_focal_gamma": 1.5
}
```

**Manual Override:**

If you want to skip auto-adjustment (not recommended):
```bash
# Currently no flag for this - auto-adjustment is always applied
# This is intentional to prevent accidental overcorrection
# If needed, manually edit train_transformer.py line ~850
```

---

### 2. TensorBoard (Real-time Visualization)

**What it does:** Logs training metrics in real-time for interactive visualization.

**How to use:**
```bash
# Start training with TensorBoard
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --tensorboard \
  --epochs 10

# In another terminal, start TensorBoard
tensorboard --logdir=models/transformer/runs
```

**What you'll see:**
- Loss curves (train vs val) in real-time
- F1, accuracy, precision, recall progression
- Prediction distribution (vulnerable vs safe counts)
- Learning rate schedule
- All metrics grouped and filterable

**Benefits:**
- Real-time monitoring (no waiting for training to finish)
- Compare multiple runs side-by-side
- Interactive zooming and filtering
- Share results via TensorBoard.dev (optional)

**TensorBoard URL:** Open browser to `http://localhost:6006`

---

### 3. Auto-plotting (Generate Training Curves)

**What it does:** Automatically generates comprehensive training visualizations from CSV logs.

**How to use:**
```bash
# After training completes
python training/visualize_training.py --model-dir models/transformer

# Or specify CSV directly
python training/visualize_training.py --metrics-csv models/transformer/metrics_history.csv
```

**What plots are generated:**
1. **loss_curves.png** - Train vs val loss over epochs
2. **f1_progression.png** - F1 score with best epoch marked
3. **metrics_overview.png** - Accuracy, precision, recall, F1 together
4. **prediction_distribution.png** - Vulnerable vs safe predictions (counts + percentages)
5. **lr_schedule.png** - Learning rate over epochs (log scale)
6. **training_dashboard.png** - Comprehensive dashboard with all metrics + summary stats

**Benefits:**
- Publication-ready plots with one command
- Automatic detection of best epoch
- Summary statistics included
- No manual matplotlib coding needed

**Example output:**
```
[+] Loaded 10 epochs from models/transformer/metrics_history.csv
[*] Generating plots...
[+] Saved: models/transformer/plots/loss_curves.png
[+] Saved: models/transformer/plots/f1_progression.png
[+] Saved: models/transformer/plots/metrics_overview.png
[+] Saved: models/transformer/plots/prediction_distribution.png
[+] Saved: models/transformer/plots/lr_schedule.png
[+] Saved comprehensive dashboard: models/transformer/plots/training_dashboard.png
[+] All plots generated successfully!
```

---

### Full Example Workflow (with all new features)

```bash
# 1. Find optimal LR + train with TensorBoard
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --test-data data/processed/codexglue/test.jsonl \
  --find-lr \
  --tensorboard \
  --epochs 10 \
  --batch-size 32 \
  --use-weighted-sampler \
  --use-code-features \
  --seed 42

# 2. (In another terminal) Start TensorBoard
tensorboard --logdir=models/transformer/runs

# 3. After training, generate plots
python training/visualize_training.py --model-dir models/transformer

# 4. View results
# - TensorBoard: http://localhost:6006
# - Plots: models/transformer/plots/
# - CSV: models/transformer/metrics_history.csv
```

---

## Configuration Flags Reference

### Critical Flags (Use These!)

| Flag | Default | Recommended | Description |
|------|---------|-------------|-------------|
| `--use-weighted-sampler` | False | **True** | Balance batches by class (critical!) |
| `--use-code-features` | False | **True** | Add 10 security metrics (+5-10 F1) |
| `--lr-override` | None | 1.5e-5 (quick), 2e-5 (full) | Override LR calculation |
| `--weight-multiplier` | 1.5 | 1.5-2.0 | Boost minority class weight |

### Advanced Flags

| Flag | Default | Description |
|------|---------|-------------|
| `--focal-loss` | False | Use Focal Loss (helps hard negatives) |
| `--focal-gamma` | 2.0 | Focal loss focusing parameter (1.0-2.5) |
| `--batch-size` | 16 | Batch size |
| `--epochs` | 5 | Number of epochs |
| `--lr` | 2e-5 | Base learning rate |
| `--accumulation-steps` | 1 | Gradient accumulation |
| `--mixed-precision` | False | Enable AMP |
| `--quick-test` | False | Use 500 train, 100 val samples |

### NEW: Priority 1 Flags (Quick Wins)

| Flag | Default | Description |
|------|---------|-------------|
| `--find-lr` | False | **Run LR Finder before training (auto-detect optimal LR)** |
| `--lr-finder-iterations` | 100 | Number of iterations for LR finder |
| `--force-find-lr` | False | **Ignore cached LR, always run LR Finder fresh** |
| `--lr-cache-max-age` | 168 | **LR cache validity in hours (default: 1 week)** |
| `--tensorboard` | False | **Enable TensorBoard logging for real-time visualization** |

### Dataset Flags

| Flag | Required | Description |
|------|----------|-------------|
| `--train-data` | Yes | Path to train.jsonl |
| `--val-data` | Yes | Path to valid.jsonl |
| `--test-data` | No | Path to test.jsonl (optional) |

---

## Output Files

After training, check these files:

### Checkpoints
- `models/transformer/checkpoints/best_model.pt` - Best model (highest F1)
- `models/transformer/checkpoints/checkpoint_epoch_N.pt` - Per-epoch checkpoints

### Metrics & Logs
- `models/transformer/metrics_history.csv` - All metrics per epoch
- `models/transformer/exp_config.json` - Full experiment config
- `models/transformer/collapse_report.json` - Diagnostic report (if collapse occurred)
- `models/transformer/lr_finder_plot.png` - LR Finder results (if --find-lr used)
- `models/transformer/runs/` - TensorBoard logs (if --tensorboard used)
- `models/transformer/plots/` - Auto-generated visualizations (from visualize_training.py)

### CSV Columns
```
epoch, train_loss, val_loss, val_f1, val_acc, val_precision, val_recall,
pred_vuln, pred_safe, actual_vuln, actual_safe, lr
```

**Use this to:**
- Plot training curves
- Debug collapse issues
- Track prediction distributions
- Monitor LR schedule

---

## Expected Training Times

| Config | GPU | Time | F1 |
|--------|-----|------|-----|
| Quick test | T4 | 10-15 min | 0.7 |
| Quick test | V100 | 5-8 min | 0.7 |
| Full training | T4 | 2-3 hours | 0.75-0.78 |
| Full + features | T4 | 2.5-3.5 hours | 0.78-0.82 |
| Full + features | V100 | 1-1.5 hours | 0.78-0.82 |
| Full + features | A100 | 30-45 min | 0.78-0.82 |

---

## Monitoring Training

### Good Signs âœ…

```
[*] Using WeightedRandomSampler (inverse-frequency)
[*] Quick test mode: class weights (1.0 vs 1.66)
[*] Final LR: 1.50e-05
[*] Discriminative LR:
    Encoder (123 params): LR=1.50e-06, WD=0.01
    Head (45 params): LR=1.50e-05, WD=0.01
    No-decay (12 params): LR=1.50e-05, WD=0.0
[*] Code features enabled: 10 metrics + fusion layer

Epoch 1/10:
Train Loss: 0.6521
Val F1 (vulnerable): 0.5832
Predictions: Vulnerable=50/54, Safe=50/46
[+] New best model! F1: 0.5832

Epoch 5/10:
Train Loss: 0.5123
Val F1 (vulnerable): 0.7245
Predictions: Vulnerable=52/54, Safe=48/46
[+] New best model! F1: 0.7245
```

### Bad Signs âŒ (Old behavior - should NOT see this now)

```
Epoch 1/10:
Val F1 (vulnerable): 0.4337
Predictions: Vulnerable=45/54, Safe=55/46

Epoch 2/10:
Val F1 (vulnerable): 0.0000
Predictions: Vulnerable=0/54, Safe=100/46
[!] COLLAPSE SIGNAL: Zero vulnerable predictions

Epoch 3/10:
Val F1 (vulnerable): 0.0000
Predictions: Vulnerable=0/54, Safe=100/46
[!] CRITICAL: Collapse detected for 2 consecutive epochs
[!] STOPPING TRAINING. Recommended fixes:
    1. Add: --use-weighted-sampler
    2. Try: --lr-override 2e-5
    3. Increase: --weight-multiplier 2.0
    4. Try: --focal-loss
```

---

## Example Workflows

### Workflow 1: Quick Validation (Before Full Run)

```bash
# Step 1: Quick test (10 min)
python training/train_transformer.py \
  --quick-test --epochs 5 --use-weighted-sampler

# Step 2: If F1 > 0.6, proceed to full training
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --test-data data/processed/codexglue/test.jsonl \
  --epochs 10 \
  --use-weighted-sampler \
  --use-code-features
```

### Workflow 2: Hyperparameter Sweep

```bash
# Try different weight multipliers
for mult in 1.2 1.5 2.0; do
  python training/train_transformer.py \
    --quick-test \
    --use-weighted-sampler \
    --weight-multiplier $mult \
    --output-dir models/transformer_mult_${mult}
done

# Compare results
cat models/transformer_mult_*/metrics_history.csv
```

### Workflow 3: Production Training

```bash
# Full training with all features
python training/train_transformer.py \
  --train-data data/processed/codexglue/train.jsonl \
  --val-data data/processed/codexglue/valid.jsonl \
  --test-data data/processed/codexglue/test.jsonl \
  --epochs 10 \
  --batch-size 32 \
  --use-weighted-sampler \
  --use-code-features \
  --weight-multiplier 1.2 \
  --mixed-precision \
  --seed 42 \
  --output-dir models/transformer_production

# Evaluate best model
python scripts/evaluate_model.py \
  --model-path models/transformer_production/checkpoints/best_model.pt \
  --test-data data/processed/codexglue/test.jsonl
```

---

## FAQ

### Q: What's the difference between quick test and full training?

**Quick test (`--quick-test`):**
- Uses 500 train, 100 val samples
- Faster (10-15 min)
- Good for verifying fixes work
- LR automatically set to 1.5e-5
- Dropout disabled
- Patience increased to 6 epochs

**Full training:**
- Uses all data (21K train, 2.7K val)
- Slower (2-3 hours)
- Production-quality model
- LR scaled by batch size
- Normal regularization

### Q: When should I use --focal-loss?

Use Focal Loss when:
- Standard CrossEntropy causes collapse
- Hard negatives dominate (difficult to classify examples)
- Class imbalance is severe (>70/30 split)
- You want to focus learning on hard examples

### Q: What do the code features do?

The 10 code metrics capture:
- **Basic:** LOC, SLOC (code size)
- **SQL injection signals:** sql_concat, execute_calls, user_input
- **Complexity:** loops, conditionals, function_calls, try_blocks, string_ops

These provide explicit signals that complement CodeBERT's learned representations, typically adding **+5-10 F1 points**.

### Q: How do I know if training is working?

**Check these indicators:**

1. âœ… **F1 increases each epoch** (not stuck at 0.0)
2. âœ… **Predictions are balanced** (~40-60% each class)
3. âœ… **Loss decreases steadily**
4. âœ… **No collapse warnings**
5. âœ… **CSV log shows improvement**

If any of these fail, see Troubleshooting section.

### Q: Can I resume training from a checkpoint?

Yes! Add `--resume-from <checkpoint_path>`:

```bash
python training/train_transformer.py \
  --resume-from models/transformer/checkpoints/checkpoint_epoch_5.pt \
  --epochs 10
```

This will load model, optimizer, scheduler states and continue from that epoch.

---

## Getting Help

If you encounter issues:

1. **Check collapse_report.json** (if training stopped)
2. **Check metrics_history.csv** for trends
3. **Try recommended fixes** from error messages
4. **Open an issue** with:
   - Full command used
   - Last 20 lines of output
   - collapse_report.json (if exists)
   - GPU type and CUDA version

---

## Next Steps

After successful training:

1. **Evaluate on test set** (if not done already)
2. **Try Phase 2 data** (collector datasets) for continuous improvement
3. **Experiment with GNN + Fusion** models
4. **Deploy model** for real-time detection

---

**Training should now be stable with F1 â‰¥ 0.75 on full data! ğŸ‰**
