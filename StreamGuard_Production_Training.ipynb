{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71obQ29i_Dy8"
      },
      "source": [
        "# StreamGuard ML Training - Complete Notebook\n",
        "\n",
        "**Version:** 1.7 (Safety Features Available - See instructions at end)  \n",
        "**Last Updated:** 2025-11-01  \n",
        "**Platform:** Google Colab (Free/Pro/Pro+)  \n",
        "**GPU:** T4/V100/A100 (Adaptive Configuration)  \n",
        "**Duration:** 11-24 hours (depends on GPU & config)  \n",
        "\n",
        "This notebook trains all three StreamGuard models with **adaptive configuration** that automatically optimizes for your GPU.\n",
        "\n",
        "## \ud83c\udfaf Training Phases\n",
        "1. **Enhanced SQL Intent Transformer** (2-8 hours depending on GPU)\n",
        "2. **Enhanced Taint-Flow GNN** (4-12 hours depending on GPU)\n",
        "3. **Fusion Layer** (2-10 hours depending on GPU)\n",
        "\n",
        "## \u2728 What's New in v1.7 (Safety Features)\n",
        "\n",
        "**NEW: Optional Safety Features Available**\n",
        "- \u2705 **LR Finder with Safety Validation** (auto-detects optimal learning rate, 5e-4 cap, smart fallback)\n",
        "- \u2705 **LR Caching** (skip 5-10 min LR Finder on reruns, 168-hour cache)\n",
        "- \u2705 **Triple Weighting Auto-Adjustment** (prevents overcorrection when using sampler + weights + focal)\n",
        "- \u2705 **Enhanced Checkpoint Metadata** (includes seed, git commit, LR analysis)\n",
        "- \u2705 **Unit Tests** (14 tests verify all safety features)\n",
        "\n",
        "**See instructions at the END of this notebook for how to use these features.**\n",
        "\n",
        "**Backward Compatible:** All existing cells work exactly as before. New features are opt-in via CLI flags.\n",
        "\n",
        "## \u2728 What's New in v1.6 (Issue #11 - Training Collapse Fix)\n",
        "\n",
        "### **CRITICAL: Training Collapse Fixed (Issue #11)**\n",
        "- \u2705 **Class-balanced loss with inverse frequency weights** (fixes model predicting only safe class)\n",
        "- \u2705 **LR scaling for large batches** (square-root rule: batch 64 gets 2x base LR)\n",
        "- \u2705 **Per-step scheduler** (moved inside train_epoch, was per-epoch before)\n",
        "- \u2705 **Gradient clipping** (max_norm=1.0 prevents exploding gradients)\n",
        "- \u2705 **Prediction distribution monitoring** (detects collapse early)\n",
        "- \u2705 **Enhanced collapse detection** (stops training if model predicts only one class)\n",
        "- \u2705 **Conservative label smoothing** (0.05 instead of 0.1)\n",
        "- \u2705 **Simplified loss calculation** (removed unnecessary sample-level weighting)\n",
        "\n",
        "**Root Cause (Issue #11):** Model collapsed from F1=0.4337 (epoch 1) to F1=0.0000 (epoch 3+) due to:\n",
        "1. No class balancing (54.2% safe vs 45.8% vulnerable)\n",
        "2. LR designed for batch=16 but using batch=64\n",
        "3. Scheduler stepping per-epoch instead of per-step\n",
        "4. No gradient clipping\n",
        "5. No early collapse detection\n",
        "\n",
        "**The Fix:** All 8 critical fixes implemented in train_transformer.py (see `docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md`)\n",
        "\n",
        "### **Previous Fixes (v1.5 - Issue #10)**\n",
        "- \u2705 **Max seq length configuration fixed** (512 for all GPUs, not 1024/768)\n",
        "- \u2705 **Automatic validation** to prevent exceeding CodeBERT's 512-token limit\n",
        "- \u2705 **Tensor size mismatch error prevented**\n",
        "- \u2705 **Updated PyTorch AMP API** (torch.amp instead of torch.cuda.amp)\n",
        "\n",
        "### **Previous Fixes (v1.4 - Issue #9)**\n",
        "- \u2705 **Fixed CrossEntropyLoss tensor-to-scalar error**\n",
        "- \u2705 **Fixed sample weights handling**\n",
        "- \u2705 **Updated deprecated autocast/GradScaler**\n",
        "- \u2705 **Added Cell 1.5** (robust GPU detection with fallback)\n",
        "\n",
        "### **Previous Fixes (v1.3 - Issue #8)**\n",
        "- \u2705 **Fixed NumPy binary incompatibility** (numpy==1.26.4 enforced)\n",
        "- \u2705 **Fixed tokenizers/transformers conflict** (tokenizers 0.14.1)\n",
        "- \u2705 **Fixed PyG circular import errors**\n",
        "\n",
        "### **Adaptive GPU Configuration (Colab Pro)**\n",
        "- \ud83d\udd0d **Auto-detects GPU type** (T4/V100/A100) via Cell 1.5\n",
        "- \u2699\ufe0f  **Selects optimal hyperparameters** automatically\n",
        "- \ud83d\udcca **Three configuration tiers**:\n",
        "  - **OPTIMIZED** (T4): 10/150/30 epochs, batch 32/64, seq 512, ~13-17h\n",
        "  - **ENHANCED** (V100): 15/200/50 epochs, batch 48/96, seq 512, ~18-22h (2-3x faster)\n",
        "  - **AGGRESSIVE** (A100): 20/300/100 epochs, batch 64/128, seq 512, ~20-24h (5-7x faster)\n",
        "\n",
        "**Note:** All configurations use `max_seq_len = 512` (CodeBERT/RoBERTa model limit). Better GPUs benefit from larger batch sizes and more epochs.\n",
        "\n",
        "### **Colab Pro Benefits**\n",
        "- \u2705 24-hour runtime (vs 12h free)\n",
        "- \u2705 Better GPU access (V100, A100)\n",
        "- \u2705 Background execution\n",
        "- \u2705 **Larger batches \u2192 better gradient estimates**\n",
        "\n",
        "**Recommended:** V100 on Colab Pro ($10/mo) for best balance of speed and availability.\n",
        "\n",
        "## \ud83d\udd27 All Critical Fixes Applied (v1.1 \u2192 v1.7)\n",
        "\n",
        "### **v1.7 Fixes (Safety Features) - NEW**\n",
        "- \u2705 LR Finder with safety validation (5e-4 cap, 1e-5 fallback)\n",
        "- \u2705 LR caching (168-hour default, dataset fingerprint-based)\n",
        "- \u2705 Triple weighting auto-adjustment (20% reduction when all enabled)\n",
        "- \u2705 Enhanced checkpoint metadata (seed, git, LR analysis)\n",
        "- \u2705 Unit tests (14 tests for all safety features)\n",
        "\n",
        "### **v1.6 Fixes (Issue #11)**\n",
        "- \u2705 Class-balanced loss with inverse frequency weights\n",
        "- \u2705 LR scaling for large batches (square-root rule)\n",
        "- \u2705 Warmup ratio adjustment (proportional, capped at 20%)\n",
        "- \u2705 Per-step scheduler (moved inside train_epoch)\n",
        "- \u2705 Gradient clipping (max_norm=1.0)\n",
        "- \u2705 Prediction distribution monitoring\n",
        "- \u2705 Enhanced collapse detection\n",
        "- \u2705 Conservative label smoothing (0.05)\n",
        "- \u2705 Drive-based data workflow (automatic copy to local storage)\n",
        "- \u2705 Pre-training validation tests\n",
        "\n",
        "### **v1.5 Fixes (Issue #10)**\n",
        "- \u2705 Max seq length configuration fixed\n",
        "- \u2705 Automatic validation added\n",
        "- \u2705 Tensor size mismatch prevented\n",
        "- \u2705 PyTorch AMP API updated\n",
        "\n",
        "### **v1.4 Fixes (Issue #9)**\n",
        "- \u2705 CrossEntropyLoss tensor-to-scalar error fixed\n",
        "- \u2705 Sample weights handling validated\n",
        "- \u2705 Deprecated API updated\n",
        "- \u2705 GPU detection robustness improved\n",
        "\n",
        "### **v1.3 Fixes (Issue #8)**\n",
        "- \u2705 NumPy binary compatibility fixed\n",
        "- \u2705 tokenizers/transformers conflict resolved\n",
        "- \u2705 PyG circular import fixed\n",
        "\n",
        "### **v1.1-v1.2 Fixes (Issues #1-#7)**\n",
        "- \u2705 Runtime-aware PyTorch Geometric installation\n",
        "- \u2705 Robust tree-sitter build with fallback\n",
        "- \u2705 Version compatibility validation\n",
        "- \u2705 Enhanced dependency conflict detection\n",
        "- \u2705 Optimized OOF fusion\n",
        "\n",
        "## \ud83d\udccb Before Starting\n",
        "\n",
        "### **Colab Configuration:**\n",
        "1. Enable GPU: **Runtime \u2192 Change runtime type \u2192 GPU**\n",
        "2. **Recommended:** Subscribe to Colab Pro ($10/mo) for:\n",
        "   - 24-hour runtime (required for full training)\n",
        "   - Access to V100/A100 GPUs (2-7x faster than T4)\n",
        "   - Background execution\n",
        "\n",
        "### **Data Requirements - IMPORTANT:**\n",
        "\n",
        "**You MUST upload preprocessed data files to Google Drive:**\n",
        "\n",
        "```\n",
        "My Drive/streamguard/data/processed/codexglue/\n",
        "\u251c\u2500\u2500 train.jsonl (504 MB, 21,854 samples)\n",
        "\u251c\u2500\u2500 valid.jsonl (63 MB, 2,732 samples)\n",
        "\u251c\u2500\u2500 test.jsonl (63 MB, 2,732 samples)\n",
        "\u2514\u2500\u2500 preprocessing_metadata.json (1.6 KB)\n",
        "```\n",
        "\n",
        "**Total size:** ~630 MB\n",
        "\n",
        "**Why Google Drive?**\n",
        "- Data files are too large for GitHub (exceeds 100 MB limit)\n",
        "- They are in `.gitignore` and won't be cloned from the repository\n",
        "- **Cell 6** will automatically mount Drive and copy data to Colab local storage\n",
        "- Local storage provides faster I/O during training (vs reading from Drive each time)\n",
        "\n",
        "**How to upload:**\n",
        "1. Open Google Drive: https://drive.google.com/\n",
        "2. Create folder structure: `My Drive/streamguard/data/processed/codexglue/`\n",
        "3. Upload the 4 data files to this folder\n",
        "4. Run notebook Cell 6 - it will copy files to Colab automatically\n",
        "\n",
        "## \ud83d\udcca Expected Results by Configuration\n",
        "\n",
        "| Config | GPU | Time | Batch Sizes (T/G) | Seq Len | Speed vs T4 |\n",
        "|--------|-----|------|-------------------|---------|-------------|\n",
        "| **OPTIMIZED** | T4 | 13-17h | 32 / 64 | 512 | 1.0x |\n",
        "| **ENHANCED** | V100 | 18-22h | 48 / 96 | 512 | 2-3x faster |\n",
        "| **AGGRESSIVE** | A100 | 20-24h | 64 / 128 | 512 | 5-7x faster |\n",
        "\n",
        "*Note: All configs use max_seq_len=512 (CodeBERT limit). Better GPUs use larger batches/epochs for quality.*\n",
        "\n",
        "## \ud83d\ude80 Quick Start\n",
        "\n",
        "1. **Upload data to Drive** (see Data Requirements above)\n",
        "2. Run **Cell 1**: Verify GPU is enabled\n",
        "3. Run **Cell 1.5**: Auto-detect GPU and select configuration  \n",
        "4. Run **Cell 2**: Install dependencies with compatibility fixes\n",
        "5. Run **Cell 2.5**: Validate compatibility\n",
        "6. Run **Cell 3**: Clone repository from GitHub\n",
        "7. Run **Cell 4**: Setup tree-sitter\n",
        "8. Run **Cell 6**: Mount Drive and copy data to local storage \u2b50\n",
        "9. **Run TEST CELLS 6.5 & 6.6**: Verify Issue #11 fixes (5-15 min total)\n",
        "10. Run **Cells 7, 9, 11**: Full training with adaptive configuration\n",
        "11. Monitor progress (can close browser with Colab Pro)\n",
        "\n",
        "**IMPORTANT:** Run the test cells (6.5 & 6.6) before full training to verify all fixes are working!\n",
        "\n",
        "**NEW:** For v1.7 safety features, see instructions at the END of this notebook.\n",
        "\n",
        "## \ud83d\udd17 Documentation\n",
        "\n",
        "- **Training Collapse Fix:** See [docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md)\n",
        "- **Final Recommendations:** See [docs/ISSUE_11_FINAL_CAUTIONS_AND_RECOMMENDATIONS.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_11_FINAL_CAUTIONS_AND_RECOMMENDATIONS.md)\n",
        "- **Max Seq Length Fix:** See [docs/ISSUE_10_MAX_SEQ_LEN_FIX.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_10_MAX_SEQ_LEN_FIX.md)\n",
        "- **Critical Fixes Details:** See [docs/COLAB_CRITICAL_FIXES.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/COLAB_CRITICAL_FIXES.md)\n",
        "- **Troubleshooting:** Check Issue #8, #9, #10, and #11 documentation for common errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ_fI8de_Dy_"
      },
      "source": [
        "---\n",
        "## Part 1: Environment Setup\n",
        "Run these cells once at the beginning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr8Lmrx7_DzA",
        "outputId": "9380a6e0-d1bc-48dd-e4ca-a34d16a3380d"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Verify GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  WARNING: GPU not available! Enable GPU in Runtime \u2192 Change runtime type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LAE7YnvL8Xp",
        "outputId": "c9c0056e-0b50-47d4-da21-883edfa35de8"
      },
      "outputs": [],
      "source": [
        "# Cell 1.5: GPU Detection & Adaptive Configuration (Colab Pro Optimization)\n",
        "import subprocess\n",
        "import json\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def get_gpu_info():\n",
        "    \"\"\"Detect GPU type and memory with robust fallback.\"\"\"\n",
        "    try:\n",
        "        # Try nvidia-smi first (most reliable)\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
        "            capture_output=True, text=True, timeout=5\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            lines = result.stdout.strip().split('\\n')\n",
        "            # Use first GPU if multiple\n",
        "            gpu_line = lines[0].split(',')\n",
        "            gpu_name = gpu_line[0].strip()\n",
        "\n",
        "            # Parse memory (handle \"15360 MiB\" or \"15.36 GB\")\n",
        "            mem_str = gpu_line[1].strip()\n",
        "            if 'MiB' in mem_str:\n",
        "                gpu_memory = float(re.findall(r'\\d+', mem_str)[0]) / 1024  # MiB to GB\n",
        "            else:\n",
        "                gpu_memory = float(re.findall(r'[\\d.]+', mem_str)[0])\n",
        "\n",
        "            return gpu_name, gpu_memory\n",
        "    except (subprocess.TimeoutExpired, FileNotFoundError, IndexError, ValueError):\n",
        "        pass\n",
        "\n",
        "    # Fallback to PyTorch\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Bytes to GB\n",
        "        return gpu_name, gpu_memory\n",
        "\n",
        "    # No GPU available\n",
        "    return \"CPU\", 0.0\n",
        "\n",
        "gpu_name, gpu_memory_gb = get_gpu_info()\n",
        "gpu_name_lower = gpu_name.lower()\n",
        "\n",
        "# Determine configuration tier (case-insensitive matching)\n",
        "# CRITICAL FIX (Issue #9): CodeBERT max_seq_len is 512 (514 with special tokens) - RoBERTa limitation\n",
        "# Using max_seq_len > 512 causes: RuntimeError: The expanded size of the tensor (1024) must match the existing size (514)\n",
        "if 'a100' in gpu_name_lower:\n",
        "    config_tier = 'AGGRESSIVE'\n",
        "    config = {\n",
        "        'transformer': {'epochs': 20, 'batch_size': 64, 'max_seq_len': 512, 'patience': 5},\n",
        "        'gnn': {'epochs': 300, 'batch_size': 128, 'hidden_dim': 512, 'num_layers': 5, 'patience': 15},\n",
        "        'fusion': {'n_folds': 10, 'epochs': 100}\n",
        "    }\n",
        "    note = \"Maximum configuration - larger batches and more epochs for best training quality\"\n",
        "elif 'v100' in gpu_name_lower:\n",
        "    config_tier = 'ENHANCED'\n",
        "    config = {\n",
        "        'transformer': {'epochs': 15, 'batch_size': 48, 'max_seq_len': 512, 'patience': 3},\n",
        "        'gnn': {'epochs': 200, 'batch_size': 96, 'hidden_dim': 384, 'num_layers': 5, 'patience': 12},\n",
        "        'fusion': {'n_folds': 5, 'epochs': 50}\n",
        "    }\n",
        "    note = \"Enhanced configuration - 2-3x faster than T4, larger batches for better gradient estimates\"\n",
        "else:  # T4 or other\n",
        "    config_tier = 'OPTIMIZED'\n",
        "    config = {\n",
        "        'transformer': {'epochs': 10, 'batch_size': 32, 'max_seq_len': 512, 'patience': 2},\n",
        "        'gnn': {'epochs': 150, 'batch_size': 64, 'hidden_dim': 256, 'num_layers': 4, 'patience': 10},\n",
        "        'fusion': {'n_folds': 5, 'epochs': 30}\n",
        "    }\n",
        "    note = \"Optimized for T4 - reliable and cost-effective\"\n",
        "\n",
        "# Save config for training cells\n",
        "config_data = {'tier': config_tier, 'gpu': gpu_name, 'config': config}\n",
        "with open('/tmp/gpu_training_config.json', 'w') as f:\n",
        "    json.dump(config_data, f)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ADAPTIVE GPU CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Detected GPU: {gpu_name}\")\n",
        "print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
        "print(f\"\\nConfiguration Tier: {config_tier}\")\n",
        "print(f\"Note: {note}\")\n",
        "print(\"\\nHyperparameters:\")\n",
        "print(f\"  Transformer: {config['transformer']['epochs']} epochs, batch {config['transformer']['batch_size']}, seq {config['transformer']['max_seq_len']}\")\n",
        "print(f\"  GNN: {config['gnn']['epochs']} epochs, batch {config['gnn']['batch_size']}, hidden {config['gnn']['hidden_dim']}\")\n",
        "print(f\"  Fusion: {config['fusion']['n_folds']} folds, {config['fusion']['epochs']} epochs\")\n",
        "print(\"\\n\ud83d\udca1 Note: max_seq_len is 512 for all configs (CodeBERT/RoBERTa model limit)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4uF4Wv7_DzB",
        "outputId": "a3345c97-7207-4a9a-bb44-90f36a98b0f4"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Install dependencies with runtime detection and compatibility fixes\n",
        "# \u26a0\ufe0f CRITICAL: Includes NumPy compatibility fix, correct tokenizers version, and PyG error handling\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    \"\"\"Run shell command and return success status.\"\"\"\n",
        "    print(f\"Running: {cmd}\")\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error: {result.stderr}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INSTALLING DEPENDENCIES WITH COMPATIBILITY FIXES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# [1/9] CRITICAL: Fix NumPy version FIRST (before any torch imports)\n",
        "print(\"\\n[1/9] Ensuring NumPy compatibility...\")\n",
        "try:\n",
        "    import numpy\n",
        "    numpy_ver = numpy.__version__\n",
        "    numpy_major = int(numpy_ver.split('.')[0])\n",
        "\n",
        "    if numpy_major >= 2:\n",
        "        print(f\"\u26a0\ufe0f  Detected NumPy {numpy_ver} (v2.x)\")\n",
        "        print(\"   PyTorch wheels may have binary incompatibility\")\n",
        "        print(\"   Downgrading to NumPy 1.26.4...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.26.4\", \"--force-reinstall\"], check=True)\n",
        "        print(\"\u2713 NumPy downgraded to 1.26.4\")\n",
        "        # Reload numpy\n",
        "        importlib.reload(numpy)\n",
        "        print(f\"\u2713 NumPy {numpy.__version__} loaded (binary compatible)\")\n",
        "    else:\n",
        "        print(f\"\u2713 NumPy {numpy_ver} (v1.x - already compatible)\")\n",
        "except ImportError:\n",
        "    print(\"NumPy not installed, installing 1.26.4...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.26.4\"], check=True)\n",
        "    import numpy\n",
        "    print(f\"\u2713 NumPy {numpy.__version__} installed\")\n",
        "\n",
        "# [2/9] Detect PyTorch and CUDA versions (now safe with correct numpy)\n",
        "print(\"\\n[2/9] Detecting PyTorch and CUDA versions...\")\n",
        "import torch\n",
        "\n",
        "torch_version = torch.__version__.split('+')[0]  # e.g., '2.8.0'\n",
        "cuda_version = torch.version.cuda  # e.g., '12.6'\n",
        "cuda_tag = f\"cu{cuda_version.replace('.', '')}\" if cuda_version else 'cpu'  # e.g., 'cu126'\n",
        "\n",
        "print(f\"\u2713 Detected PyTorch {torch_version}\")\n",
        "print(f\"\u2713 Detected CUDA {cuda_version if cuda_version else 'N/A (CPU only)'}\")\n",
        "print(f\"\u2713 Using wheel tag: {cuda_tag}\")\n",
        "\n",
        "# [3/9] Install PyTorch Geometric with enhanced error handling\n",
        "print(\"\\n[3/9] Installing PyTorch Geometric (runtime-aware with fallback)...\")\n",
        "pyg_wheel_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
        "print(f\"Wheel URL: {pyg_wheel_url}\")\n",
        "\n",
        "pyg_packages = ['torch-scatter', 'torch-sparse', 'torch-cluster', 'torch-spline-conv']\n",
        "pyg_install_success = True\n",
        "\n",
        "for pkg in pyg_packages:\n",
        "    print(f\"  Installing {pkg}...\")\n",
        "    if not run_cmd(f\"pip install -q {pkg} -f {pyg_wheel_url}\"):\n",
        "        print(f\"    \u26a0\ufe0f  Wheel install failed, trying source build...\")\n",
        "        if not run_cmd(f\"pip install -q {pkg} --no-binary {pkg}\"):\n",
        "            print(f\"    \u274c Failed to install {pkg}\")\n",
        "            pyg_install_success = False\n",
        "        else:\n",
        "            print(f\"    \u2713 {pkg} installed from source (slower)\")\n",
        "    else:\n",
        "        print(f\"    \u2713 {pkg} installed from wheel\")\n",
        "\n",
        "if pyg_install_success:\n",
        "    run_cmd(\"pip install -q torch-geometric==2.4.0\")\n",
        "    print(\"\u2705 PyTorch Geometric installed successfully\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  Some PyG packages failed - GNN training may have issues\")\n",
        "\n",
        "# [4/9] Install Transformers with COMPATIBLE tokenizers version\n",
        "print(\"\\n[4/9] Installing Transformers with compatible tokenizers...\")\n",
        "print(\"\u26a0\ufe0f  Note: Using tokenizers 0.14.1 (compatible with transformers 4.35.0)\")\n",
        "\n",
        "# Install transformers first, then pin tokenizers to compatible version\n",
        "if not run_cmd(\"pip install -q transformers==4.35.0\"):\n",
        "    print(\"\u274c Transformers installation failed\")\n",
        "else:\n",
        "    # Now pin tokenizers to compatible version\n",
        "    if not run_cmd(\"pip install -q tokenizers==0.14.1\"):\n",
        "        print(\"\u26a0\ufe0f  Could not pin tokenizers to 0.14.1, using auto-resolved version\")\n",
        "    else:\n",
        "        print(\"\u2713 Tokenizers 0.14.1 installed (compatible)\")\n",
        "\n",
        "# Install accelerate\n",
        "run_cmd(\"pip install -q accelerate==0.24.1\")\n",
        "\n",
        "# [5/9] Install tree-sitter\n",
        "print(\"\\n[5/9] Installing tree-sitter...\")\n",
        "run_cmd(\"pip install -q tree-sitter==0.20.4\")\n",
        "\n",
        "# [6/9] Install additional packages\n",
        "print(\"\\n[6/9] Installing additional packages...\")\n",
        "run_cmd(\"pip install -q scikit-learn==1.3.2 scipy==1.11.4 tqdm\")\n",
        "\n",
        "# [7/9] Verify installations with enhanced checks\n",
        "print(\"\\n[7/9] Verifying installations...\")\n",
        "try:\n",
        "    # Check NumPy first (critical)\n",
        "    import numpy\n",
        "    numpy_ver = numpy.__version__\n",
        "    numpy_major = int(numpy_ver.split('.')[0])\n",
        "    if numpy_major >= 2:\n",
        "        print(f\"\u26a0\ufe0f  WARNING: NumPy {numpy_ver} detected (should be 1.x)\")\n",
        "        print(\"   Binary compatibility issues may occur\")\n",
        "    else:\n",
        "        print(f\"\u2713 NumPy: {numpy_ver} (binary compatible)\")\n",
        "\n",
        "    # Check other packages\n",
        "    import torch\n",
        "    import torch_geometric\n",
        "    import transformers\n",
        "    import tree_sitter\n",
        "    import sklearn\n",
        "\n",
        "    print(f\"\u2713 PyTorch: {torch.__version__}\")\n",
        "    print(f\"\u2713 PyTorch Geometric: {torch_geometric.__version__}\")\n",
        "    print(f\"\u2713 Transformers: {transformers.__version__}\")\n",
        "\n",
        "    # Check tokenizers compatibility\n",
        "    import tokenizers\n",
        "    tokenizers_ver = tokenizers.__version__\n",
        "    print(f\"\u2713 Tokenizers: {tokenizers_ver}\")\n",
        "\n",
        "    if tokenizers_ver.startswith(\"0.15\"):\n",
        "        print(f\"  \u26a0\ufe0f  WARNING: tokenizers {tokenizers_ver} may conflict with transformers 4.35.0\")\n",
        "    elif tokenizers_ver.startswith(\"0.14\"):\n",
        "        print(f\"  \u2713 Tokenizers version compatible\")\n",
        "\n",
        "    print(f\"\u2713 tree-sitter: {tree_sitter.__version__}\")\n",
        "    print(f\"\u2713 scikit-learn: {sklearn.__version__}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Verification failed: {e}\")\n",
        "    print(\"   Please restart runtime and try again\")\n",
        "    print(\"   If issue persists, check:\")\n",
        "    print(\"   1. NumPy version (should be 1.26.4)\")\n",
        "    print(\"   2. Tokenizers version (should be 0.14.1)\")\n",
        "\n",
        "# [8/9] Test PyTorch Geometric installation\n",
        "print(\"\\n[8/9] Testing PyTorch Geometric...\")\n",
        "try:\n",
        "    from torch_geometric.data import Data\n",
        "    test_data = Data(x=torch.randn(5, 3), edge_index=torch.tensor([[0, 1], [1, 0]]))\n",
        "    print(\"\u2713 PyTorch Geometric working correctly\")\n",
        "    print(f\"\u2713 Test data created: {test_data}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  PyTorch Geometric test failed: {e}\")\n",
        "    print(\"   GNN training may have issues\")\n",
        "    print(\"   Possible causes:\")\n",
        "    print(\"   1. NumPy binary incompatibility\")\n",
        "    print(\"   2. PyG wheel installation failed\")\n",
        "    print(\"   3. CUDA version mismatch\")\n",
        "\n",
        "# [9/9] Display final summary\n",
        "print(\"\\n[9/9] Installation Summary:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "success_indicators = {\n",
        "    'numpy_compatible': numpy_major < 2 if 'numpy_major' in locals() else False,\n",
        "    'pyg_installed': pyg_install_success,\n",
        "    'transformers_installed': True,  # Assume success if we got here\n",
        "    'tokenizers_compatible': tokenizers_ver.startswith(\"0.14\") if 'tokenizers_ver' in locals() else False\n",
        "}\n",
        "\n",
        "all_success = all(success_indicators.values())\n",
        "\n",
        "if all_success:\n",
        "    print(\"\u2705 ALL INSTALLATIONS SUCCESSFUL\")\n",
        "    print(\"\u2713 NumPy 1.x (binary compatible)\")\n",
        "    print(\"\u2713 PyTorch Geometric with correct wheels\")\n",
        "    print(\"\u2713 Transformers with compatible tokenizers\")\n",
        "    print(\"\u2713 All packages verified\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  INSTALLATION COMPLETED WITH WARNINGS:\")\n",
        "    if not success_indicators['numpy_compatible']:\n",
        "        print(\"  \u2022 NumPy version may cause binary incompatibility\")\n",
        "    if not success_indicators['pyg_installed']:\n",
        "        print(\"  \u2022 PyG packages had installation issues\")\n",
        "    if not success_indicators['tokenizers_compatible']:\n",
        "        print(\"  \u2022 Tokenizers version may conflict with transformers\")\n",
        "    print(\"\\n  Training may still work, but monitor for errors\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN-qu_tx_DzC",
        "outputId": "6747b25b-c95a-4f81-9e00-c5cfa1a67aa6"
      },
      "outputs": [],
      "source": [
        "# Cell 2.5: Enhanced Version & Dependency Compatibility Check (v1.1)\n",
        "# Validates versions, checks for dependency conflicts, validates PyG wheels\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "import transformers\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENHANCED DEPENDENCY & VERSION COMPATIBILITY CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# [1/4] Check core versions\n",
        "torch_ver = torch.__version__\n",
        "pyg_ver = torch_geometric.__version__\n",
        "transformers_ver = transformers.__version__\n",
        "cuda_ver = torch.version.cuda if torch.cuda.is_available() else \"N/A\"\n",
        "\n",
        "print(f\"\\n[1/4] Installed Core Versions:\")\n",
        "print(f\"  PyTorch: {torch_ver}\")\n",
        "print(f\"  PyTorch Geometric: {pyg_ver}\")\n",
        "print(f\"  Transformers: {transformers_ver}\")\n",
        "print(f\"  CUDA: {cuda_ver}\")\n",
        "\n",
        "# [2/4] Check for problematic optional dependencies (CRITICAL FIX #4)\n",
        "print(f\"\\n[2/4] Checking Optional Dependencies:\")\n",
        "optional_deps = {\n",
        "    'sentence_transformers': None,\n",
        "    'datasets': None,\n",
        "    'fsspec': None,\n",
        "    'gcsfs': None\n",
        "}\n",
        "\n",
        "for pkg_name in optional_deps.keys():\n",
        "    try:\n",
        "        pkg = importlib.import_module(pkg_name)\n",
        "        version = getattr(pkg, '__version__', 'unknown')\n",
        "        optional_deps[pkg_name] = version\n",
        "        print(f\"  \u26a0\ufe0f  {pkg_name}: {version} (not needed for training)\")\n",
        "    except ImportError:\n",
        "        print(f\"  \u2713 {pkg_name}: not installed (correct)\")\n",
        "\n",
        "# Check for version conflicts\n",
        "has_conflicts = False\n",
        "if optional_deps.get('sentence_transformers'):\n",
        "    print(\"\\n  \u26a0\ufe0f  WARNING: sentence-transformers detected\")\n",
        "    print(\"     May conflict with transformers==4.35.0\")\n",
        "    print(\"     If errors occur, uninstall: !pip uninstall -y sentence-transformers\")\n",
        "    has_conflicts = True\n",
        "\n",
        "if optional_deps.get('datasets'):\n",
        "    print(\"\\n  \u26a0\ufe0f  WARNING: datasets library detected\")\n",
        "    print(\"     May pull incompatible transformers/tokenizers versions\")\n",
        "    has_conflicts = True\n",
        "\n",
        "# [3/4] Validate PyG wheel URL (CRITICAL FIX #4)\n",
        "print(f\"\\n[3/4] Validating PyTorch Geometric Installation:\")\n",
        "torch_version = torch_ver.split('+')[0]\n",
        "cuda_tag = f\"cu{cuda_ver.replace('.', '')}\" if cuda_ver != \"N/A\" else 'cpu'\n",
        "pyg_wheel_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
        "\n",
        "print(f\"  Expected wheel URL: {pyg_wheel_url}\")\n",
        "\n",
        "# Quick test PyG installation\n",
        "try:\n",
        "    from torch_geometric.data import Data\n",
        "    test_data = Data(x=torch.randn(5, 3), edge_index=torch.tensor([[0, 1], [1, 0]]))\n",
        "    print(f\"  \u2713 PyTorch Geometric working correctly\")\n",
        "    print(f\"  \u2713 Wheels matched PyTorch {torch_version} + {cuda_tag}\")\n",
        "except Exception as e:\n",
        "    print(f\"  \u274c PyTorch Geometric test failed: {e}\")\n",
        "    print(f\"  \u26a0\ufe0f  Wheel URL may be incorrect - check {pyg_wheel_url}\")\n",
        "\n",
        "# [4/4] Core compatibility checks\n",
        "print(f\"\\n[4/4] Core Compatibility Checks:\")\n",
        "warnings = []\n",
        "errors = []\n",
        "\n",
        "# Check PyTorch version\n",
        "torch_major = int(torch_ver.split('.')[0])\n",
        "if torch_major < 2:\n",
        "    warnings.append(\"\u26a0\ufe0f  PyTorch 2.x+ recommended (you have {torch_ver})\")\n",
        "\n",
        "# Check CUDA availability (CRITICAL)\n",
        "if not torch.cuda.is_available():\n",
        "    errors.append(\"\u274c CUDA not available - training will be EXTREMELY slow\")\n",
        "    errors.append(\"   Enable GPU: Runtime \u2192 Change runtime type \u2192 GPU\")\n",
        "\n",
        "# Check PyG compatibility\n",
        "pyg_major = int(pyg_ver.split('.')[0])\n",
        "if pyg_major < 2:\n",
        "    warnings.append(\"\u26a0\ufe0f  PyTorch Geometric 2.x+ recommended\")\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    if gpu_mem_gb < 12:\n",
        "        warnings.append(f\"\u26a0\ufe0f  GPU has only {gpu_mem_gb:.1f} GB RAM (16GB+ recommended)\")\n",
        "        warnings.append(\"   Consider reducing batch sizes if OOM errors occur\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if errors:\n",
        "    print(\"\ud83d\udd34 CRITICAL ERRORS:\")\n",
        "    for e in errors:\n",
        "        print(f\"  {e}\")\n",
        "    print(\"\\n\u274c CANNOT PROCEED - Fix errors above\")\n",
        "    print(\"=\"*70)\n",
        "    raise RuntimeError(\"Environment validation failed\")\n",
        "elif warnings or has_conflicts:\n",
        "    if warnings:\n",
        "        print(\"\u26a0\ufe0f  Compatibility Warnings:\")\n",
        "        for w in warnings:\n",
        "            print(f\"  {w}\")\n",
        "    if has_conflicts:\n",
        "        print(\"\\n\u26a0\ufe0f  Dependency Conflicts Detected:\")\n",
        "        print(\"  Monitor for errors during training\")\n",
        "        print(\"  If issues occur, restart runtime and reinstall dependencies\")\n",
        "    print(\"\\n\u2713 You can proceed but may need adjustments\")\n",
        "else:\n",
        "    print(\"\u2705 ALL CHECKS PASSED - Ready for production training!\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aJ2uNnO_DzD",
        "outputId": "12c85186-b642-4c86-e91f-45978cebe56a"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Clone/Update repository from GitHub\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone or update StreamGuard repository\n",
        "if not Path('streamguard').exists():\n",
        "    print(\"Cloning StreamGuard repository...\")\n",
        "    !git clone https://github.com/VimalSajanGeorge/streamguard.git\n",
        "    print(\"\u2713 Repository cloned\")\n",
        "else:\n",
        "    print(\"\u2713 Repository already exists\")\n",
        "    print(\"Pulling latest changes...\")\n",
        "    os.chdir('streamguard')\n",
        "    !git pull origin master\n",
        "    print(\"\u2713 Repository updated\")\n",
        "    os.chdir('..')\n",
        "\n",
        "os.chdir('streamguard')\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(\"\\n\ud83d\udca1 All code changes from GitHub are now available!\")\n",
        "print(\"   No need to manually upload files to Google Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wncSmaKW_DzE",
        "outputId": "3c6e39b2-0fab-418f-e351-1e3803db109f"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Setup tree-sitter with robust error handling\n",
        "# \u26a0\ufe0f CRITICAL: Includes fallback if build fails\n",
        "\n",
        "from pathlib import Path\n",
        "from tree_sitter import Language\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TREE-SITTER SETUP (with fallback support)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clone tree-sitter-c\n",
        "vendor_dir = Path('vendor')\n",
        "vendor_dir.mkdir(exist_ok=True)\n",
        "\n",
        "if not (vendor_dir / 'tree-sitter-c').exists():\n",
        "    print(\"\\n[1/3] Cloning tree-sitter-c...\")\n",
        "    !cd vendor && git clone --depth 1 https://github.com/tree-sitter/tree-sitter-c.git\n",
        "    print(\"\u2713 tree-sitter-c cloned\")\n",
        "else:\n",
        "    print(\"\\n[1/3] \u2713 tree-sitter-c already exists\")\n",
        "\n",
        "# Build library with error handling\n",
        "build_dir = Path('build')\n",
        "build_dir.mkdir(exist_ok=True)\n",
        "lib_path = build_dir / 'my-languages.so'\n",
        "\n",
        "build_success = False\n",
        "\n",
        "if not lib_path.exists():\n",
        "    print(\"\\n[2/3] Building tree-sitter library...\")\n",
        "    try:\n",
        "        Language.build_library(\n",
        "            str(lib_path),\n",
        "            [str(vendor_dir / 'tree-sitter-c')]\n",
        "        )\n",
        "        print(\"\u2713 Build completed\")\n",
        "\n",
        "        # Verify build\n",
        "        if lib_path.exists():\n",
        "            print(\"\\n[3/3] Verifying build...\")\n",
        "            try:\n",
        "                test_lang = Language(str(lib_path), 'c')\n",
        "                print(\"\u2713 tree-sitter library verified successfully\")\n",
        "                build_success = True\n",
        "            except Exception as e:\n",
        "                print(f\"\u26a0\ufe0f  Verification failed: {e}\")\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f  Build completed but library file not found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f  Build failed: {e}\")\n",
        "        print(\"   Common causes: missing compiler, permission issues\")\n",
        "else:\n",
        "    print(\"\\n[2/3] \u2713 tree-sitter library already exists\")\n",
        "    print(\"\\n[3/3] Verifying existing build...\")\n",
        "    try:\n",
        "        test_lang = Language(str(lib_path), 'c')\n",
        "        print(\"\u2713 Existing library verified\")\n",
        "        build_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f  Existing library invalid: {e}\")\n",
        "\n",
        "# Display final status\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if build_success:\n",
        "    print(\"\u2705 AST PARSING ENABLED (optimal)\")\n",
        "    print(\"   Preprocessing will use full AST structure\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  AST PARSING WILL USE FALLBACK MODE\")\n",
        "    print(\"   Preprocessing will use token-sequence graphs\")\n",
        "    print(\"   \u2713 Training will still work correctly\")\n",
        "    print(\"   \u2713 Performance impact: minimal (<5%)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD2ONUWq_DzF"
      },
      "source": [
        "### Platform Notes: tree-sitter on Windows/Linux\n",
        "\n",
        "**Google Colab (Linux):**\n",
        "- \u2705 Works out-of-the-box with `.so` libraries\n",
        "- \u2705 GCC compiler available by default\n",
        "\n",
        "**Windows (Local Development):**\n",
        "- \u26a0\ufe0f  Requires Microsoft Visual C++ 14.0+ (MSVC)\n",
        "- \u26a0\ufe0f  May fail with \"compiler not found\" errors\n",
        "- **Solution 1:** Use WSL (Windows Subsystem for Linux) for preprocessing\n",
        "- **Solution 2:** Use Colab for all preprocessing tasks\n",
        "- **Solution 3:** Install Visual Studio Build Tools (large download)\n",
        "- \u2713 **Fallback:** Token-sequence graphs work fine (<5% performance impact)\n",
        "\n",
        "**Recommendation:** For Windows users, use Colab for data preprocessing and training. Download preprocessed data to Windows only for inference/deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI-Y5fiEL8Xu"
      },
      "source": [
        "---\n",
        "## Part 1.5: Pre-Training Validation Tests (Issue #11 Fix Verification)\n",
        "\n",
        "**IMPORTANT:** Run these test cells BEFORE full training to verify all Issue #11 fixes are working correctly.\n",
        "\n",
        "These tests verify:\n",
        "1. \u2705 Class-balanced loss is working (model doesn't collapse to one class)\n",
        "2. \u2705 LR scaling and warmup are correct\n",
        "3. \u2705 Scheduler steps properly (per-step, not per-epoch)\n",
        "4. \u2705 Gradient clipping prevents exploding gradients\n",
        "5. \u2705 Prediction distribution monitoring detects collapse\n",
        "6. \u2705 Checkpoint saving/loading works with PyTorch 2.6+\n",
        "\n",
        "**Expected Results:**\n",
        "- **Test 1 (Tiny Overfitting Test):** Loss should decrease to near 0, F1 should reach 0.9+\n",
        "- **Test 2 (Short Full-Data Test):** F1 should increase each epoch, prediction distribution should be balanced\n",
        "- If tests pass, proceed to full training with confidence!\n",
        "\n",
        "**Duration:** 5-10 minutes total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q29KEnue_DzG",
        "outputId": "ea9a5fa9-ceac-4660-ed79-2967b2743b47"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Setup data from Google Drive\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SETTING UP DATA FROM GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Ensure we're in the streamguard directory\n",
        "os.chdir('/content/streamguard')\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "print(f\"\\n[1/5] Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"\u2713 Google Drive mounted\")\n",
        "\n",
        "# Step 2: Check if data exists in Drive\n",
        "drive_data_path = Path('/content/drive/MyDrive/streamguard/data/processed/codexglue')\n",
        "print(f\"\\n[2/5] Checking for data in Google Drive...\")\n",
        "print(f\"   Looking in: {drive_data_path}\")\n",
        "\n",
        "if not drive_data_path.exists():\n",
        "    print(f\"\u274c ERROR: Data not found in Google Drive!\")\n",
        "    print(f\"\\n\ud83d\udca1 Please upload the preprocessed data to Google Drive:\")\n",
        "    print(f\"   1. Create folder: My Drive/streamguard/data/processed/codexglue/\")\n",
        "    print(f\"   2. Upload these files:\")\n",
        "    print(f\"      \u2022 train.jsonl (504 MB)\")\n",
        "    print(f\"      \u2022 valid.jsonl (63 MB)\")\n",
        "    print(f\"      \u2022 test.jsonl (63 MB)\")\n",
        "    print(f\"      \u2022 preprocessing_metadata.json (1.6 KB)\")\n",
        "    print(f\"\\n   Total: ~630 MB\")\n",
        "    raise FileNotFoundError(f\"Data not found in Drive: {drive_data_path}\")\n",
        "\n",
        "print(f\"\u2713 Data found in Google Drive\")\n",
        "\n",
        "# Step 3: Check all required files\n",
        "print(f\"\\n[3/5] Verifying data files in Drive...\")\n",
        "required_files = ['train.jsonl', 'valid.jsonl', 'test.jsonl', 'preprocessing_metadata.json']\n",
        "missing_files = []\n",
        "\n",
        "drive_sizes = {}\n",
        "for file in required_files:\n",
        "    file_path = drive_data_path / file\n",
        "    if file_path.exists():\n",
        "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "        drive_sizes[file] = size_mb\n",
        "        print(f\"  \u2713 {file:<30} ({size_mb:>8.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  \u274c {file:<30} MISSING\")\n",
        "        missing_files.append(file)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\n\u274c ERROR: Missing {len(missing_files)} required file(s) in Drive\")\n",
        "    print(f\"   Missing: {', '.join(missing_files)}\")\n",
        "    raise FileNotFoundError(f\"Missing data files in Drive: {missing_files}\")\n",
        "\n",
        "total_size = sum(drive_sizes.values())\n",
        "print(f\"\\n\ud83d\udce6 Total data size in Drive: {total_size:.2f} MB\")\n",
        "\n",
        "# Step 4: Create local data directory and copy files\n",
        "local_data_path = Path('/content/streamguard/data/processed/codexglue')\n",
        "local_data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n[4/5] Copying data from Drive to Colab local storage...\")\n",
        "print(f\"   Source: {drive_data_path}\")\n",
        "print(f\"   Destination: {local_data_path}\")\n",
        "print(f\"   (This provides faster I/O during training)\\n\")\n",
        "\n",
        "for file in required_files:\n",
        "    src = drive_data_path / file\n",
        "    dst = local_data_path / file\n",
        "\n",
        "    if dst.exists():\n",
        "        # Check if sizes match (skip if already copied)\n",
        "        src_size = src.stat().st_size\n",
        "        dst_size = dst.stat().st_size\n",
        "        if src_size == dst_size:\n",
        "            print(f\"  \u2713 {file:<30} (already copied, skipping)\")\n",
        "            continue\n",
        "\n",
        "    print(f\"  \ud83d\udccb Copying {file:<30} ({drive_sizes[file]:.2f} MB)...\", end='', flush=True)\n",
        "    shutil.copy2(src, dst)\n",
        "    print(\" \u2713\")\n",
        "\n",
        "print(f\"\\n\u2705 All data files copied to local storage!\")\n",
        "\n",
        "# Step 5: Load and display metadata\n",
        "print(f\"\\n[5/5] Loading dataset statistics...\")\n",
        "metadata_path = local_data_path / 'preprocessing_metadata.json'\n",
        "if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca Dataset Statistics:\")\n",
        "    total_samples = 0\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        if split in metadata:\n",
        "            count = metadata[split].get('total_samples', 0)\n",
        "            total_samples += count\n",
        "            print(f\"  {split.capitalize():<12}: {count:>6} samples\")\n",
        "\n",
        "    print(f\"\\n\ud83d\udca1 Total samples: {total_samples:,}\")\n",
        "\n",
        "    # Show class distribution if available\n",
        "    if 'train' in metadata and 'label_distribution' in metadata['train']:\n",
        "        dist = metadata['train']['label_distribution']\n",
        "        print(f\"\\n\ud83d\udcca Class Distribution (Training Set):\")\n",
        "        for label, count in dist.items():\n",
        "            percentage = (count / metadata['train']['total_samples']) * 100\n",
        "            print(f\"  {label:<15}: {count:>6} ({percentage:>5.1f}%)\")\n",
        "else:\n",
        "    print(f\"  \u26a0\ufe0f  Metadata file not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 DATA SETUP COMPLETE - Ready for training!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n\ud83d\udca1 Training scripts will read from:\")\n",
        "print(f\"   \u2022 {local_data_path / 'train.jsonl'}\")\n",
        "print(f\"   \u2022 {local_data_path / 'valid.jsonl'}\")\n",
        "print(f\"   \u2022 {local_data_path / 'test.jsonl'}\")\n",
        "print(f\"\\n\ud83d\udcbe Data is now in Colab local storage (faster I/O than Drive)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Production Training Pipeline (v1.7)\n",
        "\n",
        "**Multi-seed training with optimized LR Finder and safety features**\n",
        "\n",
        "### What's New in v1.7:\n",
        "- \u2705 LR Finder runs ONCE (not 3 times) - saves 10-20 minutes\n",
        "- \u2705 Fixed data path bug (valid.jsonl not val.jsonl)\n",
        "- \u2705 Better error handling and progress tracking\n",
        "- \u2705 Automatic F1 score extraction from logs\n",
        "- \u2705 Graph data validation before training\n",
        "\n",
        "### Training Pipeline:\n",
        "1. **Data Validation** - Verify all required data exists\n",
        "2. **Transformer Training** - 3 seeds: [42, 2025, 7] (~40-60 min)\n",
        "3. **GNN Training** - 3 seeds: [42, 2025, 7] (~45-70 min)\n",
        "4. **Results Summary** - Aggregated metrics across seeds\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Data Validation Pre-Flight Check\n",
        "# Verify all required data exists before starting production training\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "print('='*80)\n",
        "print('DATA VALIDATION PRE-FLIGHT CHECK')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "# Check CodeXGLUE data\n",
        "data_files = {\n",
        "    'Training data': 'data/processed/codexglue/train.jsonl',\n",
        "    'Validation data': 'data/processed/codexglue/valid.jsonl',  # FIXED: not val.jsonl\n",
        "    'Test data': 'data/processed/codexglue/test.jsonl'\n",
        "}\n",
        "\n",
        "all_files_exist = True\n",
        "for name, path in data_files.items():\n",
        "    file_path = Path(path)\n",
        "    exists = file_path.exists()\n",
        "    status = '\u2705' if exists else '\u274c'\n",
        "    \n",
        "    if exists:\n",
        "        # Count lines\n",
        "        with open(file_path, 'r') as f:\n",
        "            count = sum(1 for _ in f)\n",
        "        print(f'{status} {name:20s}: {path:50s} ({count:,} samples)')\n",
        "    else:\n",
        "        print(f'{status} {name:20s}: {path:50s} (NOT FOUND)')\n",
        "        all_files_exist = False\n",
        "\n",
        "# Check graph data (optional for GNN)\n",
        "print('\\n' + '-'*80)\n",
        "print('GRAPH DATA (Required for GNN Training)')\n",
        "print('-'*80 + '\\n')\n",
        "\n",
        "graph_train = Path('data/processed/graphs/train')\n",
        "graph_val = Path('data/processed/graphs/val')\n",
        "\n",
        "if graph_train.exists() and graph_val.exists():\n",
        "    print('\u2705 Graph data found:')\n",
        "    print(f'   Train: {graph_train}')\n",
        "    print(f'   Val:   {graph_val}')\n",
        "else:\n",
        "    print('\u26a0\ufe0f  Graph data not found. GNN training will fail.')\n",
        "    print('\\n   To create graph data, run:')\n",
        "    print('   !python training/preprocessing/create_simple_graph_data.py \\\\')\n",
        "    print('     --input data/processed/codexglue/train.jsonl \\\\')\n",
        "    print('     --output data/processed/graphs/train')\n",
        "    print('\\n   (This will be automated in Cell 16 if needed)')\n",
        "\n",
        "# Summary\n",
        "print('\\n' + '='*80)\n",
        "if all_files_exist:\n",
        "    print('\u2705 PRE-FLIGHT CHECK PASSED - Ready for production training!')\n",
        "else:\n",
        "    print('\u274c PRE-FLIGHT CHECK FAILED - Missing required data files')\n",
        "    print('\\n   Please ensure data preprocessing completed successfully.')\n",
        "print('='*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Step 1: Transformer v1.7 Production Training\n",
        "\n",
        "**3-seed training with LR Finder optimization**\n",
        "\n",
        "**Duration:** ~40-60 minutes total\n",
        "- LR Finder: 2-3 min (runs once)\n",
        "- Seed 42: ~12-18 min\n",
        "- Seed 2025: ~12-18 min\n",
        "- Seed 7: ~12-18 min\n",
        "\n",
        "**Key Features:**\n",
        "- Multi-seed training for statistical validity\n",
        "- Optimized LR Finder (runs once, cached for all seeds)\n",
        "- Mixed precision training (faster on A100/V100)\n",
        "- Weighted sampling for class balance\n",
        "- Early stopping on F1 score\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transformer_preset_catalog"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 13: Transformer Training Preset Catalog (Stability-first)\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "_PROJECT_ROOT_HINTS = [\n",
        "    Path('/content/streamguard'),\n",
        "    Path.cwd(),\n",
        "    Path.cwd() / 'streamguard'\n",
        "]\n",
        "\n",
        "if 'locate_project_root' not in globals():\n",
        "    def locate_project_root():\n",
        "        for candidate in _PROJECT_ROOT_HINTS:\n",
        "            candidate = candidate.resolve()\n",
        "            if (candidate / 'training' / 'train_transformer.py').exists():\n",
        "                return candidate\n",
        "        raise FileNotFoundError('Could not locate StreamGuard project root containing training/train_transformer.py')\n",
        "\n",
        "PROJECT_ROOT = locate_project_root() if 'PROJECT_ROOT' not in globals() else Path(PROJECT_ROOT).resolve()\n",
        "TRAIN_DATA = 'data/processed/codexglue/train.jsonl'\n",
        "VAL_DATA = 'data/processed/codexglue/valid.jsonl'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingPreset:\n",
        "    description: str\n",
        "    epochs: int\n",
        "    batch_size: int\n",
        "    lr: float\n",
        "    weight_multiplier: float\n",
        "    dropout: float\n",
        "    warmup_ratio: float\n",
        "    max_seq_len: int\n",
        "    accumulation_steps: int = 1\n",
        "    early_stopping: int = 3\n",
        "    weight_decay: float = 0.01\n",
        "    mixed_precision: bool = True\n",
        "    weighted_sampler: bool = True\n",
        "    code_features: bool = False\n",
        "    quick_test: bool = False\n",
        "    focal_loss: bool = False\n",
        "    lr_override: Optional[float] = None\n",
        "    extra_args: List[str] = field(default_factory=list)\n",
        "    notes: str = ''\n",
        "\n",
        "    def cli_flags(self) -> List[str]:\n",
        "        flags = [\n",
        "            f'--epochs={self.epochs}',\n",
        "            f'--batch-size={self.batch_size}',\n",
        "            f'--lr={self.lr}',\n",
        "            f'--weight-multiplier={self.weight_multiplier}',\n",
        "            f'--dropout={self.dropout}',\n",
        "            f'--warmup-ratio={self.warmup_ratio}',\n",
        "            f'--max-seq-len={self.max_seq_len}',\n",
        "            f'--accumulation-steps={self.accumulation_steps}',\n",
        "            f'--early-stopping-patience={self.early_stopping}',\n",
        "            f'--weight-decay={self.weight_decay}'\n",
        "        ]\n",
        "        if self.lr_override is not None:\n",
        "            flags.append(f'--lr-override={self.lr_override}')\n",
        "        if self.mixed_precision:\n",
        "            flags.append('--mixed-precision')\n",
        "        if self.weighted_sampler:\n",
        "            flags.append('--use-weighted-sampler')\n",
        "        if self.code_features:\n",
        "            flags.append('--use-code-features')\n",
        "        if self.quick_test:\n",
        "            flags.append('--quick-test')\n",
        "        if self.focal_loss:\n",
        "            flags.append('--focal-loss')\n",
        "        flags.extend(self.extra_args)\n",
        "        return flags\n",
        "\n",
        "\n",
        "TRAINING_PRESETS: Dict[str, TrainingPreset] = {\n",
        "    'sanity_fast': TrainingPreset(\n",
        "        description='2-epoch smoke test on 500 samples to confirm the pipeline runs end-to-end.',\n",
        "        epochs=2,\n",
        "        batch_size=16,\n",
        "        lr=1.2e-5,\n",
        "        weight_multiplier=1.3,\n",
        "        dropout=0.15,\n",
        "        warmup_ratio=0.05,\n",
        "        max_seq_len=320,\n",
        "        accumulation_steps=1,\n",
        "        early_stopping=2,\n",
        "        quick_test=True,\n",
        "        notes='Finishes in <10 minutes on a T4 while validating the data/loader stack.'\n",
        "    ),\n",
        "    'balanced_medium': TrainingPreset(\n",
        "        description='Stable full-dataset baseline for 22 GB GPUs (keeps memory <90%).',\n",
        "        epochs=8,\n",
        "        batch_size=24,\n",
        "        lr=2.0e-5,\n",
        "        weight_multiplier=1.5,\n",
        "        dropout=0.20,\n",
        "        warmup_ratio=0.10,\n",
        "        max_seq_len=448,\n",
        "        accumulation_steps=2,\n",
        "        early_stopping=3,\n",
        "        notes='Gradient accumulation (x2) keeps the effective batch \u224848 without triggering OOM.'\n",
        "    ),\n",
        "    'high_recall': TrainingPreset(\n",
        "        description='High-recall preset with stronger weighting + code features.',\n",
        "        epochs=12,\n",
        "        batch_size=20,\n",
        "        lr=1.8e-5,\n",
        "        weight_multiplier=1.8,\n",
        "        dropout=0.25,\n",
        "        warmup_ratio=0.08,\n",
        "        max_seq_len=448,\n",
        "        accumulation_steps=2,\n",
        "        early_stopping=4,\n",
        "        code_features=True,\n",
        "        focal_loss=True,\n",
        "        notes='Prioritize vulnerable recall when auditing critical releases.'\n",
        "    )\n",
        "}\n",
        "\n",
        "DEFAULT_PRESET = 'balanced_medium'\n",
        "\n",
        "print('=' * 80)\n",
        "print('TRANSFORMER TRAINING PRESET CATALOG')\n",
        "print('=' * 80)\n",
        "for name, preset in TRAINING_PRESETS.items():\n",
        "    print()\n",
        "    print(f\"Preset: {name}\")\n",
        "    print(f\"  Description: {preset.description}\")\n",
        "    print(\n",
        "        f\"  Key hyperparameters: batch={preset.batch_size}, epochs={preset.epochs}, \"\n",
        "        f\"lr={preset.lr:.2e}, accumulation={preset.accumulation_steps}, \"\n",
        "        f\"max_seq_len={preset.max_seq_len}, weight_mult={preset.weight_multiplier}\"\n",
        "    )\n",
        "    if preset.notes:\n",
        "        print(f\"  Notes: {preset.notes}\")\n",
        "\n",
        "print()\n",
        "print(f\"Default preset: {DEFAULT_PRESET}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transformer_preset_runner"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 14: Transformer Preset Training Runner (Single/Multi Seed)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import re\n",
        "from datetime import datetime\n",
        "from statistics import mean, pstdev\n",
        "\n",
        "if 'TRAINING_PRESETS' not in globals():\n",
        "    raise RuntimeError('Preset catalog not initialized. Run Cell 13 first.')\n",
        "\n",
        "PRESET_NAME = DEFAULT_PRESET\n",
        "SEEDS = [42]\n",
        "ENABLE_LR_FINDER = False\n",
        "FORCE_LR_FINDER = False\n",
        "EXTRA_FLAGS = []  # e.g. ['--use-code-features']\n",
        "\n",
        "project_root = Path(PROJECT_ROOT).resolve() if 'PROJECT_ROOT' in globals() else locate_project_root()\n",
        "PROJECT_ROOT = project_root\n",
        "if Path.cwd().resolve() != project_root:\n",
        "    os.chdir(project_root)\n",
        "    print(f\"[setup] Working directory set to {project_root}\")\n",
        "\n",
        "for label, rel_path in [('train', TRAIN_DATA), ('validation', VAL_DATA)]:\n",
        "    data_path = project_root / rel_path\n",
        "    if not data_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing {label} data file: {data_path}\")\n",
        "\n",
        "OUTPUT_BASE = project_root / 'training' / 'outputs' / 'transformer_presets' / PRESET_NAME\n",
        "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR = OUTPUT_BASE / '_logs'\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if 'stream_subprocess' not in globals():\n",
        "    def stream_subprocess(name, cmd, log_path):\n",
        "        print('=' * 80)\n",
        "        print(f\"[run] {name}\")\n",
        "        print('=' * 80)\n",
        "        print(' '.join(map(str, cmd)))\n",
        "        print('-' * 80)\n",
        "        log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        captured_lines = []\n",
        "        with log_path.open('w', encoding='utf-8') as log_file:\n",
        "            process = subprocess.Popen(\n",
        "                cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                text=True,\n",
        "                cwd=project_root\n",
        "            )\n",
        "            for line in process.stdout:\n",
        "                print(line, end='')\n",
        "                log_file.write(line)\n",
        "                log_file.flush()\n",
        "                captured_lines.append(line)\n",
        "            return_code = process.wait()\n",
        "        print('-' * 80)\n",
        "        print(f\"[log] Output saved to {log_path}\")\n",
        "        print()\n",
        "        output_text = ''.join(captured_lines)\n",
        "        if return_code != 0:\n",
        "            raise subprocess.CalledProcessError(return_code, cmd, output=output_text)\n",
        "        return output_text\n",
        "\n",
        "preset = TRAINING_PRESETS[PRESET_NAME]\n",
        "print('=' * 80)\n",
        "print('TRANSFORMER PRESET TRAINING')\n",
        "print('=' * 80)\n",
        "print(f\"Preset: {PRESET_NAME} -> {preset.description}\")\n",
        "print(\n",
        "    f\"Preset HP: batch={preset.batch_size}, epochs={preset.epochs}, lr={preset.lr:.2e}, \"\n",
        "    f\"accumulation={preset.accumulation_steps}, max_seq_len={preset.max_seq_len}, weight_mult={preset.weight_multiplier}\"\n",
        ")\n",
        "print(f\"Seeds: {SEEDS}\")\n",
        "print(f\"LR Finder: {'ON' if ENABLE_LR_FINDER else 'OFF'}\")\n",
        "print(f\"Output Base: {OUTPUT_BASE}\")\n",
        "print('=' * 80)\n",
        "print()\n",
        "\n",
        "if not SEEDS:\n",
        "    raise ValueError('SEEDS list must contain at least one value.')\n",
        "\n",
        "results = []\n",
        "f1_pattern = re.compile(\n",
        "    r\"(?:New best model! F1: ([0-9.]+))|(?:Best validation F1 \\(vulnerable\\): ([0-9.]+))\"\n",
        ")\n",
        "\n",
        "for idx, seed in enumerate(SEEDS, start=1):\n",
        "    print('=' * 80)\n",
        "    print(f\"TRAINING WITH SEED: {seed} ({idx}/{len(SEEDS)})\")\n",
        "    print('=' * 80)\n",
        "    print()\n",
        "\n",
        "    seed_output_dir = OUTPUT_BASE / f'seed_{seed}'\n",
        "    seed_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        'training/train_transformer.py',\n",
        "        f'--train-data={TRAIN_DATA}',\n",
        "        f'--val-data={VAL_DATA}',\n",
        "        f'--output-dir={seed_output_dir.as_posix()}',\n",
        "        f'--seed={seed}'\n",
        "    ]\n",
        "    cmd.extend(preset.cli_flags())\n",
        "    if EXTRA_FLAGS:\n",
        "        cmd.extend(EXTRA_FLAGS)\n",
        "    if ENABLE_LR_FINDER:\n",
        "        cmd.append('--find-lr')\n",
        "        if FORCE_LR_FINDER:\n",
        "            cmd.append('--force-find-lr')\n",
        "\n",
        "    train_log = LOG_DIR / f'seed_{seed}.log'\n",
        "    try:\n",
        "        output_text = stream_subprocess(f'Transformer training (preset {PRESET_NAME}, seed {seed})', cmd, train_log)\n",
        "\n",
        "        metrics_path = seed_output_dir / 'metrics.json'\n",
        "        best_f1 = None\n",
        "        if metrics_path.exists():\n",
        "            try:\n",
        "                with metrics_path.open('r', encoding='utf-8') as metrics_file:\n",
        "                    metrics_payload = json.load(metrics_file)\n",
        "                if 'best_f1_vulnerable' in metrics_payload:\n",
        "                    best_f1 = float(metrics_payload['best_f1_vulnerable'])\n",
        "            except Exception as metrics_err:\n",
        "                print(f\"[warn] Failed to read {metrics_path}: {metrics_err}\")\n",
        "\n",
        "        if best_f1 is None:\n",
        "            matches = f1_pattern.findall(output_text)\n",
        "            if matches:\n",
        "                flattened = [value for pair in matches for value in pair if value]\n",
        "                if flattened:\n",
        "                    best_f1 = float(flattened[-1])\n",
        "\n",
        "        if best_f1 is None:\n",
        "            best_f1 = 0.0\n",
        "\n",
        "        results.append({'seed': seed, 'best_f1': best_f1})\n",
        "        print(f\"[ok] Seed {seed} complete. Best F1: {best_f1:.4f}\")\n",
        "    except subprocess.CalledProcessError as exc:\n",
        "        tail = '\\n'.join(exc.output.strip().splitlines()[-25:]) if exc.output else 'No stderr captured.'\n",
        "        print(f\"[error] Training failed for seed {seed} (exit {exc.returncode}).\")\n",
        "        print(f'        See log: {train_log}')\n",
        "        print('        Last log lines:')\n",
        "        print(tail)\n",
        "        results.append({'seed': seed, 'error': f'exit {exc.returncode}'})\n",
        "\n",
        "print('-' * 80)\n",
        "print('PRESET RUN SUMMARY')\n",
        "print('-' * 80)\n",
        "\n",
        "completed = [r for r in results if 'error' not in r]\n",
        "if completed:\n",
        "    f1_scores = [r['best_f1'] for r in completed]\n",
        "    avg = float(mean(f1_scores))\n",
        "    std = float(pstdev(f1_scores)) if len(f1_scores) > 1 else 0.0\n",
        "    for r in completed:\n",
        "        print(f\"  Seed {r['seed']:4d}: F1 = {r['best_f1']:.4f}\")\n",
        "    print()\n",
        "    print(f\"  Mean F1: {avg:.4f} +/- {std:.4f}\")\n",
        "else:\n",
        "    print('[!] All preset runs failed. Check logs for details.')\n",
        "\n",
        "summary = {\n",
        "    'preset': PRESET_NAME,\n",
        "    'description': preset.description,\n",
        "    'seeds': SEEDS,\n",
        "    'results': results,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "if completed:\n",
        "    summary['mean_f1'] = avg\n",
        "    summary['std_f1'] = std\n",
        "\n",
        "summary_path = OUTPUT_BASE / 'preset_summary.json'\n",
        "with summary_path.open('w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print()\n",
        "print(f\"[ok] Preset summary saved to {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 15: Transformer v1.7 Production Training\n",
        "# Multi-seed training with stabilized hyperparameters (LR Finder optional)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "SEEDS = [42, 2025, 7]\n",
        "TRAIN_DATA = 'data/processed/codexglue/train.jsonl'\n",
        "VAL_DATA = 'data/processed/codexglue/valid.jsonl'\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "ACCUMULATION_STEPS = 2\n",
        "BASE_LR = 2.0e-5\n",
        "WEIGHT_MULTIPLIER = 1.6\n",
        "DROPOUT = 0.20\n",
        "WARMUP_RATIO = 0.10\n",
        "MAX_SEQ_LEN = 448\n",
        "EARLY_STOPPING = 3\n",
        "USE_WEIGHTED_SAMPLER = True\n",
        "USE_CODE_FEATURES = False\n",
        "ENABLE_LR_FINDER = False\n",
        "FORCE_LR_FINDER = False\n",
        "LR_FINDER_ITERATIONS = 100\n",
        "\n",
        "if 'locate_project_root' not in globals():\n",
        "    def locate_project_root():\n",
        "        candidates = [\n",
        "            Path('/content/streamguard'),\n",
        "            Path.cwd(),\n",
        "            Path.cwd() / 'streamguard'\n",
        "        ]\n",
        "        for candidate in candidates:\n",
        "            candidate = candidate.resolve()\n",
        "            if (candidate / 'training' / 'train_transformer.py').exists():\n",
        "                return candidate\n",
        "        raise FileNotFoundError('Could not locate StreamGuard project root containing training/train_transformer.py')\n",
        "\n",
        "if 'PROJECT_ROOT' in globals():\n",
        "    PROJECT_ROOT = Path(PROJECT_ROOT).resolve()\n",
        "else:\n",
        "    PROJECT_ROOT = locate_project_root()\n",
        "\n",
        "if Path.cwd().resolve() != PROJECT_ROOT:\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "    print(f\"[setup] Working directory set to {PROJECT_ROOT}\")\n",
        "\n",
        "OUTPUT_DIR = PROJECT_ROOT / 'training' / 'outputs' / 'transformer_v17'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR = OUTPUT_DIR / '_logs'\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_DATA_PATH = PROJECT_ROOT / TRAIN_DATA\n",
        "VAL_DATA_PATH = PROJECT_ROOT / VAL_DATA\n",
        "for label, file_path in [('train', TRAIN_DATA_PATH), ('validation', VAL_DATA_PATH)]:\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing {label} data file: {file_path}\")\n",
        "\n",
        "if 'stream_subprocess' not in globals():\n",
        "    def stream_subprocess(name, cmd, log_path):\n",
        "        print('=' * 80)\n",
        "        print(f\"[run] {name}\")\n",
        "        print('=' * 80)\n",
        "        print(' '.join(map(str, cmd)))\n",
        "        print('-' * 80)\n",
        "        log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        captured_lines = []\n",
        "        with log_path.open('w', encoding='utf-8') as log_file:\n",
        "            process = subprocess.Popen(\n",
        "                cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                text=True,\n",
        "                cwd=PROJECT_ROOT\n",
        "            )\n",
        "            for line in process.stdout:\n",
        "                print(line, end='')\n",
        "                log_file.write(line)\n",
        "                log_file.flush()\n",
        "                captured_lines.append(line)\n",
        "            return_code = process.wait()\n",
        "        print('-' * 80)\n",
        "        print(f\"[log] Output saved to {log_path}\")\n",
        "        print()\n",
        "        output_text = ''.join(captured_lines)\n",
        "        if return_code != 0:\n",
        "            raise subprocess.CalledProcessError(return_code, cmd, output=output_text)\n",
        "        return output_text\n",
        "\n",
        "print('=' * 80)\n",
        "print('TRANSFORMER v1.7 PRODUCTION TRAINING')\n",
        "print('=' * 80)\n",
        "print(f\"Seeds: {SEEDS}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Config: epochs={EPOCHS}, batch={BATCH_SIZE}, accumulation={ACCUMULATION_STEPS}, max_seq_len={MAX_SEQ_LEN}\")\n",
        "print(f\"LR finder: {'ENABLED' if ENABLE_LR_FINDER else 'DISABLED'}\")\n",
        "print('=' * 80)\n",
        "print()\n",
        "\n",
        "if ENABLE_LR_FINDER:\n",
        "    print('[1/2] Running LR Finder (once for all seeds)...')\n",
        "    print()\n",
        "\n",
        "    lr_finder_cmd = [\n",
        "        sys.executable,\n",
        "        'training/train_transformer.py',\n",
        "        f'--train-data={TRAIN_DATA}',\n",
        "        f'--val-data={VAL_DATA}',\n",
        "        '--output-dir=training/outputs/.lr_finder_temp',\n",
        "        '--quick-test',\n",
        "        '--find-lr',\n",
        "        f'--lr-finder-iterations={LR_FINDER_ITERATIONS}',\n",
        "        '--epochs=5',\n",
        "        '--batch-size=16',\n",
        "        '--seed=42'\n",
        "    ]\n",
        "    if FORCE_LR_FINDER:\n",
        "        lr_finder_cmd.append('--force-find-lr')\n",
        "\n",
        "    lr_finder_log = LOG_DIR / 'lr_finder.log'\n",
        "    try:\n",
        "        stream_subprocess('LR Finder (quick run)', lr_finder_cmd, lr_finder_log)\n",
        "        print('[ok] LR Finder complete. Cache ready for all seeds.')\n",
        "        print()\n",
        "    except subprocess.CalledProcessError as exc:\n",
        "        print(f\"[warn] LR Finder failed with exit code {exc.returncode}. See {lr_finder_log} for details.\")\n",
        "        print('       Continuing with BASE_LR fallback.')\n",
        "        print()\n",
        "\n",
        "    training_step_label = '[2/2]'\n",
        "else:\n",
        "    print('[1/1] LR Finder disabled \u2014 starting training with preset hyperparameters.')\n",
        "    print()\n",
        "    training_step_label = '[1/1]'\n",
        "\n",
        "print(f\"{training_step_label} Training all seeds with stabilized config...\")\n",
        "print()\n",
        "\n",
        "results_all_seeds = []\n",
        "f1_pattern = re.compile(r'Best F1: ([0-9.]+)')\n",
        "\n",
        "for seed_idx, seed in enumerate(SEEDS, start=1):\n",
        "    print('=' * 80)\n",
        "    print(f\"TRAINING WITH SEED: {seed} ({seed_idx}/{len(SEEDS)})\")\n",
        "    print('=' * 80)\n",
        "    print()\n",
        "\n",
        "    seed_output_dir = OUTPUT_DIR / f'seed_{seed}'\n",
        "    seed_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        'training/train_transformer.py',\n",
        "        f'--train-data={TRAIN_DATA}',\n",
        "        f'--val-data={VAL_DATA}',\n",
        "        f'--output-dir={seed_output_dir.as_posix()}',\n",
        "        f'--seed={seed}',\n",
        "        f'--epochs={EPOCHS}',\n",
        "        f'--batch-size={BATCH_SIZE}',\n",
        "        f'--accumulation-steps={ACCUMULATION_STEPS}',\n",
        "        f'--lr={BASE_LR}',\n",
        "        f'--weight-multiplier={WEIGHT_MULTIPLIER}',\n",
        "        f'--dropout={DROPOUT}',\n",
        "        f'--warmup-ratio={WARMUP_RATIO}',\n",
        "        f'--max-seq-len={MAX_SEQ_LEN}',\n",
        "        f'--early-stopping-patience={EARLY_STOPPING}'\n",
        "    ]\n",
        "\n",
        "    cmd.append('--mixed-precision')\n",
        "    if USE_WEIGHTED_SAMPLER:\n",
        "        cmd.append('--use-weighted-sampler')\n",
        "    if USE_CODE_FEATURES:\n",
        "        cmd.append('--use-code-features')\n",
        "    if ENABLE_LR_FINDER:\n",
        "        cmd.append('--find-lr')\n",
        "        cmd.append(f'--lr-finder-iterations={LR_FINDER_ITERATIONS}')\n",
        "        if FORCE_LR_FINDER:\n",
        "            cmd.append('--force-find-lr')\n",
        "\n",
        "    train_log = LOG_DIR / f'seed_{seed}.log'\n",
        "    try:\n",
        "        output_text = stream_subprocess(f'Transformer training (seed {seed})', cmd, train_log)\n",
        "        matches = f1_pattern.findall(output_text)\n",
        "        best_f1 = float(matches[-1]) if matches else 0.0\n",
        "        results_all_seeds.append({'seed': seed, 'best_f1': best_f1})\n",
        "        print(f\"[ok] Seed {seed} complete. Best F1: {best_f1:.4f}\")\n",
        "    except subprocess.CalledProcessError as exc:\n",
        "        tail = '\\n'.join(exc.output.strip().splitlines()[-25:]) if exc.output else 'No stderr captured.'\n",
        "        print(f\"[error] Training failed for seed {seed} (exit {exc.returncode}).\")\n",
        "        print(f'        See log: {train_log}')\n",
        "        print('        Last log lines:')\n",
        "        print(tail)\n",
        "        results_all_seeds.append({'seed': seed, 'error': f'exit {exc.returncode}'})\n",
        "\n",
        "print('=' * 80)\n",
        "print('TRANSFORMER TRAINING COMPLETE')\n",
        "print('=' * 80)\n",
        "print()\n",
        "\n",
        "valid_results = [r for r in results_all_seeds if 'error' not in r]\n",
        "if valid_results:\n",
        "    f1_scores = [r['best_f1'] for r in valid_results]\n",
        "    mean_f1 = float(np.mean(f1_scores))\n",
        "    std_f1 = float(np.std(f1_scores))\n",
        "\n",
        "    print(f\"Results across {len(valid_results)} completed seeds:\")\n",
        "    for r in valid_results:\n",
        "        print(f\"  Seed {r['seed']:4d}: F1 = {r['best_f1']:.4f}\")\n",
        "    print()\n",
        "    print(f\"Mean F1: {mean_f1:.4f} +/- {std_f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "    summary = {\n",
        "        'model': 'transformer_v17',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'seeds': SEEDS,\n",
        "        'results': results_all_seeds,\n",
        "        'mean_f1': mean_f1,\n",
        "        'std_f1': std_f1,\n",
        "        'config': {\n",
        "            'epochs': EPOCHS,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'accumulation_steps': ACCUMULATION_STEPS,\n",
        "            'base_lr': BASE_LR,\n",
        "            'weight_multiplier': WEIGHT_MULTIPLIER,\n",
        "            'dropout': DROPOUT,\n",
        "            'warmup_ratio': WARMUP_RATIO,\n",
        "            'max_seq_len': MAX_SEQ_LEN,\n",
        "            'use_weighted_sampler': USE_WEIGHTED_SAMPLER,\n",
        "            'use_code_features': USE_CODE_FEATURES,\n",
        "            'enable_lr_finder': ENABLE_LR_FINDER\n",
        "        }\n",
        "    }\n",
        "\n",
        "    summary_path = OUTPUT_DIR / 'production_summary.json'\n",
        "    with summary_path.open('w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"[ok] Summary saved to {summary_path}\")\n",
        "else:\n",
        "    print(f\"[error] All seeds failed. Review logs under {LOG_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Step 2: GNN v1.7 Production Training\n",
        "\n",
        "**Graph-based vulnerability detection with multi-seed training**\n",
        "\n",
        "**Duration:** ~45-70 minutes total\n",
        "- Graph preprocessing: ~5-10 min (if needed)\n",
        "- LR Finder: 2-3 min (runs once)\n",
        "- Seed 42: ~13-20 min\n",
        "- Seed 2025: ~13-20 min\n",
        "- Seed 7: ~13-20 min\n",
        "\n",
        "**Key Features:**\n",
        "- Graph Neural Network architecture\n",
        "- Focal loss for hard negatives\n",
        "- Weighted sampling + focal loss combination\n",
        "- 15 epochs per seed (vs 10 for Transformer)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 16: Graph Data Preprocessing (Conditional)\n",
        "# Check if graph data exists, if not run preprocessing\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print('='*80)\n",
        "print('GRAPH DATA PREPROCESSING CHECK')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "graph_train = Path('data/processed/graphs/train')\n",
        "graph_val = Path('data/processed/graphs/val')\n",
        "\n",
        "if graph_train.exists() and graph_val.exists():\n",
        "    print('\u2705 Graph data already exists. Skipping preprocessing.\\n')\n",
        "    print(f'   Train: {graph_train}')\n",
        "    print(f'   Val:   {graph_val}')\n",
        "else:\n",
        "    print('\u26a0\ufe0f  Graph data not found. Running preprocessing...\\n')\n",
        "    \n",
        "    # Create graph data for training set\n",
        "    print('[1/2] Creating graph data for training set...')\n",
        "    cmd_train = [\n",
        "        sys.executable,\n",
        "        'training/preprocessing/create_simple_graph_data.py',\n",
        "        '--input', 'data/processed/codexglue/train.jsonl',\n",
        "        '--output', 'data/processed/graphs/train'\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        subprocess.run(cmd_train, check=True)\n",
        "        print('\u2705 Training graph data created\\n')\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f'\u274c Failed to create training graph data: {e}\\n')\n",
        "        raise\n",
        "    \n",
        "    # Create graph data for validation set\n",
        "    print('[2/2] Creating graph data for validation set...')\n",
        "    cmd_val = [\n",
        "        sys.executable,\n",
        "        'training/preprocessing/create_simple_graph_data.py',\n",
        "        '--input', 'data/processed/codexglue/valid.jsonl',\n",
        "        '--output', 'data/processed/graphs/val'\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        subprocess.run(cmd_val, check=True)\n",
        "        print('\u2705 Validation graph data created\\n')\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f'\u274c Failed to create validation graph data: {e}\\n')\n",
        "        raise\n",
        "    \n",
        "    print('\\n\u2705 Graph preprocessing complete!')\n",
        "\n",
        "print('='*80)\n",
        "print('READY FOR GNN TRAINING')\n",
        "print('='*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: GNN v1.7 Production Training\n",
        "# Multi-seed GNN training with optimized LR Finder\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Configuration\n",
        "SEEDS = [42, 2025, 7]\n",
        "OUTPUT_DIR = Path('training/outputs/gnn_v17')\n",
        "TRAIN_DATA = 'data/processed/graphs/train'\n",
        "VAL_DATA = 'data/processed/graphs/val'\n",
        "\n",
        "# Verify graph data exists\n",
        "if not Path(TRAIN_DATA).exists():\n",
        "    print(f'\u274c Graph training data not found: {TRAIN_DATA}')\n",
        "    print('Run Cell 16 first to create graph data.')\n",
        "    raise FileNotFoundError(f'Graph data not found: {TRAIN_DATA}')\n",
        "\n",
        "print('='*80)\n",
        "print('GNN v1.7 PRODUCTION TRAINING')\n",
        "print('='*80)\n",
        "print(f'Seeds: {SEEDS}')\n",
        "print(f'Output: {OUTPUT_DIR}')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Run LR Finder ONCE (quick mode, ~2-3 min)\n",
        "# ============================================================\n",
        "print('[1/2] Running LR Finder (once for all seeds)...\\n')\n",
        "\n",
        "lr_finder_cmd = [\n",
        "    sys.executable,\n",
        "    'training/train_gnn.py',\n",
        "    f'--train-data={TRAIN_DATA}',\n",
        "    f'--val-data={VAL_DATA}',\n",
        "    '--output-dir=training/outputs/.lr_finder_temp_gnn',\n",
        "    '--quick-test',\n",
        "    '--find-lr',\n",
        "    '--force-find-lr',\n",
        "    '--epochs=5',\n",
        "    '--batch-size=16',\n",
        "    '--seed=42'\n",
        "]\n",
        "\n",
        "try:\n",
        "    print('\u23f3 Running LR Finder on 64 samples...')\n",
        "    result = subprocess.run(lr_finder_cmd, check=True, capture_output=True, text=True)\n",
        "    print('\u2705 LR Finder complete! LR cached for all seeds.\\n')\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f'\u26a0\ufe0f  LR Finder failed: {e}')\n",
        "    print('Continuing with default LR (1e-3)...\\n')\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Train all seeds (uses cached LR)\n",
        "# ============================================================\n",
        "print('[2/2] Training all seeds with cached LR...\\n')\n",
        "\n",
        "results_all_seeds = []\n",
        "\n",
        "for seed_idx, seed in enumerate(SEEDS):\n",
        "    print(f'\\n{\"=\"*80}')\n",
        "    print(f'TRAINING WITH SEED: {seed} ({seed_idx + 1}/{len(SEEDS)})')\n",
        "    print(f'{\"=\"*80}\\n')\n",
        "    \n",
        "    seed_output_dir = OUTPUT_DIR / f'seed_{seed}'\n",
        "    seed_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        'training/train_gnn.py',\n",
        "        f'--train-data={TRAIN_DATA}',\n",
        "        f'--val-data={VAL_DATA}',\n",
        "        f'--output-dir={seed_output_dir}',\n",
        "        f'--seed={seed}',\n",
        "        '--epochs=15',\n",
        "        '--batch-size=64',\n",
        "        '--mixed-precision',\n",
        "        '--find-lr',  # Uses cache (instant)\n",
        "        '--use-weighted-sampler',\n",
        "        '--focal-loss'\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "        \n",
        "        # Extract F1 from logs\n",
        "        best_f1 = 0.0\n",
        "        f1_pattern = r'Best F1: ([0-9.]+)'\n",
        "        matches = re.findall(f1_pattern, result.stdout)\n",
        "        if matches:\n",
        "            best_f1 = float(matches[-1])\n",
        "        \n",
        "        results_all_seeds.append({'seed': seed, 'best_f1': best_f1})\n",
        "        print(f'\\n\u2705 Seed {seed} complete. Best F1: {best_f1:.4f}')\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f'\\n\u274c Training failed for seed {seed}: {e}')\n",
        "        results_all_seeds.append({'seed': seed, 'error': str(e)})\n",
        "\n",
        "# ============================================================\n",
        "# Aggregate\n",
        "# ============================================================\n",
        "print(f'\\n{\"=\"*80}')\n",
        "print('GNN TRAINING COMPLETE')\n",
        "print(f'{\"=\"*80}\\n')\n",
        "\n",
        "valid_results = [r for r in results_all_seeds if 'error' not in r]\n",
        "if valid_results:\n",
        "    f1_scores = [r['best_f1'] for r in valid_results]\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "    std_f1 = np.std(f1_scores)\n",
        "    \n",
        "    print(f'\ud83d\udcca Results across {len(valid_results)} seeds:')\n",
        "    for r in valid_results:\n",
        "        print(f\"  Seed {r['seed']:4d}: F1 = {r['best_f1']:.4f}\")\n",
        "    print(f'\\n\ud83d\udcc8 Mean F1: {mean_f1:.4f} \u00b1 {std_f1:.4f}')\n",
        "    \n",
        "    summary = {\n",
        "        'model': 'gnn_v17',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'seeds': SEEDS,\n",
        "        'results': results_all_seeds,\n",
        "        'mean_f1': float(mean_f1),\n",
        "        'std_f1': float(std_f1)\n",
        "    }\n",
        "    \n",
        "    summary_path = OUTPUT_DIR / 'production_summary.json'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f'\\n\ud83d\udcbe Summary saved: {summary_path}')\n",
        "else:\n",
        "    print('\u274c All seeds failed. Check errors above.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83c\udf89 Production Training Complete!\n",
        "\n",
        "### Results Summary:\n",
        "\n",
        "**Transformer v1.7:**\n",
        "- Results: `training/outputs/transformer_v17/production_summary.json`\n",
        "- Checkpoints: `training/outputs/transformer_v17/seed_*/best_model.pt`\n",
        "\n",
        "**GNN v1.7:**\n",
        "- Results: `training/outputs/gnn_v17/production_summary.json`\n",
        "- Checkpoints: `training/outputs/gnn_v17/seed_*/best_model.pt`\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **View Results:** Run Cell 20 to see aggregated metrics\n",
        "2. **Fusion Training (Optional):** See Cell 23 for instructions\n",
        "3. **Model Deployment:** Use best checkpoints for inference\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Optional Advanced Features\n",
        "\n",
        "The following cells are optional:\n",
        "- **Cell 20:** View detailed training results\n",
        "- **Cell 21:** Run LR Finder safety validation test\n",
        "- **Cell 22-23:** Fusion training instructions (requires Transformer + GNN checkpoints)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 20: View Detailed Training Results\n",
        "# Load and display production summaries\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print('='*80)\n",
        "print('PRODUCTION TRAINING RESULTS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "# Transformer results\n",
        "transformer_summary = Path('training/outputs/transformer_v17/production_summary.json')\n",
        "if transformer_summary.exists():\n",
        "    with open(transformer_summary, 'r') as f:\n",
        "        t_data = json.load(f)\n",
        "    \n",
        "    print('\ud83d\udcca TRANSFORMER v1.7')\n",
        "    print('-'*80)\n",
        "    print(f\"Seeds trained: {t_data['seeds']}\")\n",
        "    print(f\"Mean F1: {t_data['mean_f1']:.4f} \u00b1 {t_data['std_f1']:.4f}\")\n",
        "    print('\\nPer-seed results:')\n",
        "    for r in t_data['results']:\n",
        "        if 'error' not in r:\n",
        "            print(f\"  Seed {r['seed']:4d}: F1 = {r['best_f1']:.4f}\")\n",
        "        else:\n",
        "            print(f\"  Seed {r['seed']:4d}: FAILED\")\n",
        "else:\n",
        "    print('\u26a0\ufe0f  Transformer results not found. Run Cell 14 first.')\n",
        "\n",
        "print('\\n' + '='*80 + '\\n')\n",
        "\n",
        "# GNN results\n",
        "gnn_summary = Path('training/outputs/gnn_v17/production_summary.json')\n",
        "if gnn_summary.exists():\n",
        "    with open(gnn_summary, 'r') as f:\n",
        "        g_data = json.load(f)\n",
        "    \n",
        "    print('\ud83d\udcca GNN v1.7')\n",
        "    print('-'*80)\n",
        "    print(f\"Seeds trained: {g_data['seeds']}\")\n",
        "    print(f\"Mean F1: {g_data['mean_f1']:.4f} \u00b1 {g_data['std_f1']:.4f}\")\n",
        "    print('\\nPer-seed results:')\n",
        "    for r in g_data['results']:\n",
        "        if 'error' not in r:\n",
        "            print(f\"  Seed {r['seed']:4d}: F1 = {r['best_f1']:.4f}\")\n",
        "        else:\n",
        "            print(f\"  Seed {r['seed']:4d}: FAILED\")\n",
        "else:\n",
        "    print('\u26a0\ufe0f  GNN results not found. Run Cell 17 first.')\n",
        "\n",
        "print('\\n' + '='*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 21: LR Finder Safety Validation Test (Optional)\n",
        "# Quick test of LR Finder with safety validation on small subset (2-3 min)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print('='*70)\n",
        "print('LR FINDER SAFETY VALIDATION TEST')\n",
        "print('='*70)\n",
        "print('Testing LR Finder with safety validation on 64 samples')\n",
        "print('Duration: ~2-3 minutes')\n",
        "print('='*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --quick-test \\\n",
        "  --find-lr \\\n",
        "  --epochs 5 \\\n",
        "  --batch-size 16 \\\n",
        "  --seed 42\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('\u2705 LR Finder test complete!')\n",
        "print('='*70)\n",
        "print('\\n\ud83d\udccb Check the output above for:')\n",
        "print('  \u2022 LR Finder curve analysis (confidence: high/medium/low)')\n",
        "print('  \u2022 Safety validation (cap applied? fallback used?)')\n",
        "print('  \u2022 Suggested LR and final used LR')\n",
        "print('  \u2022 Cache saved for future runs')\n",
        "print('='*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Optional: Fusion Training\n",
        "\n",
        "**Fusion training combines Transformer + GNN models**\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed Transformer training (Cell 14)\n",
        "- Completed GNN training (Cell 17)\n",
        "\n",
        "**Note:** Fusion training is currently a manual process. Cell 23 will check prerequisites and provide the command to run.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 23: Fusion Training Instructions (Optional)\n",
        "# Check prerequisites and provide manual fusion training command\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "print('='*80)\n",
        "print('FUSION v1.7 TRAINING PREREQUISITES')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "transformer_checkpoint = Path('training/outputs/transformer_v17/seed_42/best_model.pt')\n",
        "gnn_checkpoint = Path('training/outputs/gnn_v17/seed_42/best_model.pt')\n",
        "\n",
        "transformer_ok = transformer_checkpoint.exists()\n",
        "gnn_ok = gnn_checkpoint.exists()\n",
        "\n",
        "print(f'{'\u2705' if transformer_ok else '\u274c'} Transformer checkpoint: {transformer_checkpoint}')\n",
        "print(f'{'\u2705' if gnn_ok else '\u274c'} GNN checkpoint: {gnn_checkpoint}')\n",
        "\n",
        "if not transformer_ok:\n",
        "    print('\\n\u26a0\ufe0f  Transformer checkpoint not found. Run Cell 14 first.')\n",
        "elif not gnn_ok:\n",
        "    print('\\n\u26a0\ufe0f  GNN checkpoint not found. Run Cell 17 first.')\n",
        "else:\n",
        "    print('\\n\u2705 Prerequisites met! Ready for fusion training.')\n",
        "    print('\\n' + '='*80)\n",
        "    print('FUSION TRAINING COMMAND')\n",
        "    print('='*80 + '\\n')\n",
        "    print('Run the following command to train fusion model:')\n",
        "    print('\\n```bash')\n",
        "    print('!python training/train_fusion.py \\\\')\n",
        "    print('  --train-data data/processed/codexglue/train.jsonl \\\\')\n",
        "    print('  --val-data data/processed/codexglue/valid.jsonl \\\\')\n",
        "    print(f'  --transformer-checkpoint {transformer_checkpoint} \\\\')\n",
        "    print(f'  --gnn-checkpoint {gnn_checkpoint} \\\\')\n",
        "    print('  --output-dir training/outputs/fusion_v17/seed_42 \\\\')\n",
        "    print('  --epochs 12 \\\\')\n",
        "    print('  --batch-size 32')\n",
        "    print('```')\n",
        "    print('\\n' + '='*80)\n",
        "    print('Note: Fusion training is optional. Transformer and GNN models')\n",
        "    print('can be used independently for inference.')\n",
        "    print('='*80)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}