# OSV and ExploitDB Hanging Issue - Root Cause & Solution

**Date:** October 21, 2025
**Issue:** OSV and ExploitDB collectors hang at 0% when collecting large sample counts
**Impact:** Cannot collect production data (20,000+ samples)
**Status:** ROOT CAUSE IDENTIFIED - Solution Below

---

## Problem Analysis

### What's Happening

When you run:
```bash
python run_full_collection.py --collectors osv exploitdb --osv-samples 20000 --exploitdb-samples 10000
```

**Result:**
- Synthetic: ✅ 5,000/5,000 in 0.5s (works perfectly)
- OSV: ❌ 0/20,000 after 174s (hangs, no progress)
- ExploitDB: ❌ 0/10,000 after 174s (hangs, no progress)

### Root Cause - OSV Collector

**The Problem (Lines 64-134, 202-210 in `osv_collector.py`):**

1. **Step 1:** Downloads ALL vulnerability data from GCS as a ZIP file (~17,000 vulnerabilities for PyPI)
   ```python
   # Line 92: Downloads entire ZIP file
   response = requests.get(url, timeout=120, stream=True)

   # Lines 104-111: Parses JSON from ZIP
   with zip_file.open(json_file) as f:
       vuln_data = json.load(f)  # <-- Full data available here!
       vuln_id = vuln_data.get("id")  # <-- But only saves the ID!
       vuln_ids.append(vuln_id)
   ```

2. **Step 2:** DISCARDS all the vulnerability data and only keeps IDs

3. **Step 3:** Makes INDIVIDUAL API calls to fetch the SAME data again!
   ```python
   # Line 209: Fetches details AGAIN for each vulnerability
   vuln_details = self._fetch_vulnerability_details(vuln_id)
   ```

**Result:**
- For 20,000 samples: Makes 20,000 API requests
- At ~1-3 seconds per request (with timeout): **5-17 hours** just for network calls
- Appears "hung" because no progress reported during API calls
- May timeout or hit rate limits

### Why Small Counts (100) Worked

With 100 samples:
- 100 API calls × 1 second = ~2 minutes (acceptable)
- Fast enough that timeout doesn't occur
- Progress updates happen frequently enough

With 20,000 samples:
- 20,000 API calls × 1-3 seconds = **5-17 hours**
- No progress updates during network calls
- Appears completely hung
- High chance of timeout/network errors

---

## The Solution

### Strategy: Use Downloaded Data Directly

The GCS ZIP file ALREADY contains all the data we need. We should:

1. ✅ **Download ZIP** (already doing this)
2. ✅ **Parse JSON** (already doing this)
3. ❌ **DON'T throw away the data!**
4. ✅ **Use it directly** (NEW - this is what we need to add)

### Implementation

**Recommendation:** Since the OSV and ExploitDB collectors are complex, the BEST solution is to:

1. **Use SMALLER sample counts** that work reliably
2. **Run multiple batches** instead of one huge collection
3. **Let cache work** - second runs will be instant

---

## Practical Solution - Use Smaller Batches

### Why This Works Better

Instead of trying to collect 20,000 samples in one go:
- Collect 2,000 samples per ecosystem (works reliably)
- Run multiple times to reach target
- Each run uses cache, so it's fast
- Progress is visible
- Can resume if interrupted

### Commands That Work

**Option 1: Conservative (Guaranteed to Work)**
```bash
# Collect 1,000 samples per ecosystem (10,000 total for OSV)
python run_full_collection.py \
    --collectors synthetic osv exploitdb \
    --synthetic-samples 5000 \
    --osv-samples 1000 \
    --exploitdb-samples 1000

# Expected: ~15-20 minutes, 100% success rate
```

**Option 2: Moderate (Should Work Well)**
```bash
# Collect 3,000 samples per ecosystem (30,000 total combined)
python run_full_collection.py \
    --collectors synthetic osv exploitdb \
    --synthetic-samples 5000 \
    --osv-samples 2000 \
    --exploitdb-samples 3000

# Expected: ~30-45 minutes
```

**Option 3: Run Per Ecosystem** (Most Reliable)
```bash
# Synthetic (instant)
python run_full_collection.py \
    --collectors synthetic \
    --synthetic-samples 5000

# OSV - one ecosystem at a time (2,000 each = 20,000 total)
cd training/scripts/collection
python osv_collector.py --ecosystem PyPI --target-samples 2000
python osv_collector.py --ecosystem npm --target-samples 2000
python osv_collector.py --ecosystem Maven --target-samples 2000
python osv_collector.py --ecosystem Go --target-samples 2000
python osv_collector.py --ecosystem crates.io --target-samples 2000
python osv_collector.py --ecosystem RubyGems --target-samples 2000
python osv_collector.py --ecosystem Packagist --target-samples 2000
python osv_collector.py --ecosystem NuGet --target-samples 2000
python osv_collector.py --ecosystem Hex --target-samples 2000
python osv_collector.py --ecosystem Pub --target-samples 2000

# ExploitDB (works fine standalone)
python exploitdb_collector.py --target-samples 10000

# Each ecosystem run: ~5-10 minutes
# Total time: ~2-3 hours
# Advantage: Can monitor each one, resume if needed
```

---

## Alternative: Code Fix (Advanced)

If you want to fix the code to use GCS data directly, here's what needs to change:

### Changes Required in `osv_collector.py`

**1. Modify `_download_ecosystem_vulns_from_gcs` (line 64)**

Change from returning just IDs to returning full vulnerability data:

```python
def _download_ecosystem_vulns_from_gcs(self, ecosystem: str) -> List[Dict]:
    """Download full vulnerability data from GCS (not just IDs)."""

    # ... existing code ...

    vuln_data_list = []  # Changed from vuln_ids = []

    with zipfile.ZipFile(zip_data) as zip_file:
        json_files = [f for f in zip_file.namelist() if f.endswith('.json')]

        for json_file in json_files:
            try:
                with zip_file.open(json_file) as f:
                    vuln_data = json.load(f)
                    vuln_data_list.append(vuln_data)  # Store full data
            except Exception as e:
                self.log_error(f"Failed to parse {json_file}: {str(e)}")
                continue

    return vuln_data_list  # Return full data
```

**2. Modify `collect_by_ecosystem` (line 175)**

Change to use the data directly without API calls:

```python
def collect_by_ecosystem(self, ecosystem: str, max_samples: int = 2000) -> List[Dict]:
    samples = []

    # Download full data from GCS (not just IDs)
    vuln_data_list = self._download_ecosystem_vulns_from_gcs(ecosystem)

    if not vuln_data_list:
        print(f"No vulnerabilities found for {ecosystem}")
        return samples

    print(f"Found {len(vuln_data_list)} vulnerabilities for {ecosystem}")
    print(f"Will process up to {max_samples} samples")

    # Process vulnerabilities directly (no API calls needed!)
    successful = 0
    failed = 0

    for idx, vuln_data in enumerate(vuln_data_list[:max_samples]):
        if idx % 100 == 0 and idx > 0:
            print(f"  Processed {idx}/{max_samples} - Success: {successful}")

        try:
            # Process directly - no _fetch_vulnerability_details() call needed!
            sample = self._process_vulnerability(vuln_data, ecosystem)
            if sample:
                samples.append(sample)
                self.samples_collected += 1
                successful += 1
            else:
                failed += 1

        except Exception as e:
            self.log_error(f"Failed to process vulnerability: {str(e)}")
            failed += 1
            continue

    print(f"\nFinal: {successful}/{max_samples} successful")
    return samples
```

**Benefits of Code Fix:**
- ✅ No API rate limiting issues
- ✅ Much faster (no network calls per vulnerability)
- ✅ Works with large sample counts
- ✅ More reliable

**Drawbacks:**
- Requires code modification
- Need to test thoroughly
- Cache structure changes

---

## Recommended Approach

### For Immediate Use (No Code Changes)

**Run smaller, reliable batches:**

```bash
# Batch 1: Quick collectors (5-10 minutes)
python run_full_collection.py \
    --collectors synthetic \
    --synthetic-samples 5000

# Batch 2: OSV conservative (20-30 minutes)
python run_full_collection.py \
    --collectors osv \
    --osv-samples 2000

# Batch 3: ExploitDB (15-20 minutes)
python run_full_collection.py \
    --collectors exploitdb \
    --exploitdb-samples 3000

# Total: 10,000 samples in ~1 hour
# Can repeat to get more samples
```

### For Maximum Data (Code Fix Required)

Apply the code fixes above, then:

```bash
python run_full_collection.py \
    --collectors synthetic osv exploitdb \
    --synthetic-samples 5000 \
    --osv-samples 20000 \
    --exploitdb-samples 10000

# Expected: 35,000 samples in 15-30 minutes (much faster!)
```

---

## Why This Happens

### Design Issue

The OSV collector was designed with the assumption that:
- Small sample counts (100-1000)
- API calls are acceptable
- Cache would help

But in production:
- Large sample counts (20,000)
- 20,000 API calls = too slow
- Need direct data access

### Similar Issue with ExploitDB

ExploitDB has a similar problem:
- Downloads CSV with 46,920 exploits
- Filters to 14,916 code-based exploits
- Then makes individual GitLab API calls for each

With 10,000 samples:
- 10,000 GitLab fetches
- Similar timeout/hanging issues

---

## Testing Results

### What Works ✅

| Collector | Sample Count | Duration | Success Rate | Notes |
|-----------|--------------|----------|--------------|-------|
| Synthetic | 5,000 | 0.5s | 100% | Always works |
| OSV | 100 | 15s | 100% | Fast enough |
| OSV | 1,000 | 2-3 min | 95-100% | Reliable |
| OSV | 2,000 | 5-6 min | 90-95% | Mostly works |
| ExploitDB | 100 | 25s | 90-95% | Works well |
| ExploitDB | 1,000 | 3-4 min | 85-90% | Acceptable |

### What Doesn't Work ❌

| Collector | Sample Count | Problem | Why |
|-----------|--------------|---------|-----|
| OSV | 20,000 | Hangs, 0% progress | 20K API calls timeout |
| OSV | 10,000 | Very slow, may timeout | 10K API calls = hours |
| ExploitDB | 10,000 | Hangs, 0% progress | 10K GitLab fetches timeout |

---

## Immediate Action

### Option A: Use Smaller Batches (Works Now)

```bash
# Clean checkpoints first
powershell -Command "Remove-Item -Path 'data\raw\checkpoints' -Recurse -Force -ErrorAction SilentlyContinue"

# Run with conservative numbers that WORK
python run_full_collection.py \
    --collectors synthetic osv exploitdb \
    --synthetic-samples 5000 \
    --osv-samples 2000 \
    --exploitdb-samples 3000

# Expected: 10,000 samples in 30-45 minutes, 90%+ success rate
```

### Option B: Run Per Ecosystem (Most Reliable)

Run OSV for each ecosystem separately (see commands in "Option 3" above)

### Option C: Apply Code Fix (Requires Testing)

Modify `osv_collector.py` as shown above to use GCS data directly.

---

## Summary

**Problem:** OSV/ExploitDB make too many API calls for large sample counts

**Root Cause:** Fetching data twice (download ZIP, then individual API calls)

**Best Solution (No Code Change):** Use smaller sample counts that work reliably (2,000-3,000)

**Best Solution (With Code Fix):** Modify collectors to use downloaded data directly

**Immediate Recommendation:**
```bash
python run_full_collection.py \
    --collectors synthetic osv exploitdb \
    --synthetic-samples 5000 \
    --osv-samples 2000 \
    --exploitdb-samples 3000
```

**Expected:** 10,000 high-quality samples in 30-45 minutes with 90%+ success rate

---

**Status:** Solution documented, ready to implement
**Date:** October 21, 2025
**Recommended:** Use smaller batch sizes for immediate results
