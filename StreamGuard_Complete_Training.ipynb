{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71obQ29i_Dy8"
   },
   "source": "# StreamGuard ML Training - Complete Notebook\n\n**Version:** 1.6 (Issue #11 Training Collapse Fix + Drive-Based Data)  \n**Last Updated:** 2025-10-30  \n**Platform:** Google Colab (Free/Pro/Pro+)  \n**GPU:** T4/V100/A100 (Adaptive Configuration)  \n**Duration:** 11-24 hours (depends on GPU & config)  \n\nThis notebook trains all three StreamGuard models with **adaptive configuration** that automatically optimizes for your GPU.\n\n## üéØ Training Phases\n1. **Enhanced SQL Intent Transformer** (2-8 hours depending on GPU)\n2. **Enhanced Taint-Flow GNN** (4-12 hours depending on GPU)\n3. **Fusion Layer** (2-10 hours depending on GPU)\n\n## ‚ú® What's New in v1.6 (Issue #11 - Training Collapse Fix)\n\n### **CRITICAL: Training Collapse Fixed (Issue #11)**\n- ‚úÖ **Class-balanced loss with inverse frequency weights** (fixes model predicting only safe class)\n- ‚úÖ **LR scaling for large batches** (square-root rule: batch 64 gets 2x base LR)\n- ‚úÖ **Per-step scheduler** (moved inside train_epoch, was per-epoch before)\n- ‚úÖ **Gradient clipping** (max_norm=1.0 prevents exploding gradients)\n- ‚úÖ **Prediction distribution monitoring** (detects collapse early)\n- ‚úÖ **Enhanced collapse detection** (stops training if model predicts only one class)\n- ‚úÖ **Conservative label smoothing** (0.05 instead of 0.1)\n- ‚úÖ **Simplified loss calculation** (removed unnecessary sample-level weighting)\n\n**Root Cause (Issue #11):** Model collapsed from F1=0.4337 (epoch 1) to F1=0.0000 (epoch 3+) due to:\n1. No class balancing (54.2% safe vs 45.8% vulnerable)\n2. LR designed for batch=16 but using batch=64\n3. Scheduler stepping per-epoch instead of per-step\n4. No gradient clipping\n5. No early collapse detection\n\n**The Fix:** All 8 critical fixes implemented in train_transformer.py (see `docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md`)\n\n### **Previous Fixes (v1.5 - Issue #10)**\n- ‚úÖ **Max seq length configuration fixed** (512 for all GPUs, not 1024/768)\n- ‚úÖ **Automatic validation** to prevent exceeding CodeBERT's 512-token limit\n- ‚úÖ **Tensor size mismatch error prevented**\n- ‚úÖ **Updated PyTorch AMP API** (torch.amp instead of torch.cuda.amp)\n\n### **Previous Fixes (v1.4 - Issue #9)**\n- ‚úÖ **Fixed CrossEntropyLoss tensor-to-scalar error**\n- ‚úÖ **Fixed sample weights handling**\n- ‚úÖ **Updated deprecated autocast/GradScaler**\n- ‚úÖ **Added Cell 1.5** (robust GPU detection with fallback)\n\n### **Previous Fixes (v1.3 - Issue #8)**\n- ‚úÖ **Fixed NumPy binary incompatibility** (numpy==1.26.4 enforced)\n- ‚úÖ **Fixed tokenizers/transformers conflict** (tokenizers 0.14.1)\n- ‚úÖ **Fixed PyG circular import errors**\n\n### **Adaptive GPU Configuration (Colab Pro)**\n- üîç **Auto-detects GPU type** (T4/V100/A100) via Cell 1.5\n- ‚öôÔ∏è  **Selects optimal hyperparameters** automatically\n- üìä **Three configuration tiers**:\n  - **OPTIMIZED** (T4): 10/150/30 epochs, batch 32/64, seq 512, ~13-17h\n  - **ENHANCED** (V100): 15/200/50 epochs, batch 48/96, seq 512, ~18-22h (2-3x faster)\n  - **AGGRESSIVE** (A100): 20/300/100 epochs, batch 64/128, seq 512, ~20-24h (5-7x faster)\n\n**Note:** All configurations use `max_seq_len = 512` (CodeBERT/RoBERTa model limit). Better GPUs benefit from larger batch sizes and more epochs.\n\n### **Colab Pro Benefits**\n- ‚úÖ 24-hour runtime (vs 12h free)\n- ‚úÖ Better GPU access (V100, A100)\n- ‚úÖ Background execution\n- ‚úÖ **Larger batches ‚Üí better gradient estimates**\n\n**Recommended:** V100 on Colab Pro ($10/mo) for best balance of speed and availability.\n\n## üîß All Critical Fixes Applied (v1.1 ‚Üí v1.6)\n\n### **v1.6 Fixes (Issue #11) - NEW**\n- ‚úÖ Class-balanced loss with inverse frequency weights\n- ‚úÖ LR scaling for large batches (square-root rule)\n- ‚úÖ Warmup ratio adjustment (proportional, capped at 20%)\n- ‚úÖ Per-step scheduler (moved inside train_epoch)\n- ‚úÖ Gradient clipping (max_norm=1.0)\n- ‚úÖ Prediction distribution monitoring\n- ‚úÖ Enhanced collapse detection\n- ‚úÖ Conservative label smoothing (0.05)\n- ‚úÖ Drive-based data workflow (automatic copy to local storage)\n- ‚úÖ Pre-training validation tests\n\n### **v1.5 Fixes (Issue #10)**\n- ‚úÖ Max seq length configuration fixed\n- ‚úÖ Automatic validation added\n- ‚úÖ Tensor size mismatch prevented\n- ‚úÖ PyTorch AMP API updated\n\n### **v1.4 Fixes (Issue #9)**\n- ‚úÖ CrossEntropyLoss tensor-to-scalar error fixed\n- ‚úÖ Sample weights handling validated\n- ‚úÖ Deprecated API updated\n- ‚úÖ GPU detection robustness improved\n\n### **v1.3 Fixes (Issue #8)**\n- ‚úÖ NumPy binary compatibility fixed\n- ‚úÖ tokenizers/transformers conflict resolved\n- ‚úÖ PyG circular import fixed\n\n### **v1.1-v1.2 Fixes (Issues #1-#7)**\n- ‚úÖ Runtime-aware PyTorch Geometric installation\n- ‚úÖ Robust tree-sitter build with fallback\n- ‚úÖ Version compatibility validation\n- ‚úÖ Enhanced dependency conflict detection\n- ‚úÖ Optimized OOF fusion\n\n## üìã Before Starting\n\n### **Colab Configuration:**\n1. Enable GPU: **Runtime ‚Üí Change runtime type ‚Üí GPU**\n2. **Recommended:** Subscribe to Colab Pro ($10/mo) for:\n   - 24-hour runtime (required for full training)\n   - Access to V100/A100 GPUs (2-7x faster than T4)\n   - Background execution\n\n### **Data Requirements - IMPORTANT:**\n\n**You MUST upload preprocessed data files to Google Drive:**\n\n```\nMy Drive/streamguard/data/processed/codexglue/\n‚îú‚îÄ‚îÄ train.jsonl (504 MB, 21,854 samples)\n‚îú‚îÄ‚îÄ valid.jsonl (63 MB, 2,732 samples)\n‚îú‚îÄ‚îÄ test.jsonl (63 MB, 2,732 samples)\n‚îî‚îÄ‚îÄ preprocessing_metadata.json (1.6 KB)\n```\n\n**Total size:** ~630 MB\n\n**Why Google Drive?**\n- Data files are too large for GitHub (exceeds 100 MB limit)\n- They are in `.gitignore` and won't be cloned from the repository\n- **Cell 6** will automatically mount Drive and copy data to Colab local storage\n- Local storage provides faster I/O during training (vs reading from Drive each time)\n\n**How to upload:**\n1. Open Google Drive: https://drive.google.com/\n2. Create folder structure: `My Drive/streamguard/data/processed/codexglue/`\n3. Upload the 4 data files to this folder\n4. Run notebook Cell 6 - it will copy files to Colab automatically\n\n## üìä Expected Results by Configuration\n\n| Config | GPU | Time | Batch Sizes (T/G) | Seq Len | Speed vs T4 |\n|--------|-----|------|-------------------|---------|-------------|\n| **OPTIMIZED** | T4 | 13-17h | 32 / 64 | 512 | 1.0x |\n| **ENHANCED** | V100 | 18-22h | 48 / 96 | 512 | 2-3x faster |\n| **AGGRESSIVE** | A100 | 20-24h | 64 / 128 | 512 | 5-7x faster |\n\n*Note: All configs use max_seq_len=512 (CodeBERT limit). Better GPUs use larger batches/epochs for quality.*\n\n## üöÄ Quick Start\n\n1. **Upload data to Drive** (see Data Requirements above)\n2. Run **Cell 1**: Verify GPU is enabled\n3. Run **Cell 1.5**: Auto-detect GPU and select configuration  \n4. Run **Cell 2**: Install dependencies with compatibility fixes\n5. Run **Cell 2.5**: Validate compatibility\n6. Run **Cell 3**: Clone repository from GitHub\n7. Run **Cell 4**: Setup tree-sitter\n8. Run **Cell 6**: Mount Drive and copy data to local storage ‚≠ê\n9. **Run TEST CELLS 6.5 & 6.6**: Verify Issue #11 fixes (5-15 min total)\n10. Run **Cells 7, 9, 11**: Full training with adaptive configuration\n11. Monitor progress (can close browser with Colab Pro)\n\n**IMPORTANT:** Run the test cells (6.5 & 6.6) before full training to verify all fixes are working!\n\n## üîó Documentation\n\n- **Training Collapse Fix:** See [docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md)\n- **Final Recommendations:** See [docs/ISSUE_11_FINAL_CAUTIONS_AND_RECOMMENDATIONS.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_11_FINAL_CAUTIONS_AND_RECOMMENDATIONS.md)\n- **Max Seq Length Fix:** See [docs/ISSUE_10_MAX_SEQ_LEN_FIX.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_10_MAX_SEQ_LEN_FIX.md)\n- **Critical Fixes Details:** See [docs/COLAB_CRITICAL_FIXES.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/COLAB_CRITICAL_FIXES.md)\n- **Troubleshooting:** Check Issue #8, #9, #10, and #11 documentation for common errors"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ_fI8de_Dy_"
   },
   "source": [
    "---\n",
    "## Part 1: Environment Setup\n",
    "Run these cells once at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1761737747974,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "dr8Lmrx7_DzA",
    "outputId": "afd86fb7-b4c8-4a22-e8a1-62cf3ccc7498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n",
      "CUDA Version: 12.6\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: GPU not available! Enable GPU in Runtime ‚Üí Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: GPU Detection & Adaptive Configuration (Colab Pro Optimization)\n",
    "import subprocess\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Detect GPU type and memory with robust fallback.\"\"\"\n",
    "    try:\n",
    "        # Try nvidia-smi first (most reliable)\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "            capture_output=True, text=True, timeout=5\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            # Use first GPU if multiple\n",
    "            gpu_line = lines[0].split(',')\n",
    "            gpu_name = gpu_line[0].strip()\n",
    "            \n",
    "            # Parse memory (handle \"15360 MiB\" or \"15.36 GB\")\n",
    "            mem_str = gpu_line[1].strip()\n",
    "            if 'MiB' in mem_str:\n",
    "                gpu_memory = float(re.findall(r'\\d+', mem_str)[0]) / 1024  # MiB to GB\n",
    "            else:\n",
    "                gpu_memory = float(re.findall(r'[\\d.]+', mem_str)[0])\n",
    "            \n",
    "            return gpu_name, gpu_memory\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError, IndexError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    # Fallback to PyTorch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Bytes to GB\n",
    "        return gpu_name, gpu_memory\n",
    "    \n",
    "    # No GPU available\n",
    "    return \"CPU\", 0.0\n",
    "\n",
    "gpu_name, gpu_memory_gb = get_gpu_info()\n",
    "gpu_name_lower = gpu_name.lower()\n",
    "\n",
    "# Determine configuration tier (case-insensitive matching)\n",
    "# CRITICAL FIX (Issue #9): CodeBERT max_seq_len is 512 (514 with special tokens) - RoBERTa limitation\n",
    "# Using max_seq_len > 512 causes: RuntimeError: The expanded size of the tensor (1024) must match the existing size (514)\n",
    "if 'a100' in gpu_name_lower:\n",
    "    config_tier = 'AGGRESSIVE'\n",
    "    config = {\n",
    "        'transformer': {'epochs': 20, 'batch_size': 64, 'max_seq_len': 512, 'patience': 5},\n",
    "        'gnn': {'epochs': 300, 'batch_size': 128, 'hidden_dim': 512, 'num_layers': 5, 'patience': 15},\n",
    "        'fusion': {'n_folds': 10, 'epochs': 100}\n",
    "    }\n",
    "    note = \"Maximum configuration - larger batches and more epochs for best training quality\"\n",
    "elif 'v100' in gpu_name_lower:\n",
    "    config_tier = 'ENHANCED'\n",
    "    config = {\n",
    "        'transformer': {'epochs': 15, 'batch_size': 48, 'max_seq_len': 512, 'patience': 3},\n",
    "        'gnn': {'epochs': 200, 'batch_size': 96, 'hidden_dim': 384, 'num_layers': 5, 'patience': 12},\n",
    "        'fusion': {'n_folds': 5, 'epochs': 50}\n",
    "    }\n",
    "    note = \"Enhanced configuration - 2-3x faster than T4, larger batches for better gradient estimates\"\n",
    "else:  # T4 or other\n",
    "    config_tier = 'OPTIMIZED'\n",
    "    config = {\n",
    "        'transformer': {'epochs': 10, 'batch_size': 32, 'max_seq_len': 512, 'patience': 2},\n",
    "        'gnn': {'epochs': 150, 'batch_size': 64, 'hidden_dim': 256, 'num_layers': 4, 'patience': 10},\n",
    "        'fusion': {'n_folds': 5, 'epochs': 30}\n",
    "    }\n",
    "    note = \"Optimized for T4 - reliable and cost-effective\"\n",
    "\n",
    "# Save config for training cells\n",
    "config_data = {'tier': config_tier, 'gpu': gpu_name, 'config': config}\n",
    "with open('/tmp/gpu_training_config.json', 'w') as f:\n",
    "    json.dump(config_data, f)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ADAPTIVE GPU CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Detected GPU: {gpu_name}\")\n",
    "print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
    "print(f\"\\nConfiguration Tier: {config_tier}\")\n",
    "print(f\"Note: {note}\")\n",
    "print(\"\\nHyperparameters:\")\n",
    "print(f\"  Transformer: {config['transformer']['epochs']} epochs, batch {config['transformer']['batch_size']}, seq {config['transformer']['max_seq_len']}\")\n",
    "print(f\"  GNN: {config['gnn']['epochs']} epochs, batch {config['gnn']['batch_size']}, hidden {config['gnn']['hidden_dim']}\")\n",
    "print(f\"  Fusion: {config['fusion']['n_folds']} folds, {config['fusion']['epochs']} epochs\")\n",
    "print(\"\\nüí° Note: max_seq_len is 512 for all configs (CodeBERT/RoBERTa model limit)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30206,
     "status": "ok",
     "timestamp": 1761737782079,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "z4uF4Wv7_DzB",
    "outputId": "1d2fe0f4-d8e2-428f-de94-f019433da6f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INSTALLING DEPENDENCIES WITH COMPATIBILITY FIXES\n",
      "======================================================================\n",
      "\n",
      "[1/9] Ensuring NumPy compatibility...\n",
      "‚úì NumPy 1.26.4 (v1.x - already compatible)\n",
      "\n",
      "[2/9] Detecting PyTorch and CUDA versions...\n",
      "‚úì Detected PyTorch 2.8.0\n",
      "‚úì Detected CUDA 12.6\n",
      "‚úì Using wheel tag: cu126\n",
      "\n",
      "[3/9] Installing PyTorch Geometric (runtime-aware with fallback)...\n",
      "Wheel URL: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
      "  Installing torch-scatter...\n",
      "Running: pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
      "    ‚úì torch-scatter installed from wheel\n",
      "  Installing torch-sparse...\n",
      "Running: pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
      "    ‚úì torch-sparse installed from wheel\n",
      "  Installing torch-cluster...\n",
      "Running: pip install -q torch-cluster -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
      "    ‚úì torch-cluster installed from wheel\n",
      "  Installing torch-spline-conv...\n",
      "Running: pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
      "    ‚úì torch-spline-conv installed from wheel\n",
      "Running: pip install -q torch-geometric==2.4.0\n",
      "‚úÖ PyTorch Geometric installed successfully\n",
      "\n",
      "[4/9] Installing Transformers with compatible tokenizers...\n",
      "‚ö†Ô∏è  Note: Using tokenizers 0.14.1 (compatible with transformers 4.35.0)\n",
      "Running: pip install -q transformers==4.35.0\n",
      "Running: pip install -q tokenizers==0.14.1\n",
      "‚úì Tokenizers 0.14.1 installed (compatible)\n",
      "Running: pip install -q accelerate==0.24.1\n",
      "\n",
      "[5/9] Installing tree-sitter...\n",
      "Running: pip install -q tree-sitter==0.20.4\n",
      "\n",
      "[6/9] Installing additional packages...\n",
      "Running: pip install -q scikit-learn==1.3.2 scipy==1.11.4 tqdm\n",
      "\n",
      "[7/9] Verifying installations...\n",
      "‚úì NumPy: 1.26.4 (binary compatible)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyTorch: 2.8.0+cu126\n",
      "‚úì PyTorch Geometric: 2.4.0\n",
      "‚úì Transformers: 4.35.0\n",
      "‚úì Tokenizers: 0.14.1\n",
      "  ‚úì Tokenizers version compatible\n",
      "‚ùå Verification failed: module 'tree_sitter' has no attribute '__version__'\n",
      "   Please restart runtime and try again\n",
      "   If issue persists, check:\n",
      "   1. NumPy version (should be 1.26.4)\n",
      "   2. Tokenizers version (should be 0.14.1)\n",
      "\n",
      "[8/9] Testing PyTorch Geometric...\n",
      "‚úì PyTorch Geometric working correctly\n",
      "‚úì Test data created: Data(x=[5, 3], edge_index=[2, 2])\n",
      "\n",
      "[9/9] Installation Summary:\n",
      "======================================================================\n",
      "‚úÖ ALL INSTALLATIONS SUCCESSFUL\n",
      "‚úì NumPy 1.x (binary compatible)\n",
      "‚úì PyTorch Geometric with correct wheels\n",
      "‚úì Transformers with compatible tokenizers\n",
      "‚úì All packages verified\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Install dependencies with runtime detection and compatibility fixes\n",
    "# ‚ö†Ô∏è CRITICAL: Includes NumPy compatibility fix, correct tokenizers version, and PyG error handling\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    \"\"\"Run shell command and return success status.\"\"\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INSTALLING DEPENDENCIES WITH COMPATIBILITY FIXES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# [1/9] CRITICAL: Fix NumPy version FIRST (before any torch imports)\n",
    "print(\"\\n[1/9] Ensuring NumPy compatibility...\")\n",
    "try:\n",
    "    import numpy\n",
    "    numpy_ver = numpy.__version__\n",
    "    numpy_major = int(numpy_ver.split('.')[0])\n",
    "\n",
    "    if numpy_major >= 2:\n",
    "        print(f\"‚ö†Ô∏è  Detected NumPy {numpy_ver} (v2.x)\")\n",
    "        print(\"   PyTorch wheels may have binary incompatibility\")\n",
    "        print(\"   Downgrading to NumPy 1.26.4...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.26.4\", \"--force-reinstall\"], check=True)\n",
    "        print(\"‚úì NumPy downgraded to 1.26.4\")\n",
    "        # Reload numpy\n",
    "        importlib.reload(numpy)\n",
    "        print(f\"‚úì NumPy {numpy.__version__} loaded (binary compatible)\")\n",
    "    else:\n",
    "        print(f\"‚úì NumPy {numpy_ver} (v1.x - already compatible)\")\n",
    "except ImportError:\n",
    "    print(\"NumPy not installed, installing 1.26.4...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.26.4\"], check=True)\n",
    "    import numpy\n",
    "    print(f\"‚úì NumPy {numpy.__version__} installed\")\n",
    "\n",
    "# [2/9] Detect PyTorch and CUDA versions (now safe with correct numpy)\n",
    "print(\"\\n[2/9] Detecting PyTorch and CUDA versions...\")\n",
    "import torch\n",
    "\n",
    "torch_version = torch.__version__.split('+')[0]  # e.g., '2.8.0'\n",
    "cuda_version = torch.version.cuda  # e.g., '12.6'\n",
    "cuda_tag = f\"cu{cuda_version.replace('.', '')}\" if cuda_version else 'cpu'  # e.g., 'cu126'\n",
    "\n",
    "print(f\"‚úì Detected PyTorch {torch_version}\")\n",
    "print(f\"‚úì Detected CUDA {cuda_version if cuda_version else 'N/A (CPU only)'}\")\n",
    "print(f\"‚úì Using wheel tag: {cuda_tag}\")\n",
    "\n",
    "# [3/9] Install PyTorch Geometric with enhanced error handling\n",
    "print(\"\\n[3/9] Installing PyTorch Geometric (runtime-aware with fallback)...\")\n",
    "pyg_wheel_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
    "print(f\"Wheel URL: {pyg_wheel_url}\")\n",
    "\n",
    "pyg_packages = ['torch-scatter', 'torch-sparse', 'torch-cluster', 'torch-spline-conv']\n",
    "pyg_install_success = True\n",
    "\n",
    "for pkg in pyg_packages:\n",
    "    print(f\"  Installing {pkg}...\")\n",
    "    if not run_cmd(f\"pip install -q {pkg} -f {pyg_wheel_url}\"):\n",
    "        print(f\"    ‚ö†Ô∏è  Wheel install failed, trying source build...\")\n",
    "        if not run_cmd(f\"pip install -q {pkg} --no-binary {pkg}\"):\n",
    "            print(f\"    ‚ùå Failed to install {pkg}\")\n",
    "            pyg_install_success = False\n",
    "        else:\n",
    "            print(f\"    ‚úì {pkg} installed from source (slower)\")\n",
    "    else:\n",
    "        print(f\"    ‚úì {pkg} installed from wheel\")\n",
    "\n",
    "if pyg_install_success:\n",
    "    run_cmd(\"pip install -q torch-geometric==2.4.0\")\n",
    "    print(\"‚úÖ PyTorch Geometric installed successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some PyG packages failed - GNN training may have issues\")\n",
    "\n",
    "# [4/9] Install Transformers with COMPATIBLE tokenizers version\n",
    "print(\"\\n[4/9] Installing Transformers with compatible tokenizers...\")\n",
    "print(\"‚ö†Ô∏è  Note: Using tokenizers 0.14.1 (compatible with transformers 4.35.0)\")\n",
    "\n",
    "# Install transformers first, then pin tokenizers to compatible version\n",
    "if not run_cmd(\"pip install -q transformers==4.35.0\"):\n",
    "    print(\"‚ùå Transformers installation failed\")\n",
    "else:\n",
    "    # Now pin tokenizers to compatible version\n",
    "    if not run_cmd(\"pip install -q tokenizers==0.14.1\"):\n",
    "        print(\"‚ö†Ô∏è  Could not pin tokenizers to 0.14.1, using auto-resolved version\")\n",
    "    else:\n",
    "        print(\"‚úì Tokenizers 0.14.1 installed (compatible)\")\n",
    "\n",
    "# Install accelerate\n",
    "run_cmd(\"pip install -q accelerate==0.24.1\")\n",
    "\n",
    "# [5/9] Install tree-sitter\n",
    "print(\"\\n[5/9] Installing tree-sitter...\")\n",
    "run_cmd(\"pip install -q tree-sitter==0.20.4\")\n",
    "\n",
    "# [6/9] Install additional packages\n",
    "print(\"\\n[6/9] Installing additional packages...\")\n",
    "run_cmd(\"pip install -q scikit-learn==1.3.2 scipy==1.11.4 tqdm\")\n",
    "\n",
    "# [7/9] Verify installations with enhanced checks\n",
    "print(\"\\n[7/9] Verifying installations...\")\n",
    "try:\n",
    "    # Check NumPy first (critical)\n",
    "    import numpy\n",
    "    numpy_ver = numpy.__version__\n",
    "    numpy_major = int(numpy_ver.split('.')[0])\n",
    "    if numpy_major >= 2:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: NumPy {numpy_ver} detected (should be 1.x)\")\n",
    "        print(\"   Binary compatibility issues may occur\")\n",
    "    else:\n",
    "        print(f\"‚úì NumPy: {numpy_ver} (binary compatible)\")\n",
    "\n",
    "    # Check other packages\n",
    "    import torch\n",
    "    import torch_geometric\n",
    "    import transformers\n",
    "    import tree_sitter\n",
    "    import sklearn\n",
    "\n",
    "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úì PyTorch Geometric: {torch_geometric.__version__}\")\n",
    "    print(f\"‚úì Transformers: {transformers.__version__}\")\n",
    "\n",
    "    # Check tokenizers compatibility\n",
    "    import tokenizers\n",
    "    tokenizers_ver = tokenizers.__version__\n",
    "    print(f\"‚úì Tokenizers: {tokenizers_ver}\")\n",
    "\n",
    "    if tokenizers_ver.startswith(\"0.15\"):\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: tokenizers {tokenizers_ver} may conflict with transformers 4.35.0\")\n",
    "    elif tokenizers_ver.startswith(\"0.14\"):\n",
    "        print(f\"  ‚úì Tokenizers version compatible\")\n",
    "\n",
    "    print(f\"‚úì tree-sitter: {tree_sitter.__version__}\")\n",
    "    print(f\"‚úì scikit-learn: {sklearn.__version__}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Verification failed: {e}\")\n",
    "    print(\"   Please restart runtime and try again\")\n",
    "    print(\"   If issue persists, check:\")\n",
    "    print(\"   1. NumPy version (should be 1.26.4)\")\n",
    "    print(\"   2. Tokenizers version (should be 0.14.1)\")\n",
    "\n",
    "# [8/9] Test PyTorch Geometric installation\n",
    "print(\"\\n[8/9] Testing PyTorch Geometric...\")\n",
    "try:\n",
    "    from torch_geometric.data import Data\n",
    "    test_data = Data(x=torch.randn(5, 3), edge_index=torch.tensor([[0, 1], [1, 0]]))\n",
    "    print(\"‚úì PyTorch Geometric working correctly\")\n",
    "    print(f\"‚úì Test data created: {test_data}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  PyTorch Geometric test failed: {e}\")\n",
    "    print(\"   GNN training may have issues\")\n",
    "    print(\"   Possible causes:\")\n",
    "    print(\"   1. NumPy binary incompatibility\")\n",
    "    print(\"   2. PyG wheel installation failed\")\n",
    "    print(\"   3. CUDA version mismatch\")\n",
    "\n",
    "# [9/9] Display final summary\n",
    "print(\"\\n[9/9] Installation Summary:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "success_indicators = {\n",
    "    'numpy_compatible': numpy_major < 2 if 'numpy_major' in locals() else False,\n",
    "    'pyg_installed': pyg_install_success,\n",
    "    'transformers_installed': True,  # Assume success if we got here\n",
    "    'tokenizers_compatible': tokenizers_ver.startswith(\"0.14\") if 'tokenizers_ver' in locals() else False\n",
    "}\n",
    "\n",
    "all_success = all(success_indicators.values())\n",
    "\n",
    "if all_success:\n",
    "    print(\"‚úÖ ALL INSTALLATIONS SUCCESSFUL\")\n",
    "    print(\"‚úì NumPy 1.x (binary compatible)\")\n",
    "    print(\"‚úì PyTorch Geometric with correct wheels\")\n",
    "    print(\"‚úì Transformers with compatible tokenizers\")\n",
    "    print(\"‚úì All packages verified\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  INSTALLATION COMPLETED WITH WARNINGS:\")\n",
    "    if not success_indicators['numpy_compatible']:\n",
    "        print(\"  ‚Ä¢ NumPy version may cause binary incompatibility\")\n",
    "    if not success_indicators['pyg_installed']:\n",
    "        print(\"  ‚Ä¢ PyG packages had installation issues\")\n",
    "    if not success_indicators['tokenizers_compatible']:\n",
    "        print(\"  ‚Ä¢ Tokenizers version may conflict with transformers\")\n",
    "    print(\"\\n  Training may still work, but monitor for errors\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1602,
     "status": "ok",
     "timestamp": 1761737826106,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "EN-qu_tx_DzC",
    "outputId": "45efe0bd-8521-47b6-d62a-2ffc20124888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENHANCED DEPENDENCY & VERSION COMPATIBILITY CHECK\n",
      "======================================================================\n",
      "\n",
      "[1/4] Installed Core Versions:\n",
      "  PyTorch: 2.8.0+cu126\n",
      "  PyTorch Geometric: 2.4.0\n",
      "  Transformers: 4.35.0\n",
      "  CUDA: 12.6\n",
      "\n",
      "[2/4] Checking Optional Dependencies:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì sentence_transformers: not installed (correct)\n",
      "  ‚úì datasets: not installed (correct)\n",
      "  ‚ö†Ô∏è  fsspec: 2025.3.0 (not needed for training)\n",
      "  ‚ö†Ô∏è  gcsfs: 2025.3.0 (not needed for training)\n",
      "\n",
      "[3/4] Validating PyTorch Geometric Installation:\n",
      "  Expected wheel URL: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
      "  ‚úì PyTorch Geometric working correctly\n",
      "  ‚úì Wheels matched PyTorch 2.8.0 + cu126\n",
      "\n",
      "[4/4] Core Compatibility Checks:\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALL CHECKS PASSED - Ready for production training!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5: Enhanced Version & Dependency Compatibility Check (v1.1)\n",
    "# Validates versions, checks for dependency conflicts, validates PyG wheels\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import transformers\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENHANCED DEPENDENCY & VERSION COMPATIBILITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# [1/4] Check core versions\n",
    "torch_ver = torch.__version__\n",
    "pyg_ver = torch_geometric.__version__\n",
    "transformers_ver = transformers.__version__\n",
    "cuda_ver = torch.version.cuda if torch.cuda.is_available() else \"N/A\"\n",
    "\n",
    "print(f\"\\n[1/4] Installed Core Versions:\")\n",
    "print(f\"  PyTorch: {torch_ver}\")\n",
    "print(f\"  PyTorch Geometric: {pyg_ver}\")\n",
    "print(f\"  Transformers: {transformers_ver}\")\n",
    "print(f\"  CUDA: {cuda_ver}\")\n",
    "\n",
    "# [2/4] Check for problematic optional dependencies (CRITICAL FIX #4)\n",
    "print(f\"\\n[2/4] Checking Optional Dependencies:\")\n",
    "optional_deps = {\n",
    "    'sentence_transformers': None,\n",
    "    'datasets': None,\n",
    "    'fsspec': None,\n",
    "    'gcsfs': None\n",
    "}\n",
    "\n",
    "for pkg_name in optional_deps.keys():\n",
    "    try:\n",
    "        pkg = importlib.import_module(pkg_name)\n",
    "        version = getattr(pkg, '__version__', 'unknown')\n",
    "        optional_deps[pkg_name] = version\n",
    "        print(f\"  ‚ö†Ô∏è  {pkg_name}: {version} (not needed for training)\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚úì {pkg_name}: not installed (correct)\")\n",
    "\n",
    "# Check for version conflicts\n",
    "has_conflicts = False\n",
    "if optional_deps.get('sentence_transformers'):\n",
    "    print(\"\\n  ‚ö†Ô∏è  WARNING: sentence-transformers detected\")\n",
    "    print(\"     May conflict with transformers==4.35.0\")\n",
    "    print(\"     If errors occur, uninstall: !pip uninstall -y sentence-transformers\")\n",
    "    has_conflicts = True\n",
    "\n",
    "if optional_deps.get('datasets'):\n",
    "    print(\"\\n  ‚ö†Ô∏è  WARNING: datasets library detected\")\n",
    "    print(\"     May pull incompatible transformers/tokenizers versions\")\n",
    "    has_conflicts = True\n",
    "\n",
    "# [3/4] Validate PyG wheel URL (CRITICAL FIX #4)\n",
    "print(f\"\\n[3/4] Validating PyTorch Geometric Installation:\")\n",
    "torch_version = torch_ver.split('+')[0]\n",
    "cuda_tag = f\"cu{cuda_ver.replace('.', '')}\" if cuda_ver != \"N/A\" else 'cpu'\n",
    "pyg_wheel_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
    "\n",
    "print(f\"  Expected wheel URL: {pyg_wheel_url}\")\n",
    "\n",
    "# Quick test PyG installation\n",
    "try:\n",
    "    from torch_geometric.data import Data\n",
    "    test_data = Data(x=torch.randn(5, 3), edge_index=torch.tensor([[0, 1], [1, 0]]))\n",
    "    print(f\"  ‚úì PyTorch Geometric working correctly\")\n",
    "    print(f\"  ‚úì Wheels matched PyTorch {torch_version} + {cuda_tag}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå PyTorch Geometric test failed: {e}\")\n",
    "    print(f\"  ‚ö†Ô∏è  Wheel URL may be incorrect - check {pyg_wheel_url}\")\n",
    "\n",
    "# [4/4] Core compatibility checks\n",
    "print(f\"\\n[4/4] Core Compatibility Checks:\")\n",
    "warnings = []\n",
    "errors = []\n",
    "\n",
    "# Check PyTorch version\n",
    "torch_major = int(torch_ver.split('.')[0])\n",
    "if torch_major < 2:\n",
    "    warnings.append(\"‚ö†Ô∏è  PyTorch 2.x+ recommended (you have {torch_ver})\")\n",
    "\n",
    "# Check CUDA availability (CRITICAL)\n",
    "if not torch.cuda.is_available():\n",
    "    errors.append(\"‚ùå CUDA not available - training will be EXTREMELY slow\")\n",
    "    errors.append(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "# Check PyG compatibility\n",
    "pyg_major = int(pyg_ver.split('.')[0])\n",
    "if pyg_major < 2:\n",
    "    warnings.append(\"‚ö†Ô∏è  PyTorch Geometric 2.x+ recommended\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_mem_gb < 12:\n",
    "        warnings.append(f\"‚ö†Ô∏è  GPU has only {gpu_mem_gb:.1f} GB RAM (16GB+ recommended)\")\n",
    "        warnings.append(\"   Consider reducing batch sizes if OOM errors occur\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if errors:\n",
    "    print(\"üî¥ CRITICAL ERRORS:\")\n",
    "    for e in errors:\n",
    "        print(f\"  {e}\")\n",
    "    print(\"\\n‚ùå CANNOT PROCEED - Fix errors above\")\n",
    "    print(\"=\"*70)\n",
    "    raise RuntimeError(\"Environment validation failed\")\n",
    "elif warnings or has_conflicts:\n",
    "    if warnings:\n",
    "        print(\"‚ö†Ô∏è  Compatibility Warnings:\")\n",
    "        for w in warnings:\n",
    "            print(f\"  {w}\")\n",
    "    if has_conflicts:\n",
    "        print(\"\\n‚ö†Ô∏è  Dependency Conflicts Detected:\")\n",
    "        print(\"  Monitor for errors during training\")\n",
    "        print(\"  If issues occur, restart runtime and reinstall dependencies\")\n",
    "    print(\"\\n‚úì You can proceed but may need adjustments\")\n",
    "else:\n",
    "    print(\"‚úÖ ALL CHECKS PASSED - Ready for production training!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1761741440390,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "7aJ2uNnO_DzD",
    "outputId": "86a167c9-6d51-45ba-c2e2-486f15bf9ba9"
   },
   "outputs": [],
   "source": "# Cell 3: Clone/Update repository from GitHub\nimport os\nfrom pathlib import Path\n\n# Clone or update StreamGuard repository\nif not Path('streamguard').exists():\n    print(\"Cloning StreamGuard repository...\")\n    !git clone https://github.com/VimalSajanGeorge/streamguard.git\n    print(\"‚úì Repository cloned\")\nelse:\n    print(\"‚úì Repository already exists\")\n    print(\"Pulling latest changes...\")\n    os.chdir('streamguard')\n    !git pull origin master\n    print(\"‚úì Repository updated\")\n    os.chdir('..')\n\nos.chdir('streamguard')\nprint(f\"\\nWorking directory: {os.getcwd()}\")\nprint(\"\\nüí° All code changes from GitHub are now available!\")\nprint(\"   No need to manually upload files to Google Drive\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1722,
     "status": "ok",
     "timestamp": 1761737921383,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "wncSmaKW_DzE",
    "outputId": "30a44eab-49b6-4d9d-876b-de3fd830cd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TREE-SITTER SETUP (with fallback support)\n",
      "======================================================================\n",
      "\n",
      "[1/3] Cloning tree-sitter-c...\n",
      "Cloning into 'tree-sitter-c'...\n",
      "remote: Enumerating objects: 90, done.\u001b[K\n",
      "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 90 (delta 5), reused 30 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (90/90), 373.20 KiB | 3.42 MiB/s, done.\n",
      "Resolving deltas: 100% (5/5), done.\n",
      "‚úì tree-sitter-c cloned\n",
      "\n",
      "[2/3] Building tree-sitter library...\n",
      "‚úì Build completed\n",
      "\n",
      "[3/3] Verifying build...\n",
      "‚úì tree-sitter library verified successfully\n",
      "\n",
      "======================================================================\n",
      "‚úÖ AST PARSING ENABLED (optimal)\n",
      "   Preprocessing will use full AST structure\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Setup tree-sitter with robust error handling\n",
    "# ‚ö†Ô∏è CRITICAL: Includes fallback if build fails\n",
    "\n",
    "from pathlib import Path\n",
    "from tree_sitter import Language\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TREE-SITTER SETUP (with fallback support)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clone tree-sitter-c\n",
    "vendor_dir = Path('vendor')\n",
    "vendor_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if not (vendor_dir / 'tree-sitter-c').exists():\n",
    "    print(\"\\n[1/3] Cloning tree-sitter-c...\")\n",
    "    !cd vendor && git clone --depth 1 https://github.com/tree-sitter/tree-sitter-c.git\n",
    "    print(\"‚úì tree-sitter-c cloned\")\n",
    "else:\n",
    "    print(\"\\n[1/3] ‚úì tree-sitter-c already exists\")\n",
    "\n",
    "# Build library with error handling\n",
    "build_dir = Path('build')\n",
    "build_dir.mkdir(exist_ok=True)\n",
    "lib_path = build_dir / 'my-languages.so'\n",
    "\n",
    "build_success = False\n",
    "\n",
    "if not lib_path.exists():\n",
    "    print(\"\\n[2/3] Building tree-sitter library...\")\n",
    "    try:\n",
    "        Language.build_library(\n",
    "            str(lib_path),\n",
    "            [str(vendor_dir / 'tree-sitter-c')]\n",
    "        )\n",
    "        print(\"‚úì Build completed\")\n",
    "\n",
    "        # Verify build\n",
    "        if lib_path.exists():\n",
    "            print(\"\\n[3/3] Verifying build...\")\n",
    "            try:\n",
    "                test_lang = Language(str(lib_path), 'c')\n",
    "                print(\"‚úì tree-sitter library verified successfully\")\n",
    "                build_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Verification failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Build completed but library file not found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Build failed: {e}\")\n",
    "        print(\"   Common causes: missing compiler, permission issues\")\n",
    "else:\n",
    "    print(\"\\n[2/3] ‚úì tree-sitter library already exists\")\n",
    "    print(\"\\n[3/3] Verifying existing build...\")\n",
    "    try:\n",
    "        test_lang = Language(str(lib_path), 'c')\n",
    "        print(\"‚úì Existing library verified\")\n",
    "        build_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Existing library invalid: {e}\")\n",
    "\n",
    "# Display final status\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if build_success:\n",
    "    print(\"‚úÖ AST PARSING ENABLED (optimal)\")\n",
    "    print(\"   Preprocessing will use full AST structure\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  AST PARSING WILL USE FALLBACK MODE\")\n",
    "    print(\"   Preprocessing will use token-sequence graphs\")\n",
    "    print(\"   ‚úì Training will still work correctly\")\n",
    "    print(\"   ‚úì Performance impact: minimal (<5%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD2ONUWq_DzF"
   },
   "source": [
    "### Platform Notes: tree-sitter on Windows/Linux\n",
    "\n",
    "**Google Colab (Linux):**\n",
    "- ‚úÖ Works out-of-the-box with `.so` libraries\n",
    "- ‚úÖ GCC compiler available by default\n",
    "\n",
    "**Windows (Local Development):**\n",
    "- ‚ö†Ô∏è  Requires Microsoft Visual C++ 14.0+ (MSVC)\n",
    "- ‚ö†Ô∏è  May fail with \"compiler not found\" errors\n",
    "- **Solution 1:** Use WSL (Windows Subsystem for Linux) for preprocessing\n",
    "- **Solution 2:** Use Colab for all preprocessing tasks\n",
    "- **Solution 3:** Install Visual Studio Build Tools (large download)\n",
    "- ‚úì **Fallback:** Token-sequence graphs work fine (<5% performance impact)\n",
    "\n",
    "**Recommendation:** For Windows users, use Colab for data preprocessing and training. Download preprocessed data to Windows only for inference/deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Part 1.5: Pre-Training Validation Tests (Issue #11 Fix Verification)\n\n**IMPORTANT:** Run these test cells BEFORE full training to verify all Issue #11 fixes are working correctly.\n\nThese tests verify:\n1. ‚úÖ Class-balanced loss is working (model doesn't collapse to one class)\n2. ‚úÖ LR scaling and warmup are correct\n3. ‚úÖ Scheduler steps properly (per-step, not per-epoch)\n4. ‚úÖ Gradient clipping prevents exploding gradients\n5. ‚úÖ Prediction distribution monitoring detects collapse\n6. ‚úÖ Checkpoint saving/loading works with PyTorch 2.6+\n\n**Expected Results:**\n- **Test 1 (Tiny Overfitting Test):** Loss should decrease to near 0, F1 should reach 0.9+\n- **Test 2 (Short Full-Data Test):** F1 should increase each epoch, prediction distribution should be balanced\n- If tests pass, proceed to full training with confidence!\n\n**Duration:** 5-10 minutes total",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24306,
     "status": "ok",
     "timestamp": 1761738758091,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "Q29KEnue_DzG",
    "outputId": "3ac20003-aa83-4d61-82a2-b41977186f48"
   },
   "outputs": [],
   "source": "# Cell 6: Setup data from Google Drive\nimport os\nimport shutil\nfrom pathlib import Path\nimport json\n\nprint(\"=\"*70)\nprint(\"SETTING UP DATA FROM GOOGLE DRIVE\")\nprint(\"=\"*70)\n\n# Ensure we're in the streamguard directory\nos.chdir('/content/streamguard')\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Step 1: Mount Google Drive\nprint(f\"\\n[1/5] Mounting Google Drive...\")\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=False)\nprint(\"‚úì Google Drive mounted\")\n\n# Step 2: Check if data exists in Drive\ndrive_data_path = Path('/content/drive/MyDrive/streamguard/data/processed/codexglue')\nprint(f\"\\n[2/5] Checking for data in Google Drive...\")\nprint(f\"   Looking in: {drive_data_path}\")\n\nif not drive_data_path.exists():\n    print(f\"‚ùå ERROR: Data not found in Google Drive!\")\n    print(f\"\\nüí° Please upload the preprocessed data to Google Drive:\")\n    print(f\"   1. Create folder: My Drive/streamguard/data/processed/codexglue/\")\n    print(f\"   2. Upload these files:\")\n    print(f\"      ‚Ä¢ train.jsonl (504 MB)\")\n    print(f\"      ‚Ä¢ valid.jsonl (63 MB)\")\n    print(f\"      ‚Ä¢ test.jsonl (63 MB)\")\n    print(f\"      ‚Ä¢ preprocessing_metadata.json (1.6 KB)\")\n    print(f\"\\n   Total: ~630 MB\")\n    raise FileNotFoundError(f\"Data not found in Drive: {drive_data_path}\")\n\nprint(f\"‚úì Data found in Google Drive\")\n\n# Step 3: Check all required files\nprint(f\"\\n[3/5] Verifying data files in Drive...\")\nrequired_files = ['train.jsonl', 'valid.jsonl', 'test.jsonl', 'preprocessing_metadata.json']\nmissing_files = []\n\ndrive_sizes = {}\nfor file in required_files:\n    file_path = drive_data_path / file\n    if file_path.exists():\n        size_mb = file_path.stat().st_size / (1024 * 1024)\n        drive_sizes[file] = size_mb\n        print(f\"  ‚úì {file:<30} ({size_mb:>8.2f} MB)\")\n    else:\n        print(f\"  ‚ùå {file:<30} MISSING\")\n        missing_files.append(file)\n\nif missing_files:\n    print(f\"\\n‚ùå ERROR: Missing {len(missing_files)} required file(s) in Drive\")\n    print(f\"   Missing: {', '.join(missing_files)}\")\n    raise FileNotFoundError(f\"Missing data files in Drive: {missing_files}\")\n\ntotal_size = sum(drive_sizes.values())\nprint(f\"\\nüì¶ Total data size in Drive: {total_size:.2f} MB\")\n\n# Step 4: Create local data directory and copy files\nlocal_data_path = Path('/content/streamguard/data/processed/codexglue')\nlocal_data_path.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\n[4/5] Copying data from Drive to Colab local storage...\")\nprint(f\"   Source: {drive_data_path}\")\nprint(f\"   Destination: {local_data_path}\")\nprint(f\"   (This provides faster I/O during training)\\n\")\n\nfor file in required_files:\n    src = drive_data_path / file\n    dst = local_data_path / file\n    \n    if dst.exists():\n        # Check if sizes match (skip if already copied)\n        src_size = src.stat().st_size\n        dst_size = dst.stat().st_size\n        if src_size == dst_size:\n            print(f\"  ‚úì {file:<30} (already copied, skipping)\")\n            continue\n    \n    print(f\"  üìã Copying {file:<30} ({drive_sizes[file]:.2f} MB)...\", end='', flush=True)\n    shutil.copy2(src, dst)\n    print(\" ‚úì\")\n\nprint(f\"\\n‚úÖ All data files copied to local storage!\")\n\n# Step 5: Load and display metadata\nprint(f\"\\n[5/5] Loading dataset statistics...\")\nmetadata_path = local_data_path / 'preprocessing_metadata.json'\nif metadata_path.exists():\n    with open(metadata_path, 'r') as f:\n        metadata = json.load(f)\n    \n    print(f\"\\nüìä Dataset Statistics:\")\n    total_samples = 0\n    for split in ['train', 'validation', 'test']:\n        if split in metadata:\n            count = metadata[split].get('total_samples', 0)\n            total_samples += count\n            print(f\"  {split.capitalize():<12}: {count:>6} samples\")\n    \n    print(f\"\\nüí° Total samples: {total_samples:,}\")\n    \n    # Show class distribution if available\n    if 'train' in metadata and 'label_distribution' in metadata['train']:\n        dist = metadata['train']['label_distribution']\n        print(f\"\\nüìä Class Distribution (Training Set):\")\n        for label, count in dist.items():\n            percentage = (count / metadata['train']['total_samples']) * 100\n            print(f\"  {label:<15}: {count:>6} ({percentage:>5.1f}%)\")\nelse:\n    print(f\"  ‚ö†Ô∏è  Metadata file not found\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ DATA SETUP COMPLETE - Ready for training!\")\nprint(\"=\"*70)\nprint(f\"\\nüí° Training scripts will read from:\")\nprint(f\"   ‚Ä¢ {local_data_path / 'train.jsonl'}\")\nprint(f\"   ‚Ä¢ {local_data_path / 'valid.jsonl'}\")\nprint(f\"   ‚Ä¢ {local_data_path / 'test.jsonl'}\")\nprint(f\"\\nüíæ Data is now in Colab local storage (faster I/O than Drive)\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "source": "# Cell 6.5: TEST 1 - Tiny Overfitting Test (Issue #11 Fix Verification)\n# This test uses 64 samples for 10 epochs to verify the model can learn\n\nimport os\nos.chdir('/content/streamguard')\n\nprint(\"=\"*70)\nprint(\"TEST 1: TINY OVERFITTING TEST (Issue #11 Fix Verification)\")\nprint(\"=\"*70)\nprint(\"Purpose: Verify model can learn on a tiny subset\")\nprint(\"Expected: Loss ‚Üí 0, F1 ‚Üí 0.9+, balanced predictions\")\nprint(\"Duration: ~2-3 minutes\")\nprint(\"=\"*70)\n\n!python training/train_transformer.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --quick-test \\\n  --epochs 10 \\\n  --batch-size 8 \\\n  --max-seq-len 512 \\\n  --lr 2e-5 \\\n  --weight-decay 0.01 \\\n  --warmup-ratio 0.1 \\\n  --dropout 0.1 \\\n  --seed 42\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ TEST 1 COMPLETE\")\nprint(\"=\"*70)\nprint(\"\\nüìã What to check:\")\nprint(\"  1. Loss should decrease steadily (should reach < 0.5)\")\nprint(\"  2. F1 score should increase (should reach > 0.7)\")\nprint(\"  3. Prediction distribution should be balanced\")\nprint(\"  4. No collapse warnings (model predicting only one class)\")\nprint(\"\\nIf all checks pass, proceed to Test 2!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6.6: TEST 2 - Short Full-Data Test (Issue #11 Fix Verification)\n# This test uses full data for 2-3 epochs to verify training stability\n\nimport os\nos.chdir('/content/streamguard')\n\nprint(\"=\"*70)\nprint(\"TEST 2: SHORT FULL-DATA TEST (Issue #11 Fix Verification)\")\nprint(\"=\"*70)\nprint(\"Purpose: Verify training stability with full dataset\")\nprint(\"Expected: F1 increases each epoch, balanced predictions, no collapse\")\nprint(\"Duration: ~10-15 minutes (depending on GPU)\")\nprint(\"=\"*70)\n\n!python training/train_transformer.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /tmp/test_transformer \\\n  --epochs 3 \\\n  --batch-size 16 \\\n  --max-seq-len 512 \\\n  --lr 2e-5 \\\n  --weight-decay 0.01 \\\n  --warmup-ratio 0.1 \\\n  --dropout 0.1 \\\n  --seed 42\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ TEST 2 COMPLETE\")\nprint(\"=\"*70)\nprint(\"\\nüìã What to check:\")\nprint(\"  1. F1 score should increase each epoch\")\nprint(\"  2. Prediction distribution should be balanced (check the logs)\")\nprint(\"  3. No collapse warnings\")\nprint(\"  4. Class weights are being used (check '[*] Class distribution' in logs)\")\nprint(\"  5. LR scaling is applied (check '[*] Scaling LR' in logs)\")\nprint(\"\\nIf all checks pass, proceed to full training (Cell 7)!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nx0Gw_T_DzH"
   },
   "source": [
    "---\n",
    "## Part 2: Transformer Training (2-3 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jN9tmhZy_DzH",
    "outputId": "e01d13de-7ffb-4daf-fb68-68ac4cd0c844"
   },
   "outputs": [],
   "source": "# Cell 7: Transformer training with adaptive configuration\nimport os\nimport json\nfrom pathlib import Path\n\nos.chdir('/content/streamguard')\n\n# Load adaptive configuration with fallback\nconfig_path = Path('/tmp/gpu_training_config.json')\nif config_path.exists():\n    with open(config_path, 'r') as f:\n        config_data = json.load(f)\n    t_config = config_data['config']['transformer']\n    config_tier = config_data['tier']\n    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\nelse:\n    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n    t_config = {'epochs': 10, 'batch_size': 32, 'max_seq_len': 512, 'patience': 2}\n    config_tier = 'OPTIMIZED (Default)'\n\nprint(\"=\"*70)\nprint(\"STARTING TRANSFORMER TRAINING\")\nprint(\"=\"*70)\nprint(f\"Configuration: {config_tier}\")\nprint(f\"Epochs: {t_config['epochs']}\")\nprint(f\"Batch Size: {t_config['batch_size']}\")\nprint(f\"Max Seq Length: {t_config['max_seq_len']}\")\nprint(f\"Early Stopping Patience: {t_config['patience']}\")\nprint(\"\\n‚ö†Ô∏è  NOTE: --mixed-precision DISABLED for initial testing\")\nprint(\"   Re-enable after confirming training stability (3-4 epochs)\")\nprint(\"\\nüí° Data: Make sure your preprocessed data is in:\")\nprint(\"   data/processed/codexglue/ (train.jsonl, valid.jsonl, test.jsonl)\")\nprint(\"=\"*70)\n\n!python training/train_transformer.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /content/models/transformer_phase1 \\\n  --epochs {t_config['epochs']} \\\n  --batch-size {t_config['batch_size']} \\\n  --max-seq-len {t_config['max_seq_len']} \\\n  --lr 2e-5 \\\n  --weight-decay 0.01 \\\n  --warmup-ratio 0.1 \\\n  --dropout 0.1 \\\n  --early-stopping-patience {t_config['patience']} \\\n  --seed 42"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1761739833920,
     "user": {
      "displayName": "Vimal Sajan",
      "userId": "09966131838234738919"
     },
     "user_tz": -330
    },
    "id": "065HsQcB4HV5",
    "outputId": "fd5bd540-48ed-4487-d938-a0af045c764d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "total 32\n",
      "drwxr-xr-x 1 root root 4096 Oct 29 11:52 .\n",
      "drwxr-xr-x 1 root root 4096 Oct 29 09:13 ..\n",
      "drwxr-xr-x 2 root root 4096 Oct 29 11:38 build\n",
      "drwxr-xr-x 4 root root 4096 Oct 27 13:37 .config\n",
      "drwxr-xr-x 3 root root 4096 Oct 29 11:52 data\n",
      "drwx------ 5 root root 4096 Oct 29 11:49 drive\n",
      "drwxr-xr-x 1 root root 4096 Oct 27 13:37 sample_data\n",
      "drwxr-xr-x 3 root root 4096 Oct 29 11:38 vendor\n",
      "total 16\n",
      "dr-x------ 4 root root 4096 Oct 29 11:49 .Encrypted\n",
      "drwx------ 7 root root 4096 Oct 29 11:49 MyDrive\n",
      "dr-x------ 2 root root 4096 Oct 29 11:49 .shortcut-targets-by-id\n",
      "drwx------ 5 root root 4096 Oct 29 11:49 .Trash-0\n"
     ]
    }
   ],
   "source": [
    "# run in a notebook cell\n",
    "!pwd\n",
    "!ls -la /content\n",
    "!ls -la /content/drive || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fVygiuW_DzH"
   },
   "outputs": [],
   "source": "# Cell 7: Transformer training (static config for reference)\nimport os\nos.chdir('/content/streamguard')\n\nprint(\"=\"*70)\nprint(\"STARTING TRANSFORMER TRAINING\")\nprint(\"=\"*70)\nprint(\"Expected duration: 2-3 hours\")\nprint(\"\\n‚ö†Ô∏è  NOTE: --mixed-precision DISABLED for initial testing\")\nprint(\"   Re-enable after confirming training stability (3-4 epochs)\")\nprint(\"\\nüí° Data: Make sure your preprocessed data is in:\")\nprint(\"   data/processed/codexglue/ (train.jsonl, valid.jsonl, test.jsonl)\")\nprint(\"=\"*70)\n\n!python training/train_transformer.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /content/models/transformer_phase1 \\\n  --epochs 5 \\\n  --batch-size 16 \\\n  --lr 2e-5 \\\n  --weight-decay 0.01 \\\n  --warmup-ratio 0.1 \\\n  --max-seq-len 512 \\\n  --dropout 0.1 \\\n  --early-stopping-patience 2 \\\n  --seed 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VhIXkhD_DzH"
   },
   "source": [
    "---\n",
    "## Part 3: GNN Training (4-6 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaZPf1Sp_DzI"
   },
   "outputs": [],
   "source": "# Cell 9: GNN training with adaptive configuration\nimport os\nimport json\nfrom pathlib import Path\n\nos.chdir('/content/streamguard')\n\n# Load adaptive configuration with fallback\nconfig_path = Path('/tmp/gpu_training_config.json')\nif config_path.exists():\n    with open(config_path, 'r') as f:\n        config_data = json.load(f)\n    g_config = config_data['config']['gnn']\n    config_tier = config_data['tier']\n    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\nelse:\n    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n    g_config = {'epochs': 150, 'batch_size': 64, 'hidden_dim': 256, 'num_layers': 4, 'patience': 10}\n    config_tier = 'OPTIMIZED (Default)'\n\nprint(\"=\"*70)\nprint(\"STARTING GNN TRAINING\")\nprint(\"=\"*70)\nprint(f\"Configuration: {config_tier}\")\nprint(f\"Epochs: {g_config['epochs']}\")\nprint(f\"Batch Size: {g_config['batch_size']}\")\nprint(f\"Hidden Dimensions: {g_config['hidden_dim']}\")\nprint(f\"Num Layers: {g_config['num_layers']}\")\nprint(f\"Early Stopping Patience: {g_config['patience']}\")\nprint(\"=\"*70)\n\n!python training/train_gnn.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /content/models/gnn_phase1 \\\n  --epochs {g_config['epochs']} \\\n  --batch-size {g_config['batch_size']} \\\n  --hidden-dim {g_config['hidden_dim']} \\\n  --num-layers {g_config['num_layers']} \\\n  --lr 1e-3 \\\n  --weight-decay 1e-4 \\\n  --dropout 0.3 \\\n  --early-stopping-patience {g_config['patience']} \\\n  --auto-batch-size \\\n  --seed 42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIGq25-o_DzI"
   },
   "outputs": [],
   "source": [
    "# Cell 11: Fusion training with fallback config\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir('/content/streamguard')\n",
    "\n",
    "# Load adaptive configuration with fallback\n",
    "config_path = Path('/tmp/gpu_training_config.json')\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "    f_config = config_data['config']['fusion']\n",
    "    config_tier = config_data['tier']\n",
    "    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n",
    "    f_config = {'n_folds': 5, 'epochs': 30}\n",
    "    config_tier = 'OPTIMIZED (Default)'\n",
    "    config_data = {'tier': config_tier, 'gpu': 'Unknown'}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING FUSION TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configuration: {config_tier}\")\n",
    "print(f\"N-Folds (OOF): {f_config['n_folds']}\")\n",
    "print(f\"Epochs: {f_config['epochs']}\")\n",
    "\n",
    "# Display performance note based on config\n",
    "if 'OPTIMIZED' in config_tier:\n",
    "    print(\"\\nüí° T4/DEFAULT CONFIGURATION:\")\n",
    "    print(\"   Using n_folds=5 for good ensemble robustness\")\n",
    "    print(\"   Larger batches and extended training can improve quality\")\n",
    "elif 'ENHANCED' in config_tier:\n",
    "    print(\"\\nüí° V100 CONFIGURATION:\")\n",
    "    print(\"   Using n_folds=5, 2-3x faster than T4\")\n",
    "    print(\"   Larger batches for better gradient estimates\")\n",
    "elif 'AGGRESSIVE' in config_tier:\n",
    "    print(\"\\nüí° A100 MAXIMUM CONFIGURATION:\")\n",
    "    print(\"   Using n_folds=10 for maximum robustness\")\n",
    "    print(\"   Extended training for highest quality\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python training/train_fusion.py \\\n",
    "  --train-data /content/data/processed/codexglue/train.jsonl \\\n",
    "  --val-data /content/data/processed/codexglue/valid.jsonl \\\n",
    "  --test-data /content/data/processed/codexglue/test.jsonl \\\n",
    "  --output-dir /content/models/fusion_phase1 \\\n",
    "  --transformer-checkpoint /content/models/transformer_phase1/checkpoints/best_model.pt \\\n",
    "  --gnn-checkpoint /content/models/gnn_phase1/checkpoints/best_model.pt \\\n",
    "  --n-folds {f_config['n_folds']} \\\n",
    "  --epochs {f_config['epochs']} \\\n",
    "  --lr 1e-3 \\\n",
    "  --seed 42\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FUSION TRAINING COMPLETE\")\n",
    "print(f\"Configuration: {config_tier}\")\n",
    "print(f\"Folds trained: {f_config['n_folds']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB81siKS_DzI"
   },
   "source": "# Cell 9: GNN training (static config for reference)\nimport os\nos.chdir('/content/streamguard')\n\nprint(\"=\"*70)\nprint(\"STARTING GNN TRAINING\")\nprint(\"=\"*70)\nprint(\"Expected duration: 4-6 hours\")\nprint(\"=\"*70)\n\n!python training/train_gnn.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /content/models/gnn_phase1 \\\n  --epochs 100 \\\n  --batch-size 32 \\\n  --lr 1e-3 \\\n  --weight-decay 1e-4 \\\n  --hidden-dim 256 \\\n  --num-layers 4 \\\n  --dropout 0.3 \\\n  --early-stopping-patience 10 \\\n  --auto-batch-size \\\n  --seed 42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXTOcOtf_DzI"
   },
   "outputs": [],
   "source": "# Cell 11: Fusion training with adaptive configuration\nimport os\nimport json\nfrom pathlib import Path\n\nos.chdir('/content/streamguard')\n\n# Load adaptive configuration with fallback\nconfig_path = Path('/tmp/gpu_training_config.json')\nif config_path.exists():\n    with open(config_path, 'r') as f:\n        config_data = json.load(f)\n    f_config = config_data['config']['fusion']\n    config_tier = config_data['tier']\n    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\nelse:\n    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n    f_config = {'n_folds': 5, 'epochs': 30}\n    config_tier = 'OPTIMIZED (Default)'\n    config_data = {'tier': config_tier, 'gpu': 'Unknown'}\n\nprint(\"=\"*70)\nprint(\"STARTING FUSION TRAINING\")\nprint(\"=\"*70)\nprint(f\"Configuration: {config_tier}\")\nprint(f\"N-Folds (OOF): {f_config['n_folds']}\")\nprint(f\"Epochs: {f_config['epochs']}\")\n\n# Display performance note based on config\nif 'OPTIMIZED' in config_tier:\n    print(\"\\nüí° T4/DEFAULT CONFIGURATION:\")\n    print(\"   Using n_folds=5 for good ensemble robustness\")\n    print(\"   Larger batches and extended training can improve quality\")\nelif 'ENHANCED' in config_tier:\n    print(\"\\nüí° V100 CONFIGURATION:\")\n    print(\"   Using n_folds=5, 2-3x faster than T4\")\n    print(\"   Larger batches for better gradient estimates\")\nelif 'AGGRESSIVE' in config_tier:\n    print(\"\\nüí° A100 MAXIMUM CONFIGURATION:\")\n    print(\"   Using n_folds=10 for maximum robustness\")\n    print(\"   Extended training for highest quality\")\n\nprint(\"=\"*70)\n\n!python training/train_fusion.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /content/models/fusion_phase1 \\\n  --transformer-checkpoint /content/models/transformer_phase1/checkpoints/best_model.pt \\\n  --gnn-checkpoint /content/models/gnn_phase1/checkpoints/best_model.pt \\\n  --n-folds {f_config['n_folds']} \\\n  --epochs {f_config['epochs']} \\\n  --lr 1e-3 \\\n  --seed 42\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìä FUSION TRAINING COMPLETE\")\nprint(f\"Configuration: {config_tier}\")\nprint(f\"Folds trained: {f_config['n_folds']}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pMVOsdT_DzJ"
   },
   "outputs": [],
   "source": [
    "# Cell 12: Save Fusion to Drive\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "drive_fusion = Path('/content/drive/MyDrive/streamguard/models/fusion_phase1')\n",
    "drive_fusion.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "local_fusion = Path('/content/models/fusion_phase1')\n",
    "\n",
    "print(\"Saving Fusion model to Google Drive...\")\n",
    "\n",
    "for file in local_fusion.glob('*'):\n",
    "    if file.is_file():\n",
    "        shutil.copy2(file, drive_fusion / file.name)\n",
    "        print(f\"  ‚úì {file.name} saved\")\n",
    "\n",
    "print(f\"\\n‚úÖ Fusion saved to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1Qh1BvG_DzJ"
   },
   "source": "# Cell 11: Fusion training (static config for reference)\nimport os\nos.chdir('/content/streamguard')\n\nprint(\"=\"*70)\nprint(\"STARTING FUSION TRAINING\")\nprint(\"=\"*70)\nprint(\"Expected duration: 2-3 hours (n_folds=3)\")\nprint(\"Note: Using n_folds=3 for Colab (5-fold for SageMaker/powerful hardware)\")\nprint(\"=\"*70)\n\n# CRITICAL FIX #2: Reduced n_folds for Colab constraints\n# 5-fold OOF increases runtime significantly on limited GPU instances\n# 3-fold provides good speed/robustness tradeoff for Colab\n!python training/train_fusion.py \\\n  --train-data data/processed/codexglue/train.jsonl \\\n  --val-data data/processed/codexglue/valid.jsonl \\\n  --test-data data/processed/codexglue/test.jsonl \\\n  --output-dir /content/models/fusion_phase1 \\\n  --transformer-checkpoint /content/models/transformer_phase1/checkpoints/best_model.pt \\\n  --gnn-checkpoint /content/models/gnn_phase1/checkpoints/best_model.pt \\\n  --n-folds 3 \\\n  --epochs 20 \\\n  --lr 1e-3 \\\n  --seed 42\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üí° PERFORMANCE NOTE:\")\nprint(\"  - n_folds=3 used for Colab (good speed/robustness tradeoff)\")\nprint(\"  - For production with powerful hardware, use n_folds=5\")\nprint(\"  - 3-fold OOF typically achieves 95-98% of 5-fold performance\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7OP1__A_DzJ"
   },
   "outputs": [],
   "source": [
    "# Cell 14: Final backup\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "backup_dir = Path(f'/content/drive/MyDrive/streamguard/backups/training_{timestamp}')\n",
    "backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Creating backup: {backup_dir}\")\n",
    "\n",
    "for model_name in ['transformer_phase1', 'gnn_phase1', 'fusion_phase1']:\n",
    "    src = Path(f'/content/models/{model_name}')\n",
    "    if src.exists():\n",
    "        dst = backup_dir / model_name\n",
    "        print(f\"  Backing up {model_name}...\", end='')\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "        print(\" ‚úì\")\n",
    "\n",
    "if Path('/content/evaluation_results.json').exists():\n",
    "    shutil.copy2(\n",
    "        '/content/evaluation_results.json',\n",
    "        backup_dir / 'evaluation_results.json'\n",
    "    )\n",
    "    print(\"  ‚úì Evaluation results\")\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'models': ['transformer_phase1', 'gnn_phase1', 'fusion_phase1'],\n",
    "    'status': 'complete',\n",
    "    'notebook_version': '1.1_critical_fixes'\n",
    "}\n",
    "\n",
    "with open(backup_dir / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Backup complete: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY8F1pbQ_DzJ"
   },
   "source": [
    "---\n",
    "## Training Complete! üéâ\n",
    "\n",
    "Your models are now saved in Google Drive at:\n",
    "- `My Drive/streamguard/models/transformer_phase1/`\n",
    "- `My Drive/streamguard/models/gnn_phase1/`\n",
    "- `My Drive/streamguard/models/fusion_phase1/`\n",
    "\n",
    "**Critical Fixes Applied:**\n",
    "- ‚úÖ Runtime PyTorch/CUDA detection\n",
    "- ‚úÖ Robust tree-sitter with fallback\n",
    "- ‚úÖ Version compatibility validation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download models from Google Drive\n",
    "2. Deploy to production (see deployment guide)\n",
    "3. Optional: Run Phase 2 with collector data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}