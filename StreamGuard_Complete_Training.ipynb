{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71obQ29i_Dy8"
      },
      "source": [
        "# StreamGuard ML Training - Complete Notebook\n",
        "\n",
        "**Version:** 1.7 (Safety Features Available - See instructions at end)  \n",
        "**Last Updated:** 2025-11-01  \n",
        "**Platform:** Google Colab (Free/Pro/Pro+)  \n",
        "**GPU:** T4/V100/A100 (Adaptive Configuration)  \n",
        "**Duration:** 11-24 hours (depends on GPU & config)  \n",
        "\n",
        "This notebook trains all three StreamGuard models with **adaptive configuration** that automatically optimizes for your GPU.\n",
        "\n",
        "## üéØ Training Phases\n",
        "1. **Enhanced SQL Intent Transformer** (2-8 hours depending on GPU)\n",
        "2. **Enhanced Taint-Flow GNN** (4-12 hours depending on GPU)\n",
        "3. **Fusion Layer** (2-10 hours depending on GPU)\n",
        "\n",
        "## ‚ú® What's New in v1.7 (Safety Features)\n",
        "\n",
        "**NEW: Optional Safety Features Available**\n",
        "- ‚úÖ **LR Finder with Safety Validation** (auto-detects optimal learning rate, 5e-4 cap, smart fallback)\n",
        "- ‚úÖ **LR Caching** (skip 5-10 min LR Finder on reruns, 168-hour cache)\n",
        "- ‚úÖ **Triple Weighting Auto-Adjustment** (prevents overcorrection when using sampler + weights + focal)\n",
        "- ‚úÖ **Enhanced Checkpoint Metadata** (includes seed, git commit, LR analysis)\n",
        "- ‚úÖ **Unit Tests** (14 tests verify all safety features)\n",
        "\n",
        "**See instructions at the END of this notebook for how to use these features.**\n",
        "\n",
        "**Backward Compatible:** All existing cells work exactly as before. New features are opt-in via CLI flags.\n",
        "\n",
        "## ‚ú® What's New in v1.6 (Issue #11 - Training Collapse Fix)\n",
        "\n",
        "### **CRITICAL: Training Collapse Fixed (Issue #11)**\n",
        "- ‚úÖ **Class-balanced loss with inverse frequency weights** (fixes model predicting only safe class)\n",
        "- ‚úÖ **LR scaling for large batches** (square-root rule: batch 64 gets 2x base LR)\n",
        "- ‚úÖ **Per-step scheduler** (moved inside train_epoch, was per-epoch before)\n",
        "- ‚úÖ **Gradient clipping** (max_norm=1.0 prevents exploding gradients)\n",
        "- ‚úÖ **Prediction distribution monitoring** (detects collapse early)\n",
        "- ‚úÖ **Enhanced collapse detection** (stops training if model predicts only one class)\n",
        "- ‚úÖ **Conservative label smoothing** (0.05 instead of 0.1)\n",
        "- ‚úÖ **Simplified loss calculation** (removed unnecessary sample-level weighting)\n",
        "\n",
        "**Root Cause (Issue #11):** Model collapsed from F1=0.4337 (epoch 1) to F1=0.0000 (epoch 3+) due to:\n",
        "1. No class balancing (54.2% safe vs 45.8% vulnerable)\n",
        "2. LR designed for batch=16 but using batch=64\n",
        "3. Scheduler stepping per-epoch instead of per-step\n",
        "4. No gradient clipping\n",
        "5. No early collapse detection\n",
        "\n",
        "**The Fix:** All 8 critical fixes implemented in train_transformer.py (see `docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md`)\n",
        "\n",
        "### **Previous Fixes (v1.5 - Issue #10)**\n",
        "- ‚úÖ **Max seq length configuration fixed** (512 for all GPUs, not 1024/768)\n",
        "- ‚úÖ **Automatic validation** to prevent exceeding CodeBERT's 512-token limit\n",
        "- ‚úÖ **Tensor size mismatch error prevented**\n",
        "- ‚úÖ **Updated PyTorch AMP API** (torch.amp instead of torch.cuda.amp)\n",
        "\n",
        "### **Previous Fixes (v1.4 - Issue #9)**\n",
        "- ‚úÖ **Fixed CrossEntropyLoss tensor-to-scalar error**\n",
        "- ‚úÖ **Fixed sample weights handling**\n",
        "- ‚úÖ **Updated deprecated autocast/GradScaler**\n",
        "- ‚úÖ **Added Cell 1.5** (robust GPU detection with fallback)\n",
        "\n",
        "### **Previous Fixes (v1.3 - Issue #8)**\n",
        "- ‚úÖ **Fixed NumPy binary incompatibility** (numpy==1.26.4 enforced)\n",
        "- ‚úÖ **Fixed tokenizers/transformers conflict** (tokenizers 0.14.1)\n",
        "- ‚úÖ **Fixed PyG circular import errors**\n",
        "\n",
        "### **Adaptive GPU Configuration (Colab Pro)**\n",
        "- üîç **Auto-detects GPU type** (T4/V100/A100) via Cell 1.5\n",
        "- ‚öôÔ∏è  **Selects optimal hyperparameters** automatically\n",
        "- üìä **Three configuration tiers**:\n",
        "  - **OPTIMIZED** (T4): 10/150/30 epochs, batch 32/64, seq 512, ~13-17h\n",
        "  - **ENHANCED** (V100): 15/200/50 epochs, batch 48/96, seq 512, ~18-22h (2-3x faster)\n",
        "  - **AGGRESSIVE** (A100): 20/300/100 epochs, batch 64/128, seq 512, ~20-24h (5-7x faster)\n",
        "\n",
        "**Note:** All configurations use `max_seq_len = 512` (CodeBERT/RoBERTa model limit). Better GPUs benefit from larger batch sizes and more epochs.\n",
        "\n",
        "### **Colab Pro Benefits**\n",
        "- ‚úÖ 24-hour runtime (vs 12h free)\n",
        "- ‚úÖ Better GPU access (V100, A100)\n",
        "- ‚úÖ Background execution\n",
        "- ‚úÖ **Larger batches ‚Üí better gradient estimates**\n",
        "\n",
        "**Recommended:** V100 on Colab Pro ($10/mo) for best balance of speed and availability.\n",
        "\n",
        "## üîß All Critical Fixes Applied (v1.1 ‚Üí v1.7)\n",
        "\n",
        "### **v1.7 Fixes (Safety Features) - NEW**\n",
        "- ‚úÖ LR Finder with safety validation (5e-4 cap, 1e-5 fallback)\n",
        "- ‚úÖ LR caching (168-hour default, dataset fingerprint-based)\n",
        "- ‚úÖ Triple weighting auto-adjustment (20% reduction when all enabled)\n",
        "- ‚úÖ Enhanced checkpoint metadata (seed, git, LR analysis)\n",
        "- ‚úÖ Unit tests (14 tests for all safety features)\n",
        "\n",
        "### **v1.6 Fixes (Issue #11)**\n",
        "- ‚úÖ Class-balanced loss with inverse frequency weights\n",
        "- ‚úÖ LR scaling for large batches (square-root rule)\n",
        "- ‚úÖ Warmup ratio adjustment (proportional, capped at 20%)\n",
        "- ‚úÖ Per-step scheduler (moved inside train_epoch)\n",
        "- ‚úÖ Gradient clipping (max_norm=1.0)\n",
        "- ‚úÖ Prediction distribution monitoring\n",
        "- ‚úÖ Enhanced collapse detection\n",
        "- ‚úÖ Conservative label smoothing (0.05)\n",
        "- ‚úÖ Drive-based data workflow (automatic copy to local storage)\n",
        "- ‚úÖ Pre-training validation tests\n",
        "\n",
        "### **v1.5 Fixes (Issue #10)**\n",
        "- ‚úÖ Max seq length configuration fixed\n",
        "- ‚úÖ Automatic validation added\n",
        "- ‚úÖ Tensor size mismatch prevented\n",
        "- ‚úÖ PyTorch AMP API updated\n",
        "\n",
        "### **v1.4 Fixes (Issue #9)**\n",
        "- ‚úÖ CrossEntropyLoss tensor-to-scalar error fixed\n",
        "- ‚úÖ Sample weights handling validated\n",
        "- ‚úÖ Deprecated API updated\n",
        "- ‚úÖ GPU detection robustness improved\n",
        "\n",
        "### **v1.3 Fixes (Issue #8)**\n",
        "- ‚úÖ NumPy binary compatibility fixed\n",
        "- ‚úÖ tokenizers/transformers conflict resolved\n",
        "- ‚úÖ PyG circular import fixed\n",
        "\n",
        "### **v1.1-v1.2 Fixes (Issues #1-#7)**\n",
        "- ‚úÖ Runtime-aware PyTorch Geometric installation\n",
        "- ‚úÖ Robust tree-sitter build with fallback\n",
        "- ‚úÖ Version compatibility validation\n",
        "- ‚úÖ Enhanced dependency conflict detection\n",
        "- ‚úÖ Optimized OOF fusion\n",
        "\n",
        "## üìã Before Starting\n",
        "\n",
        "### **Colab Configuration:**\n",
        "1. Enable GPU: **Runtime ‚Üí Change runtime type ‚Üí GPU**\n",
        "2. **Recommended:** Subscribe to Colab Pro ($10/mo) for:\n",
        "   - 24-hour runtime (required for full training)\n",
        "   - Access to V100/A100 GPUs (2-7x faster than T4)\n",
        "   - Background execution\n",
        "\n",
        "### **Data Requirements - IMPORTANT:**\n",
        "\n",
        "**You MUST upload preprocessed data files to Google Drive:**\n",
        "\n",
        "```\n",
        "My Drive/streamguard/data/processed/codexglue/\n",
        "‚îú‚îÄ‚îÄ train.jsonl (504 MB, 21,854 samples)\n",
        "‚îú‚îÄ‚îÄ valid.jsonl (63 MB, 2,732 samples)\n",
        "‚îú‚îÄ‚îÄ test.jsonl (63 MB, 2,732 samples)\n",
        "‚îî‚îÄ‚îÄ preprocessing_metadata.json (1.6 KB)\n",
        "```\n",
        "\n",
        "**Total size:** ~630 MB\n",
        "\n",
        "**Why Google Drive?**\n",
        "- Data files are too large for GitHub (exceeds 100 MB limit)\n",
        "- They are in `.gitignore` and won't be cloned from the repository\n",
        "- **Cell 6** will automatically mount Drive and copy data to Colab local storage\n",
        "- Local storage provides faster I/O during training (vs reading from Drive each time)\n",
        "\n",
        "**How to upload:**\n",
        "1. Open Google Drive: https://drive.google.com/\n",
        "2. Create folder structure: `My Drive/streamguard/data/processed/codexglue/`\n",
        "3. Upload the 4 data files to this folder\n",
        "4. Run notebook Cell 6 - it will copy files to Colab automatically\n",
        "\n",
        "## üìä Expected Results by Configuration\n",
        "\n",
        "| Config | GPU | Time | Batch Sizes (T/G) | Seq Len | Speed vs T4 |\n",
        "|--------|-----|------|-------------------|---------|-------------|\n",
        "| **OPTIMIZED** | T4 | 13-17h | 32 / 64 | 512 | 1.0x |\n",
        "| **ENHANCED** | V100 | 18-22h | 48 / 96 | 512 | 2-3x faster |\n",
        "| **AGGRESSIVE** | A100 | 20-24h | 64 / 128 | 512 | 5-7x faster |\n",
        "\n",
        "*Note: All configs use max_seq_len=512 (CodeBERT limit). Better GPUs use larger batches/epochs for quality.*\n",
        "\n",
        "## üöÄ Quick Start\n",
        "\n",
        "1. **Upload data to Drive** (see Data Requirements above)\n",
        "2. Run **Cell 1**: Verify GPU is enabled\n",
        "3. Run **Cell 1.5**: Auto-detect GPU and select configuration  \n",
        "4. Run **Cell 2**: Install dependencies with compatibility fixes\n",
        "5. Run **Cell 2.5**: Validate compatibility\n",
        "6. Run **Cell 3**: Clone repository from GitHub\n",
        "7. Run **Cell 4**: Setup tree-sitter\n",
        "8. Run **Cell 6**: Mount Drive and copy data to local storage ‚≠ê\n",
        "9. **Run TEST CELLS 6.5 & 6.6**: Verify Issue #11 fixes (5-15 min total)\n",
        "10. Run **Cells 7, 9, 11**: Full training with adaptive configuration\n",
        "11. Monitor progress (can close browser with Colab Pro)\n",
        "\n",
        "**IMPORTANT:** Run the test cells (6.5 & 6.6) before full training to verify all fixes are working!\n",
        "\n",
        "**NEW:** For v1.7 safety features, see instructions at the END of this notebook.\n",
        "\n",
        "## üîó Documentation\n",
        "\n",
        "- **Training Collapse Fix:** See [docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_11_TRAINING_COLLAPSE_COMPLETE_FIX.md)\n",
        "- **Final Recommendations:** See [docs/ISSUE_11_FINAL_CAUTIONS_AND_RECOMMENDATIONS.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_11_FINAL_CAUTIONS_AND_RECOMMENDATIONS.md)\n",
        "- **Max Seq Length Fix:** See [docs/ISSUE_10_MAX_SEQ_LEN_FIX.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/ISSUE_10_MAX_SEQ_LEN_FIX.md)\n",
        "- **Critical Fixes Details:** See [docs/COLAB_CRITICAL_FIXES.md](https://github.com/VimalSajanGeorge/streamguard/blob/master/docs/COLAB_CRITICAL_FIXES.md)\n",
        "- **Troubleshooting:** Check Issue #8, #9, #10, and #11 documentation for common errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ_fI8de_Dy_"
      },
      "source": [
        "---\n",
        "## Part 1: Environment Setup\n",
        "Run these cells once at the beginning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr8Lmrx7_DzA",
        "outputId": "9380a6e0-d1bc-48dd-e4ca-a34d16a3380d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.47 GB\n",
            "CUDA Version: 12.6\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Verify GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: GPU not available! Enable GPU in Runtime ‚Üí Change runtime type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3LAE7YnvL8Xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c0056e-0b50-47d4-da21-883edfa35de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ADAPTIVE GPU CONFIGURATION\n",
            "======================================================================\n",
            "Detected GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 40.00 GB\n",
            "\n",
            "Configuration Tier: AGGRESSIVE\n",
            "Note: Maximum configuration - larger batches and more epochs for best training quality\n",
            "\n",
            "Hyperparameters:\n",
            "  Transformer: 20 epochs, batch 64, seq 512\n",
            "  GNN: 300 epochs, batch 128, hidden 512\n",
            "  Fusion: 10 folds, 100 epochs\n",
            "\n",
            "üí° Note: max_seq_len is 512 for all configs (CodeBERT/RoBERTa model limit)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 1.5: GPU Detection & Adaptive Configuration (Colab Pro Optimization)\n",
        "import subprocess\n",
        "import json\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def get_gpu_info():\n",
        "    \"\"\"Detect GPU type and memory with robust fallback.\"\"\"\n",
        "    try:\n",
        "        # Try nvidia-smi first (most reliable)\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
        "            capture_output=True, text=True, timeout=5\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            lines = result.stdout.strip().split('\\n')\n",
        "            # Use first GPU if multiple\n",
        "            gpu_line = lines[0].split(',')\n",
        "            gpu_name = gpu_line[0].strip()\n",
        "\n",
        "            # Parse memory (handle \"15360 MiB\" or \"15.36 GB\")\n",
        "            mem_str = gpu_line[1].strip()\n",
        "            if 'MiB' in mem_str:\n",
        "                gpu_memory = float(re.findall(r'\\d+', mem_str)[0]) / 1024  # MiB to GB\n",
        "            else:\n",
        "                gpu_memory = float(re.findall(r'[\\d.]+', mem_str)[0])\n",
        "\n",
        "            return gpu_name, gpu_memory\n",
        "    except (subprocess.TimeoutExpired, FileNotFoundError, IndexError, ValueError):\n",
        "        pass\n",
        "\n",
        "    # Fallback to PyTorch\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Bytes to GB\n",
        "        return gpu_name, gpu_memory\n",
        "\n",
        "    # No GPU available\n",
        "    return \"CPU\", 0.0\n",
        "\n",
        "gpu_name, gpu_memory_gb = get_gpu_info()\n",
        "gpu_name_lower = gpu_name.lower()\n",
        "\n",
        "# Determine configuration tier (case-insensitive matching)\n",
        "# CRITICAL FIX (Issue #9): CodeBERT max_seq_len is 512 (514 with special tokens) - RoBERTa limitation\n",
        "# Using max_seq_len > 512 causes: RuntimeError: The expanded size of the tensor (1024) must match the existing size (514)\n",
        "if 'a100' in gpu_name_lower:\n",
        "    config_tier = 'AGGRESSIVE'\n",
        "    config = {\n",
        "        'transformer': {'epochs': 20, 'batch_size': 64, 'max_seq_len': 512, 'patience': 5},\n",
        "        'gnn': {'epochs': 300, 'batch_size': 128, 'hidden_dim': 512, 'num_layers': 5, 'patience': 15},\n",
        "        'fusion': {'n_folds': 10, 'epochs': 100}\n",
        "    }\n",
        "    note = \"Maximum configuration - larger batches and more epochs for best training quality\"\n",
        "elif 'v100' in gpu_name_lower:\n",
        "    config_tier = 'ENHANCED'\n",
        "    config = {\n",
        "        'transformer': {'epochs': 15, 'batch_size': 48, 'max_seq_len': 512, 'patience': 3},\n",
        "        'gnn': {'epochs': 200, 'batch_size': 96, 'hidden_dim': 384, 'num_layers': 5, 'patience': 12},\n",
        "        'fusion': {'n_folds': 5, 'epochs': 50}\n",
        "    }\n",
        "    note = \"Enhanced configuration - 2-3x faster than T4, larger batches for better gradient estimates\"\n",
        "else:  # T4 or other\n",
        "    config_tier = 'OPTIMIZED'\n",
        "    config = {\n",
        "        'transformer': {'epochs': 10, 'batch_size': 32, 'max_seq_len': 512, 'patience': 2},\n",
        "        'gnn': {'epochs': 150, 'batch_size': 64, 'hidden_dim': 256, 'num_layers': 4, 'patience': 10},\n",
        "        'fusion': {'n_folds': 5, 'epochs': 30}\n",
        "    }\n",
        "    note = \"Optimized for T4 - reliable and cost-effective\"\n",
        "\n",
        "# Save config for training cells\n",
        "config_data = {'tier': config_tier, 'gpu': gpu_name, 'config': config}\n",
        "with open('/tmp/gpu_training_config.json', 'w') as f:\n",
        "    json.dump(config_data, f)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ADAPTIVE GPU CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Detected GPU: {gpu_name}\")\n",
        "print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
        "print(f\"\\nConfiguration Tier: {config_tier}\")\n",
        "print(f\"Note: {note}\")\n",
        "print(\"\\nHyperparameters:\")\n",
        "print(f\"  Transformer: {config['transformer']['epochs']} epochs, batch {config['transformer']['batch_size']}, seq {config['transformer']['max_seq_len']}\")\n",
        "print(f\"  GNN: {config['gnn']['epochs']} epochs, batch {config['gnn']['batch_size']}, hidden {config['gnn']['hidden_dim']}\")\n",
        "print(f\"  Fusion: {config['fusion']['n_folds']} folds, {config['fusion']['epochs']} epochs\")\n",
        "print(\"\\nüí° Note: max_seq_len is 512 for all configs (CodeBERT/RoBERTa model limit)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4uF4Wv7_DzB",
        "outputId": "a3345c97-7207-4a9a-bb44-90f36a98b0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "INSTALLING DEPENDENCIES WITH COMPATIBILITY FIXES\n",
            "======================================================================\n",
            "\n",
            "[1/9] Ensuring NumPy compatibility...\n",
            "‚úì NumPy 1.26.4 (v1.x - already compatible)\n",
            "\n",
            "[2/9] Detecting PyTorch and CUDA versions...\n",
            "‚úì Detected PyTorch 2.8.0\n",
            "‚úì Detected CUDA 12.6\n",
            "‚úì Using wheel tag: cu126\n",
            "\n",
            "[3/9] Installing PyTorch Geometric (runtime-aware with fallback)...\n",
            "Wheel URL: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "  Installing torch-scatter...\n",
            "Running: pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "    ‚úì torch-scatter installed from wheel\n",
            "  Installing torch-sparse...\n",
            "Running: pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "    ‚úì torch-sparse installed from wheel\n",
            "  Installing torch-cluster...\n",
            "Running: pip install -q torch-cluster -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "    ‚úì torch-cluster installed from wheel\n",
            "  Installing torch-spline-conv...\n",
            "Running: pip install -q torch-spline-conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "    ‚úì torch-spline-conv installed from wheel\n",
            "Running: pip install -q torch-geometric==2.4.0\n",
            "‚úÖ PyTorch Geometric installed successfully\n",
            "\n",
            "[4/9] Installing Transformers with compatible tokenizers...\n",
            "‚ö†Ô∏è  Note: Using tokenizers 0.14.1 (compatible with transformers 4.35.0)\n",
            "Running: pip install -q transformers==4.35.0\n",
            "Running: pip install -q tokenizers==0.14.1\n",
            "‚úì Tokenizers 0.14.1 installed (compatible)\n",
            "Running: pip install -q accelerate==0.24.1\n",
            "\n",
            "[5/9] Installing tree-sitter...\n",
            "Running: pip install -q tree-sitter==0.20.4\n",
            "\n",
            "[6/9] Installing additional packages...\n",
            "Running: pip install -q scikit-learn==1.3.2 scipy==1.11.4 tqdm\n",
            "\n",
            "[7/9] Verifying installations...\n",
            "‚úì NumPy: 1.26.4 (binary compatible)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì PyTorch: 2.8.0+cu126\n",
            "‚úì PyTorch Geometric: 2.4.0\n",
            "‚úì Transformers: 4.35.0\n",
            "‚úì Tokenizers: 0.14.1\n",
            "  ‚úì Tokenizers version compatible\n",
            "‚ùå Verification failed: module 'tree_sitter' has no attribute '__version__'\n",
            "   Please restart runtime and try again\n",
            "   If issue persists, check:\n",
            "   1. NumPy version (should be 1.26.4)\n",
            "   2. Tokenizers version (should be 0.14.1)\n",
            "\n",
            "[8/9] Testing PyTorch Geometric...\n",
            "‚úì PyTorch Geometric working correctly\n",
            "‚úì Test data created: Data(x=[5, 3], edge_index=[2, 2])\n",
            "\n",
            "[9/9] Installation Summary:\n",
            "======================================================================\n",
            "‚úÖ ALL INSTALLATIONS SUCCESSFUL\n",
            "‚úì NumPy 1.x (binary compatible)\n",
            "‚úì PyTorch Geometric with correct wheels\n",
            "‚úì Transformers with compatible tokenizers\n",
            "‚úì All packages verified\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Install dependencies with runtime detection and compatibility fixes\n",
        "# ‚ö†Ô∏è CRITICAL: Includes NumPy compatibility fix, correct tokenizers version, and PyG error handling\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    \"\"\"Run shell command and return success status.\"\"\"\n",
        "    print(f\"Running: {cmd}\")\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error: {result.stderr}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INSTALLING DEPENDENCIES WITH COMPATIBILITY FIXES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# [1/9] CRITICAL: Fix NumPy version FIRST (before any torch imports)\n",
        "print(\"\\n[1/9] Ensuring NumPy compatibility...\")\n",
        "try:\n",
        "    import numpy\n",
        "    numpy_ver = numpy.__version__\n",
        "    numpy_major = int(numpy_ver.split('.')[0])\n",
        "\n",
        "    if numpy_major >= 2:\n",
        "        print(f\"‚ö†Ô∏è  Detected NumPy {numpy_ver} (v2.x)\")\n",
        "        print(\"   PyTorch wheels may have binary incompatibility\")\n",
        "        print(\"   Downgrading to NumPy 1.26.4...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.26.4\", \"--force-reinstall\"], check=True)\n",
        "        print(\"‚úì NumPy downgraded to 1.26.4\")\n",
        "        # Reload numpy\n",
        "        importlib.reload(numpy)\n",
        "        print(f\"‚úì NumPy {numpy.__version__} loaded (binary compatible)\")\n",
        "    else:\n",
        "        print(f\"‚úì NumPy {numpy_ver} (v1.x - already compatible)\")\n",
        "except ImportError:\n",
        "    print(\"NumPy not installed, installing 1.26.4...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy==1.26.4\"], check=True)\n",
        "    import numpy\n",
        "    print(f\"‚úì NumPy {numpy.__version__} installed\")\n",
        "\n",
        "# [2/9] Detect PyTorch and CUDA versions (now safe with correct numpy)\n",
        "print(\"\\n[2/9] Detecting PyTorch and CUDA versions...\")\n",
        "import torch\n",
        "\n",
        "torch_version = torch.__version__.split('+')[0]  # e.g., '2.8.0'\n",
        "cuda_version = torch.version.cuda  # e.g., '12.6'\n",
        "cuda_tag = f\"cu{cuda_version.replace('.', '')}\" if cuda_version else 'cpu'  # e.g., 'cu126'\n",
        "\n",
        "print(f\"‚úì Detected PyTorch {torch_version}\")\n",
        "print(f\"‚úì Detected CUDA {cuda_version if cuda_version else 'N/A (CPU only)'}\")\n",
        "print(f\"‚úì Using wheel tag: {cuda_tag}\")\n",
        "\n",
        "# [3/9] Install PyTorch Geometric with enhanced error handling\n",
        "print(\"\\n[3/9] Installing PyTorch Geometric (runtime-aware with fallback)...\")\n",
        "pyg_wheel_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
        "print(f\"Wheel URL: {pyg_wheel_url}\")\n",
        "\n",
        "pyg_packages = ['torch-scatter', 'torch-sparse', 'torch-cluster', 'torch-spline-conv']\n",
        "pyg_install_success = True\n",
        "\n",
        "for pkg in pyg_packages:\n",
        "    print(f\"  Installing {pkg}...\")\n",
        "    if not run_cmd(f\"pip install -q {pkg} -f {pyg_wheel_url}\"):\n",
        "        print(f\"    ‚ö†Ô∏è  Wheel install failed, trying source build...\")\n",
        "        if not run_cmd(f\"pip install -q {pkg} --no-binary {pkg}\"):\n",
        "            print(f\"    ‚ùå Failed to install {pkg}\")\n",
        "            pyg_install_success = False\n",
        "        else:\n",
        "            print(f\"    ‚úì {pkg} installed from source (slower)\")\n",
        "    else:\n",
        "        print(f\"    ‚úì {pkg} installed from wheel\")\n",
        "\n",
        "if pyg_install_success:\n",
        "    run_cmd(\"pip install -q torch-geometric==2.4.0\")\n",
        "    print(\"‚úÖ PyTorch Geometric installed successfully\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Some PyG packages failed - GNN training may have issues\")\n",
        "\n",
        "# [4/9] Install Transformers with COMPATIBLE tokenizers version\n",
        "print(\"\\n[4/9] Installing Transformers with compatible tokenizers...\")\n",
        "print(\"‚ö†Ô∏è  Note: Using tokenizers 0.14.1 (compatible with transformers 4.35.0)\")\n",
        "\n",
        "# Install transformers first, then pin tokenizers to compatible version\n",
        "if not run_cmd(\"pip install -q transformers==4.35.0\"):\n",
        "    print(\"‚ùå Transformers installation failed\")\n",
        "else:\n",
        "    # Now pin tokenizers to compatible version\n",
        "    if not run_cmd(\"pip install -q tokenizers==0.14.1\"):\n",
        "        print(\"‚ö†Ô∏è  Could not pin tokenizers to 0.14.1, using auto-resolved version\")\n",
        "    else:\n",
        "        print(\"‚úì Tokenizers 0.14.1 installed (compatible)\")\n",
        "\n",
        "# Install accelerate\n",
        "run_cmd(\"pip install -q accelerate==0.24.1\")\n",
        "\n",
        "# [5/9] Install tree-sitter\n",
        "print(\"\\n[5/9] Installing tree-sitter...\")\n",
        "run_cmd(\"pip install -q tree-sitter==0.20.4\")\n",
        "\n",
        "# [6/9] Install additional packages\n",
        "print(\"\\n[6/9] Installing additional packages...\")\n",
        "run_cmd(\"pip install -q scikit-learn==1.3.2 scipy==1.11.4 tqdm\")\n",
        "\n",
        "# [7/9] Verify installations with enhanced checks\n",
        "print(\"\\n[7/9] Verifying installations...\")\n",
        "try:\n",
        "    # Check NumPy first (critical)\n",
        "    import numpy\n",
        "    numpy_ver = numpy.__version__\n",
        "    numpy_major = int(numpy_ver.split('.')[0])\n",
        "    if numpy_major >= 2:\n",
        "        print(f\"‚ö†Ô∏è  WARNING: NumPy {numpy_ver} detected (should be 1.x)\")\n",
        "        print(\"   Binary compatibility issues may occur\")\n",
        "    else:\n",
        "        print(f\"‚úì NumPy: {numpy_ver} (binary compatible)\")\n",
        "\n",
        "    # Check other packages\n",
        "    import torch\n",
        "    import torch_geometric\n",
        "    import transformers\n",
        "    import tree_sitter\n",
        "    import sklearn\n",
        "\n",
        "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
        "    print(f\"‚úì PyTorch Geometric: {torch_geometric.__version__}\")\n",
        "    print(f\"‚úì Transformers: {transformers.__version__}\")\n",
        "\n",
        "    # Check tokenizers compatibility\n",
        "    import tokenizers\n",
        "    tokenizers_ver = tokenizers.__version__\n",
        "    print(f\"‚úì Tokenizers: {tokenizers_ver}\")\n",
        "\n",
        "    if tokenizers_ver.startswith(\"0.15\"):\n",
        "        print(f\"  ‚ö†Ô∏è  WARNING: tokenizers {tokenizers_ver} may conflict with transformers 4.35.0\")\n",
        "    elif tokenizers_ver.startswith(\"0.14\"):\n",
        "        print(f\"  ‚úì Tokenizers version compatible\")\n",
        "\n",
        "    print(f\"‚úì tree-sitter: {tree_sitter.__version__}\")\n",
        "    print(f\"‚úì scikit-learn: {sklearn.__version__}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Verification failed: {e}\")\n",
        "    print(\"   Please restart runtime and try again\")\n",
        "    print(\"   If issue persists, check:\")\n",
        "    print(\"   1. NumPy version (should be 1.26.4)\")\n",
        "    print(\"   2. Tokenizers version (should be 0.14.1)\")\n",
        "\n",
        "# [8/9] Test PyTorch Geometric installation\n",
        "print(\"\\n[8/9] Testing PyTorch Geometric...\")\n",
        "try:\n",
        "    from torch_geometric.data import Data\n",
        "    test_data = Data(x=torch.randn(5, 3), edge_index=torch.tensor([[0, 1], [1, 0]]))\n",
        "    print(\"‚úì PyTorch Geometric working correctly\")\n",
        "    print(f\"‚úì Test data created: {test_data}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  PyTorch Geometric test failed: {e}\")\n",
        "    print(\"   GNN training may have issues\")\n",
        "    print(\"   Possible causes:\")\n",
        "    print(\"   1. NumPy binary incompatibility\")\n",
        "    print(\"   2. PyG wheel installation failed\")\n",
        "    print(\"   3. CUDA version mismatch\")\n",
        "\n",
        "# [9/9] Display final summary\n",
        "print(\"\\n[9/9] Installation Summary:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "success_indicators = {\n",
        "    'numpy_compatible': numpy_major < 2 if 'numpy_major' in locals() else False,\n",
        "    'pyg_installed': pyg_install_success,\n",
        "    'transformers_installed': True,  # Assume success if we got here\n",
        "    'tokenizers_compatible': tokenizers_ver.startswith(\"0.14\") if 'tokenizers_ver' in locals() else False\n",
        "}\n",
        "\n",
        "all_success = all(success_indicators.values())\n",
        "\n",
        "if all_success:\n",
        "    print(\"‚úÖ ALL INSTALLATIONS SUCCESSFUL\")\n",
        "    print(\"‚úì NumPy 1.x (binary compatible)\")\n",
        "    print(\"‚úì PyTorch Geometric with correct wheels\")\n",
        "    print(\"‚úì Transformers with compatible tokenizers\")\n",
        "    print(\"‚úì All packages verified\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  INSTALLATION COMPLETED WITH WARNINGS:\")\n",
        "    if not success_indicators['numpy_compatible']:\n",
        "        print(\"  ‚Ä¢ NumPy version may cause binary incompatibility\")\n",
        "    if not success_indicators['pyg_installed']:\n",
        "        print(\"  ‚Ä¢ PyG packages had installation issues\")\n",
        "    if not success_indicators['tokenizers_compatible']:\n",
        "        print(\"  ‚Ä¢ Tokenizers version may conflict with transformers\")\n",
        "    print(\"\\n  Training may still work, but monitor for errors\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN-qu_tx_DzC",
        "outputId": "6747b25b-c95a-4f81-9e00-c5cfa1a67aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ENHANCED DEPENDENCY & VERSION COMPATIBILITY CHECK\n",
            "======================================================================\n",
            "\n",
            "[1/4] Installed Core Versions:\n",
            "  PyTorch: 2.8.0+cu126\n",
            "  PyTorch Geometric: 2.4.0\n",
            "  Transformers: 4.35.0\n",
            "  CUDA: 12.6\n",
            "\n",
            "[2/4] Checking Optional Dependencies:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì sentence_transformers: not installed (correct)\n",
            "  ‚úì datasets: not installed (correct)\n",
            "  ‚ö†Ô∏è  fsspec: 2025.3.0 (not needed for training)\n",
            "  ‚ö†Ô∏è  gcsfs: 2025.3.0 (not needed for training)\n",
            "\n",
            "[3/4] Validating PyTorch Geometric Installation:\n",
            "  Expected wheel URL: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "  ‚úì PyTorch Geometric working correctly\n",
            "  ‚úì Wheels matched PyTorch 2.8.0 + cu126\n",
            "\n",
            "[4/4] Core Compatibility Checks:\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL CHECKS PASSED - Ready for production training!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 2.5: Enhanced Version & Dependency Compatibility Check (v1.1)\n",
        "# Validates versions, checks for dependency conflicts, validates PyG wheels\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "import transformers\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENHANCED DEPENDENCY & VERSION COMPATIBILITY CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# [1/4] Check core versions\n",
        "torch_ver = torch.__version__\n",
        "pyg_ver = torch_geometric.__version__\n",
        "transformers_ver = transformers.__version__\n",
        "cuda_ver = torch.version.cuda if torch.cuda.is_available() else \"N/A\"\n",
        "\n",
        "print(f\"\\n[1/4] Installed Core Versions:\")\n",
        "print(f\"  PyTorch: {torch_ver}\")\n",
        "print(f\"  PyTorch Geometric: {pyg_ver}\")\n",
        "print(f\"  Transformers: {transformers_ver}\")\n",
        "print(f\"  CUDA: {cuda_ver}\")\n",
        "\n",
        "# [2/4] Check for problematic optional dependencies (CRITICAL FIX #4)\n",
        "print(f\"\\n[2/4] Checking Optional Dependencies:\")\n",
        "optional_deps = {\n",
        "    'sentence_transformers': None,\n",
        "    'datasets': None,\n",
        "    'fsspec': None,\n",
        "    'gcsfs': None\n",
        "}\n",
        "\n",
        "for pkg_name in optional_deps.keys():\n",
        "    try:\n",
        "        pkg = importlib.import_module(pkg_name)\n",
        "        version = getattr(pkg, '__version__', 'unknown')\n",
        "        optional_deps[pkg_name] = version\n",
        "        print(f\"  ‚ö†Ô∏è  {pkg_name}: {version} (not needed for training)\")\n",
        "    except ImportError:\n",
        "        print(f\"  ‚úì {pkg_name}: not installed (correct)\")\n",
        "\n",
        "# Check for version conflicts\n",
        "has_conflicts = False\n",
        "if optional_deps.get('sentence_transformers'):\n",
        "    print(\"\\n  ‚ö†Ô∏è  WARNING: sentence-transformers detected\")\n",
        "    print(\"     May conflict with transformers==4.35.0\")\n",
        "    print(\"     If errors occur, uninstall: !pip uninstall -y sentence-transformers\")\n",
        "    has_conflicts = True\n",
        "\n",
        "if optional_deps.get('datasets'):\n",
        "    print(\"\\n  ‚ö†Ô∏è  WARNING: datasets library detected\")\n",
        "    print(\"     May pull incompatible transformers/tokenizers versions\")\n",
        "    has_conflicts = True\n",
        "\n",
        "# [3/4] Validate PyG wheel URL (CRITICAL FIX #4)\n",
        "print(f\"\\n[3/4] Validating PyTorch Geometric Installation:\")\n",
        "torch_version = torch_ver.split('+')[0]\n",
        "cuda_tag = f\"cu{cuda_ver.replace('.', '')}\" if cuda_ver != \"N/A\" else 'cpu'\n",
        "pyg_wheel_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
        "\n",
        "print(f\"  Expected wheel URL: {pyg_wheel_url}\")\n",
        "\n",
        "# Quick test PyG installation\n",
        "try:\n",
        "    from torch_geometric.data import Data\n",
        "    test_data = Data(x=torch.randn(5, 3), edge_index=torch.tensor([[0, 1], [1, 0]]))\n",
        "    print(f\"  ‚úì PyTorch Geometric working correctly\")\n",
        "    print(f\"  ‚úì Wheels matched PyTorch {torch_version} + {cuda_tag}\")\n",
        "except Exception as e:\n",
        "    print(f\"  ‚ùå PyTorch Geometric test failed: {e}\")\n",
        "    print(f\"  ‚ö†Ô∏è  Wheel URL may be incorrect - check {pyg_wheel_url}\")\n",
        "\n",
        "# [4/4] Core compatibility checks\n",
        "print(f\"\\n[4/4] Core Compatibility Checks:\")\n",
        "warnings = []\n",
        "errors = []\n",
        "\n",
        "# Check PyTorch version\n",
        "torch_major = int(torch_ver.split('.')[0])\n",
        "if torch_major < 2:\n",
        "    warnings.append(\"‚ö†Ô∏è  PyTorch 2.x+ recommended (you have {torch_ver})\")\n",
        "\n",
        "# Check CUDA availability (CRITICAL)\n",
        "if not torch.cuda.is_available():\n",
        "    errors.append(\"‚ùå CUDA not available - training will be EXTREMELY slow\")\n",
        "    errors.append(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n",
        "# Check PyG compatibility\n",
        "pyg_major = int(pyg_ver.split('.')[0])\n",
        "if pyg_major < 2:\n",
        "    warnings.append(\"‚ö†Ô∏è  PyTorch Geometric 2.x+ recommended\")\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    if gpu_mem_gb < 12:\n",
        "        warnings.append(f\"‚ö†Ô∏è  GPU has only {gpu_mem_gb:.1f} GB RAM (16GB+ recommended)\")\n",
        "        warnings.append(\"   Consider reducing batch sizes if OOM errors occur\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if errors:\n",
        "    print(\"üî¥ CRITICAL ERRORS:\")\n",
        "    for e in errors:\n",
        "        print(f\"  {e}\")\n",
        "    print(\"\\n‚ùå CANNOT PROCEED - Fix errors above\")\n",
        "    print(\"=\"*70)\n",
        "    raise RuntimeError(\"Environment validation failed\")\n",
        "elif warnings or has_conflicts:\n",
        "    if warnings:\n",
        "        print(\"‚ö†Ô∏è  Compatibility Warnings:\")\n",
        "        for w in warnings:\n",
        "            print(f\"  {w}\")\n",
        "    if has_conflicts:\n",
        "        print(\"\\n‚ö†Ô∏è  Dependency Conflicts Detected:\")\n",
        "        print(\"  Monitor for errors during training\")\n",
        "        print(\"  If issues occur, restart runtime and reinstall dependencies\")\n",
        "    print(\"\\n‚úì You can proceed but may need adjustments\")\n",
        "else:\n",
        "    print(\"‚úÖ ALL CHECKS PASSED - Ready for production training!\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7aJ2uNnO_DzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c85186-b642-4c86-e91f-45978cebe56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning StreamGuard repository...\n",
            "Cloning into 'streamguard'...\n",
            "remote: Enumerating objects: 299, done.\u001b[K\n",
            "remote: Counting objects: 100% (299/299), done.\u001b[K\n",
            "remote: Compressing objects: 100% (244/244), done.\u001b[K\n",
            "remote: Total 299 (delta 87), reused 259 (delta 47), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (299/299), 742.63 KiB | 8.16 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "‚úì Repository cloned\n",
            "\n",
            "Working directory: /content/streamguard\n",
            "\n",
            "üí° All code changes from GitHub are now available!\n",
            "   No need to manually upload files to Google Drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Clone/Update repository from GitHub\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone or update StreamGuard repository\n",
        "if not Path('streamguard').exists():\n",
        "    print(\"Cloning StreamGuard repository...\")\n",
        "    !git clone https://github.com/VimalSajanGeorge/streamguard.git\n",
        "    print(\"‚úì Repository cloned\")\n",
        "else:\n",
        "    print(\"‚úì Repository already exists\")\n",
        "    print(\"Pulling latest changes...\")\n",
        "    os.chdir('streamguard')\n",
        "    !git pull origin master\n",
        "    print(\"‚úì Repository updated\")\n",
        "    os.chdir('..')\n",
        "\n",
        "os.chdir('streamguard')\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(\"\\nüí° All code changes from GitHub are now available!\")\n",
        "print(\"   No need to manually upload files to Google Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wncSmaKW_DzE",
        "outputId": "3c6e39b2-0fab-418f-e351-1e3803db109f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TREE-SITTER SETUP (with fallback support)\n",
            "======================================================================\n",
            "\n",
            "[1/3] Cloning tree-sitter-c...\n",
            "Cloning into 'tree-sitter-c'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 90 (delta 5), reused 30 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (90/90), 373.20 KiB | 3.52 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "‚úì tree-sitter-c cloned\n",
            "\n",
            "[2/3] Building tree-sitter library...\n",
            "‚úì Build completed\n",
            "\n",
            "[3/3] Verifying build...\n",
            "‚úì tree-sitter library verified successfully\n",
            "\n",
            "======================================================================\n",
            "‚úÖ AST PARSING ENABLED (optimal)\n",
            "   Preprocessing will use full AST structure\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Setup tree-sitter with robust error handling\n",
        "# ‚ö†Ô∏è CRITICAL: Includes fallback if build fails\n",
        "\n",
        "from pathlib import Path\n",
        "from tree_sitter import Language\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TREE-SITTER SETUP (with fallback support)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clone tree-sitter-c\n",
        "vendor_dir = Path('vendor')\n",
        "vendor_dir.mkdir(exist_ok=True)\n",
        "\n",
        "if not (vendor_dir / 'tree-sitter-c').exists():\n",
        "    print(\"\\n[1/3] Cloning tree-sitter-c...\")\n",
        "    !cd vendor && git clone --depth 1 https://github.com/tree-sitter/tree-sitter-c.git\n",
        "    print(\"‚úì tree-sitter-c cloned\")\n",
        "else:\n",
        "    print(\"\\n[1/3] ‚úì tree-sitter-c already exists\")\n",
        "\n",
        "# Build library with error handling\n",
        "build_dir = Path('build')\n",
        "build_dir.mkdir(exist_ok=True)\n",
        "lib_path = build_dir / 'my-languages.so'\n",
        "\n",
        "build_success = False\n",
        "\n",
        "if not lib_path.exists():\n",
        "    print(\"\\n[2/3] Building tree-sitter library...\")\n",
        "    try:\n",
        "        Language.build_library(\n",
        "            str(lib_path),\n",
        "            [str(vendor_dir / 'tree-sitter-c')]\n",
        "        )\n",
        "        print(\"‚úì Build completed\")\n",
        "\n",
        "        # Verify build\n",
        "        if lib_path.exists():\n",
        "            print(\"\\n[3/3] Verifying build...\")\n",
        "            try:\n",
        "                test_lang = Language(str(lib_path), 'c')\n",
        "                print(\"‚úì tree-sitter library verified successfully\")\n",
        "                build_success = True\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Verification failed: {e}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Build completed but library file not found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Build failed: {e}\")\n",
        "        print(\"   Common causes: missing compiler, permission issues\")\n",
        "else:\n",
        "    print(\"\\n[2/3] ‚úì tree-sitter library already exists\")\n",
        "    print(\"\\n[3/3] Verifying existing build...\")\n",
        "    try:\n",
        "        test_lang = Language(str(lib_path), 'c')\n",
        "        print(\"‚úì Existing library verified\")\n",
        "        build_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Existing library invalid: {e}\")\n",
        "\n",
        "# Display final status\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if build_success:\n",
        "    print(\"‚úÖ AST PARSING ENABLED (optimal)\")\n",
        "    print(\"   Preprocessing will use full AST structure\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  AST PARSING WILL USE FALLBACK MODE\")\n",
        "    print(\"   Preprocessing will use token-sequence graphs\")\n",
        "    print(\"   ‚úì Training will still work correctly\")\n",
        "    print(\"   ‚úì Performance impact: minimal (<5%)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD2ONUWq_DzF"
      },
      "source": [
        "### Platform Notes: tree-sitter on Windows/Linux\n",
        "\n",
        "**Google Colab (Linux):**\n",
        "- ‚úÖ Works out-of-the-box with `.so` libraries\n",
        "- ‚úÖ GCC compiler available by default\n",
        "\n",
        "**Windows (Local Development):**\n",
        "- ‚ö†Ô∏è  Requires Microsoft Visual C++ 14.0+ (MSVC)\n",
        "- ‚ö†Ô∏è  May fail with \"compiler not found\" errors\n",
        "- **Solution 1:** Use WSL (Windows Subsystem for Linux) for preprocessing\n",
        "- **Solution 2:** Use Colab for all preprocessing tasks\n",
        "- **Solution 3:** Install Visual Studio Build Tools (large download)\n",
        "- ‚úì **Fallback:** Token-sequence graphs work fine (<5% performance impact)\n",
        "\n",
        "**Recommendation:** For Windows users, use Colab for data preprocessing and training. Download preprocessed data to Windows only for inference/deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 1.5: Pre-Training Validation Tests (Issue #11 Fix Verification)\n",
        "\n",
        "**IMPORTANT:** Run these test cells BEFORE full training to verify all Issue #11 fixes are working correctly.\n",
        "\n",
        "These tests verify:\n",
        "1. ‚úÖ Class-balanced loss is working (model doesn't collapse to one class)\n",
        "2. ‚úÖ LR scaling and warmup are correct\n",
        "3. ‚úÖ Scheduler steps properly (per-step, not per-epoch)\n",
        "4. ‚úÖ Gradient clipping prevents exploding gradients\n",
        "5. ‚úÖ Prediction distribution monitoring detects collapse\n",
        "6. ‚úÖ Checkpoint saving/loading works with PyTorch 2.6+\n",
        "\n",
        "**Expected Results:**\n",
        "- **Test 1 (Tiny Overfitting Test):** Loss should decrease to near 0, F1 should reach 0.9+\n",
        "- **Test 2 (Short Full-Data Test):** F1 should increase each epoch, prediction distribution should be balanced\n",
        "- If tests pass, proceed to full training with confidence!\n",
        "\n",
        "**Duration:** 5-10 minutes total"
      ],
      "metadata": {
        "id": "sI-Y5fiEL8Xu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q29KEnue_DzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9a5fa9-ceac-4660-ed79-2967b2743b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SETTING UP DATA FROM GOOGLE DRIVE\n",
            "======================================================================\n",
            "Working directory: /content/streamguard\n",
            "\n",
            "[1/5] Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úì Google Drive mounted\n",
            "\n",
            "[2/5] Checking for data in Google Drive...\n",
            "   Looking in: /content/drive/MyDrive/streamguard/data/processed/codexglue\n",
            "‚úì Data found in Google Drive\n",
            "\n",
            "[3/5] Verifying data files in Drive...\n",
            "  ‚úì train.jsonl                    (  503.06 MB)\n",
            "  ‚úì valid.jsonl                    (   62.07 MB)\n",
            "  ‚úì test.jsonl                     (   62.63 MB)\n",
            "  ‚úì preprocessing_metadata.json    (    0.00 MB)\n",
            "\n",
            "üì¶ Total data size in Drive: 627.75 MB\n",
            "\n",
            "[4/5] Copying data from Drive to Colab local storage...\n",
            "   Source: /content/drive/MyDrive/streamguard/data/processed/codexglue\n",
            "   Destination: /content/streamguard/data/processed/codexglue\n",
            "   (This provides faster I/O during training)\n",
            "\n",
            "  üìã Copying train.jsonl                    (503.06 MB)... ‚úì\n",
            "  üìã Copying valid.jsonl                    (62.07 MB)... ‚úì\n",
            "  üìã Copying test.jsonl                     (62.63 MB)... ‚úì\n",
            "  üìã Copying preprocessing_metadata.json    (0.00 MB)... ‚úì\n",
            "\n",
            "‚úÖ All data files copied to local storage!\n",
            "\n",
            "[5/5] Loading dataset statistics...\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "\n",
            "üí° Total samples: 0\n",
            "\n",
            "======================================================================\n",
            "‚úÖ DATA SETUP COMPLETE - Ready for training!\n",
            "======================================================================\n",
            "\n",
            "üí° Training scripts will read from:\n",
            "   ‚Ä¢ /content/streamguard/data/processed/codexglue/train.jsonl\n",
            "   ‚Ä¢ /content/streamguard/data/processed/codexglue/valid.jsonl\n",
            "   ‚Ä¢ /content/streamguard/data/processed/codexglue/test.jsonl\n",
            "\n",
            "üíæ Data is now in Colab local storage (faster I/O than Drive)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Setup data from Google Drive\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SETTING UP DATA FROM GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Ensure we're in the streamguard directory\n",
        "os.chdir('/content/streamguard')\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "print(f\"\\n[1/5] Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"‚úì Google Drive mounted\")\n",
        "\n",
        "# Step 2: Check if data exists in Drive\n",
        "drive_data_path = Path('/content/drive/MyDrive/streamguard/data/processed/codexglue')\n",
        "print(f\"\\n[2/5] Checking for data in Google Drive...\")\n",
        "print(f\"   Looking in: {drive_data_path}\")\n",
        "\n",
        "if not drive_data_path.exists():\n",
        "    print(f\"‚ùå ERROR: Data not found in Google Drive!\")\n",
        "    print(f\"\\nüí° Please upload the preprocessed data to Google Drive:\")\n",
        "    print(f\"   1. Create folder: My Drive/streamguard/data/processed/codexglue/\")\n",
        "    print(f\"   2. Upload these files:\")\n",
        "    print(f\"      ‚Ä¢ train.jsonl (504 MB)\")\n",
        "    print(f\"      ‚Ä¢ valid.jsonl (63 MB)\")\n",
        "    print(f\"      ‚Ä¢ test.jsonl (63 MB)\")\n",
        "    print(f\"      ‚Ä¢ preprocessing_metadata.json (1.6 KB)\")\n",
        "    print(f\"\\n   Total: ~630 MB\")\n",
        "    raise FileNotFoundError(f\"Data not found in Drive: {drive_data_path}\")\n",
        "\n",
        "print(f\"‚úì Data found in Google Drive\")\n",
        "\n",
        "# Step 3: Check all required files\n",
        "print(f\"\\n[3/5] Verifying data files in Drive...\")\n",
        "required_files = ['train.jsonl', 'valid.jsonl', 'test.jsonl', 'preprocessing_metadata.json']\n",
        "missing_files = []\n",
        "\n",
        "drive_sizes = {}\n",
        "for file in required_files:\n",
        "    file_path = drive_data_path / file\n",
        "    if file_path.exists():\n",
        "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "        drive_sizes[file] = size_mb\n",
        "        print(f\"  ‚úì {file:<30} ({size_mb:>8.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {file:<30} MISSING\")\n",
        "        missing_files.append(file)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\n‚ùå ERROR: Missing {len(missing_files)} required file(s) in Drive\")\n",
        "    print(f\"   Missing: {', '.join(missing_files)}\")\n",
        "    raise FileNotFoundError(f\"Missing data files in Drive: {missing_files}\")\n",
        "\n",
        "total_size = sum(drive_sizes.values())\n",
        "print(f\"\\nüì¶ Total data size in Drive: {total_size:.2f} MB\")\n",
        "\n",
        "# Step 4: Create local data directory and copy files\n",
        "local_data_path = Path('/content/streamguard/data/processed/codexglue')\n",
        "local_data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n[4/5] Copying data from Drive to Colab local storage...\")\n",
        "print(f\"   Source: {drive_data_path}\")\n",
        "print(f\"   Destination: {local_data_path}\")\n",
        "print(f\"   (This provides faster I/O during training)\\n\")\n",
        "\n",
        "for file in required_files:\n",
        "    src = drive_data_path / file\n",
        "    dst = local_data_path / file\n",
        "\n",
        "    if dst.exists():\n",
        "        # Check if sizes match (skip if already copied)\n",
        "        src_size = src.stat().st_size\n",
        "        dst_size = dst.stat().st_size\n",
        "        if src_size == dst_size:\n",
        "            print(f\"  ‚úì {file:<30} (already copied, skipping)\")\n",
        "            continue\n",
        "\n",
        "    print(f\"  üìã Copying {file:<30} ({drive_sizes[file]:.2f} MB)...\", end='', flush=True)\n",
        "    shutil.copy2(src, dst)\n",
        "    print(\" ‚úì\")\n",
        "\n",
        "print(f\"\\n‚úÖ All data files copied to local storage!\")\n",
        "\n",
        "# Step 5: Load and display metadata\n",
        "print(f\"\\n[5/5] Loading dataset statistics...\")\n",
        "metadata_path = local_data_path / 'preprocessing_metadata.json'\n",
        "if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    total_samples = 0\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        if split in metadata:\n",
        "            count = metadata[split].get('total_samples', 0)\n",
        "            total_samples += count\n",
        "            print(f\"  {split.capitalize():<12}: {count:>6} samples\")\n",
        "\n",
        "    print(f\"\\nüí° Total samples: {total_samples:,}\")\n",
        "\n",
        "    # Show class distribution if available\n",
        "    if 'train' in metadata and 'label_distribution' in metadata['train']:\n",
        "        dist = metadata['train']['label_distribution']\n",
        "        print(f\"\\nüìä Class Distribution (Training Set):\")\n",
        "        for label, count in dist.items():\n",
        "            percentage = (count / metadata['train']['total_samples']) * 100\n",
        "            print(f\"  {label:<15}: {count:>6} ({percentage:>5.1f}%)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Metadata file not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ DATA SETUP COMPLETE - Ready for training!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüí° Training scripts will read from:\")\n",
        "print(f\"   ‚Ä¢ {local_data_path / 'train.jsonl'}\")\n",
        "print(f\"   ‚Ä¢ {local_data_path / 'valid.jsonl'}\")\n",
        "print(f\"   ‚Ä¢ {local_data_path / 'test.jsonl'}\")\n",
        "print(f\"\\nüíæ Data is now in Colab local storage (faster I/O than Drive)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.5: TEST 1 - Tiny Overfitting Test (Issue #11 Fix Verification)\n",
        "# This test uses 64 samples for 10 epochs to verify the model can learn\n",
        "\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST 1: TINY OVERFITTING TEST (Issue #11 Fix Verification)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Purpose: Verify model can learn on a tiny subset\")\n",
        "print(\"Expected: Loss ‚Üí 0, F1 ‚Üí 0.9+, balanced predictions\")\n",
        "print(\"Duration: ~2-3 minutes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --quick-test \\\n",
        "  --epochs 10 \\\n",
        "  --batch-size 8 \\\n",
        "  --max-seq-len 512 \\\n",
        "  --lr 2e-5 \\\n",
        "  --weight-decay 0.01 \\\n",
        "  --warmup-ratio 0.1 \\\n",
        "  --dropout 0.1 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TEST 1 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìã What to check:\")\n",
        "print(\"  1. Loss should decrease steadily (should reach < 0.5)\")\n",
        "print(\"  2. F1 score should increase (should reach > 0.7)\")\n",
        "print(\"  3. Prediction distribution should be balanced\")\n",
        "print(\"  4. No collapse warnings (model predicting only one class)\")\n",
        "print(\"\\nIf all checks pass, proceed to Test 2!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "tW83IcJGL8Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.6: TEST 2 - Short Full-Data Test (Issue #11 Fix Verification)\n",
        "# This test uses full data for 2-3 epochs to verify training stability\n",
        "\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST 2: SHORT FULL-DATA TEST (Issue #11 Fix Verification)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Purpose: Verify training stability with full dataset\")\n",
        "print(\"Expected: F1 increases each epoch, balanced predictions, no collapse\")\n",
        "print(\"Duration: ~10-15 minutes (depending on GPU)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /tmp/test_transformer \\\n",
        "  --epochs 3 \\\n",
        "  --batch-size 16 \\\n",
        "  --max-seq-len 512 \\\n",
        "  --lr 2e-5 \\\n",
        "  --weight-decay 0.01 \\\n",
        "  --warmup-ratio 0.1 \\\n",
        "  --dropout 0.1 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TEST 2 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìã What to check:\")\n",
        "print(\"  1. F1 score should increase each epoch\")\n",
        "print(\"  2. Prediction distribution should be balanced (check the logs)\")\n",
        "print(\"  3. No collapse warnings\")\n",
        "print(\"  4. Class weights are being used (check '[*] Class distribution' in logs)\")\n",
        "print(\"  5. LR scaling is applied (check '[*] Scaling LR' in logs)\")\n",
        "print(\"\\nIf all checks pass, proceed to full training (Cell 7)!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "cNdcjfZDL8Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nx0Gw_T_DzH"
      },
      "source": [
        "---\n",
        "## Part 2: Transformer Training (2-3 hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN9tmhZy_DzH"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Transformer training with adaptive configuration\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "# Load adaptive configuration with fallback\n",
        "config_path = Path('/tmp/gpu_training_config.json')\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "    t_config = config_data['config']['transformer']\n",
        "    config_tier = config_data['tier']\n",
        "    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n",
        "    t_config = {'epochs': 10, 'batch_size': 32, 'max_seq_len': 512, 'patience': 2}\n",
        "    config_tier = 'OPTIMIZED (Default)'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING TRANSFORMER TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(f\"Epochs: {t_config['epochs']}\")\n",
        "print(f\"Batch Size: {t_config['batch_size']}\")\n",
        "print(f\"Max Seq Length: {t_config['max_seq_len']}\")\n",
        "print(f\"Early Stopping Patience: {t_config['patience']}\")\n",
        "print(\"\\n‚ö†Ô∏è  NOTE: --mixed-precision DISABLED for initial testing\")\n",
        "print(\"   Re-enable after confirming training stability (3-4 epochs)\")\n",
        "print(\"\\nüí° Data: Make sure your preprocessed data is in:\")\n",
        "print(\"   data/processed/codexglue/ (train.jsonl, valid.jsonl, test.jsonl)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/transformer_phase1 \\\n",
        "  --epochs {t_config['epochs']} \\\n",
        "  --batch-size {t_config['batch_size']} \\\n",
        "  --max-seq-len {t_config['max_seq_len']} \\\n",
        "  --lr 2e-5 \\\n",
        "  --weight-decay 0.01 \\\n",
        "  --warmup-ratio 0.1 \\\n",
        "  --dropout 0.1 \\\n",
        "  --early-stopping-patience {t_config['patience']} \\\n",
        "  --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fVygiuW_DzH"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Transformer training (static config for reference)\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING TRANSFORMER TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"Expected duration: 2-3 hours\")\n",
        "print(\"\\n‚ö†Ô∏è  NOTE: --mixed-precision DISABLED for initial testing\")\n",
        "print(\"   Re-enable after confirming training stability (3-4 epochs)\")\n",
        "print(\"\\nüí° Data: Make sure your preprocessed data is in:\")\n",
        "print(\"   data/processed/codexglue/ (train.jsonl, valid.jsonl, test.jsonl)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/transformer_phase1 \\\n",
        "  --epochs 5 \\\n",
        "  --batch-size 16 \\\n",
        "  --lr 2e-5 \\\n",
        "  --weight-decay 0.01 \\\n",
        "  --warmup-ratio 0.1 \\\n",
        "  --max-seq-len 512 \\\n",
        "  --dropout 0.1 \\\n",
        "  --early-stopping-patience 2 \\\n",
        "  --seed 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VhIXkhD_DzH"
      },
      "source": [
        "---\n",
        "## Part 3: GNN Training (4-6 hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AaZPf1Sp_DzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0983734f-5c7d-41bd-ae71-2f050a516854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Using AGGRESSIVE configuration for NVIDIA A100-SXM4-40GB\n",
            "======================================================================\n",
            "STARTING GNN TRAINING\n",
            "======================================================================\n",
            "Configuration: AGGRESSIVE\n",
            "Epochs: 300\n",
            "Batch Size: 128\n",
            "Hidden Dimensions: 512\n",
            "Num Layers: 5\n",
            "Early Stopping Patience: 15\n",
            "======================================================================\n",
            "[!] boto3 not available. S3 checkpointing disabled.\n",
            "[+] Random seed set to 42\n",
            "[+] Using device: cuda\n",
            "[*] Loading graph dataset from data/processed/codexglue/train.jsonl\n",
            "[+] Loaded 21854 graphs\n",
            "    Vulnerable: 10018 (45.8%)\n",
            "    Safe: 11836 (54.2%)\n",
            "[*] Loading graph dataset from data/processed/codexglue/valid.jsonl\n",
            "[+] Loaded 2732 graphs\n",
            "    Vulnerable: 1187 (43.4%)\n",
            "    Safe: 1545 (56.6%)\n",
            "\n",
            "[*] Graph Statistics:\n",
            "    P95 nodes: 1713\n",
            "    Recommended batch size: 64\n",
            "[+] Experiment config saved: /content/models/gnn_phase1/exp_config.json\n",
            "\n",
            "[*] Calculating class weights for balanced training...\n",
            "    Class distribution: Safe=11836, Vulnerable=10018\n",
            "[+] Class weights: Safe=0.9232, Vulnerable=1.0907\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "[+] Train loader: random shuffle (no sampler)\n",
            "[*] Initializing GNN model\n",
            "[+] Loss: CrossEntropyLoss (weights=[0.9232004284858704, 1.090736746788025])\n",
            "\n",
            "======================================================================\n",
            "TRAINING START\n",
            "======================================================================\n",
            "[+] CSV metrics will be saved to: /content/models/gnn_phase1/metrics_history.csv\n",
            "\n",
            "Epoch 1/300\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6914\n",
            "Val Loss: 0.6878\n",
            "Val Accuracy: 0.5472\n",
            "Val Precision: 0.4775\n",
            "Val Recall: 0.4473\n",
            "Val F1 (vulnerable): 0.4619\n",
            "Predictions: Vulnerable=1112/1187, Safe=1620/1545\n",
            "[+] Saved best model: /content/models/gnn_phase1/checkpoints/best_model.pt\n",
            "[+] New best model! F1: 0.4619\n",
            "\n",
            "Epoch 2/300\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6878\n",
            "Val Loss: 0.6884\n",
            "Val Accuracy: 0.5135\n",
            "Val Precision: 0.4589\n",
            "Val Recall: 0.6681\n",
            "Val F1 (vulnerable): 0.5441\n",
            "Predictions: Vulnerable=1728/1187, Safe=1004/1545\n",
            "[+] Saved best model: /content/models/gnn_phase1/checkpoints/best_model.pt\n",
            "[+] New best model! F1: 0.5441\n",
            "\n",
            "Epoch 3/300\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6835\n",
            "Val Loss: 0.6770\n",
            "Val Accuracy: 0.5780\n",
            "Val Precision: 0.5904\n",
            "Val Recall: 0.0935\n",
            "Val F1 (vulnerable): 0.1615\n",
            "Predictions: Vulnerable=188/1187, Safe=2544/1545\n",
            "[*] No improvement. Patience: 1/15\n",
            "\n",
            "Epoch 4/300\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6784\n",
            "Val Loss: 0.6778\n",
            "Val Accuracy: 0.5867\n",
            "Val Precision: 0.8718\n",
            "Val Recall: 0.0573\n",
            "Val F1 (vulnerable): 0.1075\n",
            "Predictions: Vulnerable=78/1187, Safe=2654/1545\n",
            "[*] No improvement. Patience: 2/15\n",
            "\n",
            "Epoch 5/300\n",
            "----------------------------------------------------------------------\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [30,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [32,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [34,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [36,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [38,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [39,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [40,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [41,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [42,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [43,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [44,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [45,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [46,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [47,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [48,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [49,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [50,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [51,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [52,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [53,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [54,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [55,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [56,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "/pytorch/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:163: operator(): block: [79,0,0], thread: [57,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"scatter gather kernel index out of bounds\"` failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/streamguard/training/train_gnn.py\", line 1131, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/streamguard/training/train_gnn.py\", line 964, in main\n",
            "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/streamguard/training/train_gnn.py\", line 503, in train_epoch\n",
            "    logits = model(data)\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/streamguard/training/train_gnn.py\", line 308, in forward\n",
            "    x = conv(x, edge_index)\n",
            "        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/gcn_conv.py\", line 222, in forward\n",
            "    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/gcn_conv.py\", line 91, in gcn_norm\n",
            "    edge_index, edge_weight = add_remaining_self_loops(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/loop.py\", line 370, in add_remaining_self_loops\n",
            "    edge_index = torch.cat([edge_index[:, mask], loop_index], dim=1)\n",
            "                            ~~~~~~~~~~^^^^^^^^^\n",
            "torch.AcceleratorError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: GNN training with adaptive configuration\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "# Load adaptive configuration with fallback\n",
        "config_path = Path('/tmp/gpu_training_config.json')\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "    g_config = config_data['config']['gnn']\n",
        "    config_tier = config_data['tier']\n",
        "    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n",
        "    g_config = {'epochs': 150, 'batch_size': 64, 'hidden_dim': 256, 'num_layers': 4, 'patience': 10}\n",
        "    config_tier = 'OPTIMIZED (Default)'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING GNN TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(f\"Epochs: {g_config['epochs']}\")\n",
        "print(f\"Batch Size: {g_config['batch_size']}\")\n",
        "print(f\"Hidden Dimensions: {g_config['hidden_dim']}\")\n",
        "print(f\"Num Layers: {g_config['num_layers']}\")\n",
        "print(f\"Early Stopping Patience: {g_config['patience']}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_gnn.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/gnn_phase1 \\\n",
        "  --epochs {g_config['epochs']} \\\n",
        "  --batch-size {g_config['batch_size']} \\\n",
        "  --hidden-dim {g_config['hidden_dim']} \\\n",
        "  --num-layers {g_config['num_layers']} \\\n",
        "  --lr 1e-3 \\\n",
        "  --weight-decay 1e-4 \\\n",
        "  --dropout 0.3 \\\n",
        "  --early-stopping-patience {g_config['patience']} \\\n",
        "  --auto-batch-size \\\n",
        "  --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIGq25-o_DzI"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Fusion training with fallback config\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "# Load adaptive configuration with fallback\n",
        "config_path = Path('/tmp/gpu_training_config.json')\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "    f_config = config_data['config']['fusion']\n",
        "    config_tier = config_data['tier']\n",
        "    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n",
        "    f_config = {'n_folds': 5, 'epochs': 30}\n",
        "    config_tier = 'OPTIMIZED (Default)'\n",
        "    config_data = {'tier': config_tier, 'gpu': 'Unknown'}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING FUSION TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(f\"N-Folds (OOF): {f_config['n_folds']}\")\n",
        "print(f\"Epochs: {f_config['epochs']}\")\n",
        "\n",
        "# Display performance note based on config\n",
        "if 'OPTIMIZED' in config_tier:\n",
        "    print(\"\\nüí° T4/DEFAULT CONFIGURATION:\")\n",
        "    print(\"   Using n_folds=5 for good ensemble robustness\")\n",
        "    print(\"   Larger batches and extended training can improve quality\")\n",
        "elif 'ENHANCED' in config_tier:\n",
        "    print(\"\\nüí° V100 CONFIGURATION:\")\n",
        "    print(\"   Using n_folds=5, 2-3x faster than T4\")\n",
        "    print(\"   Larger batches for better gradient estimates\")\n",
        "elif 'AGGRESSIVE' in config_tier:\n",
        "    print(\"\\nüí° A100 MAXIMUM CONFIGURATION:\")\n",
        "    print(\"   Using n_folds=10 for maximum robustness\")\n",
        "    print(\"   Extended training for highest quality\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_fusion.py \\\n",
        "  --train-data /content/data/processed/codexglue/train.jsonl \\\n",
        "  --val-data /content/data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data /content/data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/fusion_phase1 \\\n",
        "  --transformer-checkpoint /content/models/transformer_phase1/checkpoints/best_model.pt \\\n",
        "  --gnn-checkpoint /content/models/gnn_phase1/checkpoints/best_model.pt \\\n",
        "  --n-folds {f_config['n_folds']} \\\n",
        "  --epochs {f_config['epochs']} \\\n",
        "  --lr 1e-3 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä FUSION TRAINING COMPLETE\")\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(f\"Folds trained: {f_config['n_folds']}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB81siKS_DzI"
      },
      "source": [
        "# Cell 9: GNN training (static config for reference)\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING GNN TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"Expected duration: 4-6 hours\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_gnn.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/gnn_phase1 \\\n",
        "  --epochs 100 \\\n",
        "  --batch-size 32 \\\n",
        "  --lr 1e-3 \\\n",
        "  --weight-decay 1e-4 \\\n",
        "  --hidden-dim 256 \\\n",
        "  --num-layers 4 \\\n",
        "  --dropout 0.3 \\\n",
        "  --early-stopping-patience 10 \\\n",
        "  --auto-batch-size \\\n",
        "  --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xXTOcOtf_DzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276d0a10-ca78-4ee1-da9b-e9bcd2877ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Using AGGRESSIVE configuration for NVIDIA A100-SXM4-40GB\n",
            "======================================================================\n",
            "STARTING FUSION TRAINING\n",
            "======================================================================\n",
            "Configuration: AGGRESSIVE\n",
            "N-Folds (OOF): 10\n",
            "Epochs: 100\n",
            "\n",
            "üí° A100 MAXIMUM CONFIGURATION:\n",
            "   Using n_folds=10 for maximum robustness\n",
            "   Extended training for highest quality\n",
            "======================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "[!] boto3 not available. S3 checkpointing disabled.\n",
            "[!] boto3 not available. S3 checkpointing disabled.\n",
            "[+] Random seed set to 42\n",
            "[+] Using device: cuda\n",
            "[*] Generating OOF predictions...\n",
            "[*] Loading base model checkpoints...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/streamguard/training/train_fusion.py\", line 610, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/streamguard/training/train_fusion.py\", line 526, in main\n",
            "    oof_generator = OOFPredictionGenerator(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/streamguard/training/train_fusion.py\", line 159, in __init__\n",
            "    transformer_ckpt = torch.load(transformer_checkpoint, map_location=device)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/models/transformer_phase1/checkpoints/best_model.pt'\n",
            "\n",
            "======================================================================\n",
            "üìä FUSION TRAINING COMPLETE\n",
            "Configuration: AGGRESSIVE\n",
            "Folds trained: 10\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Fusion training with adaptive configuration\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "# Load adaptive configuration with fallback\n",
        "config_path = Path('/tmp/gpu_training_config.json')\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "    f_config = config_data['config']['fusion']\n",
        "    config_tier = config_data['tier']\n",
        "    print(f\"‚úì Using {config_tier} configuration for {config_data['gpu']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Config file not found, using default T4 OPTIMIZED settings\")\n",
        "    f_config = {'n_folds': 5, 'epochs': 30}\n",
        "    config_tier = 'OPTIMIZED (Default)'\n",
        "    config_data = {'tier': config_tier, 'gpu': 'Unknown'}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING FUSION TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(f\"N-Folds (OOF): {f_config['n_folds']}\")\n",
        "print(f\"Epochs: {f_config['epochs']}\")\n",
        "\n",
        "# Display performance note based on config\n",
        "if 'OPTIMIZED' in config_tier:\n",
        "    print(\"\\nüí° T4/DEFAULT CONFIGURATION:\")\n",
        "    print(\"   Using n_folds=5 for good ensemble robustness\")\n",
        "    print(\"   Larger batches and extended training can improve quality\")\n",
        "elif 'ENHANCED' in config_tier:\n",
        "    print(\"\\nüí° V100 CONFIGURATION:\")\n",
        "    print(\"   Using n_folds=5, 2-3x faster than T4\")\n",
        "    print(\"   Larger batches for better gradient estimates\")\n",
        "elif 'AGGRESSIVE' in config_tier:\n",
        "    print(\"\\nüí° A100 MAXIMUM CONFIGURATION:\")\n",
        "    print(\"   Using n_folds=10 for maximum robustness\")\n",
        "    print(\"   Extended training for highest quality\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_fusion.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/fusion_phase1 \\\n",
        "  --transformer-checkpoint /content/models/transformer_phase1/checkpoints/best_model.pt \\\n",
        "  --gnn-checkpoint /content/models/gnn_phase1/checkpoints/best_model.pt \\\n",
        "  --n-folds {f_config['n_folds']} \\\n",
        "  --epochs {f_config['epochs']} \\\n",
        "  --lr 1e-3 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä FUSION TRAINING COMPLETE\")\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(f\"Folds trained: {f_config['n_folds']}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pMVOsdT_DzJ"
      },
      "outputs": [],
      "source": [
        "# Cell 12: Save Fusion to Drive\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "drive_fusion = Path('/content/drive/MyDrive/streamguard/models/fusion_phase1')\n",
        "drive_fusion.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "local_fusion = Path('/content/models/fusion_phase1')\n",
        "\n",
        "print(\"Saving Fusion model to Google Drive...\")\n",
        "\n",
        "for file in local_fusion.glob('*'):\n",
        "    if file.is_file():\n",
        "        shutil.copy2(file, drive_fusion / file.name)\n",
        "        print(f\"  ‚úì {file.name} saved\")\n",
        "\n",
        "print(f\"\\n‚úÖ Fusion saved to Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Qh1BvG_DzJ"
      },
      "source": [
        "# Cell 11: Fusion training (static config for reference)\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING FUSION TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"Expected duration: 2-3 hours (n_folds=3)\")\n",
        "print(\"Note: Using n_folds=3 for Colab (5-fold for SageMaker/powerful hardware)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# CRITICAL FIX #2: Reduced n_folds for Colab constraints\n",
        "# 5-fold OOF increases runtime significantly on limited GPU instances\n",
        "# 3-fold provides good speed/robustness tradeoff for Colab\n",
        "!python training/train_fusion.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/fusion_phase1 \\\n",
        "  --transformer-checkpoint /content/models/transformer_phase1/checkpoints/best_model.pt \\\n",
        "  --gnn-checkpoint /content/models/gnn_phase1/checkpoints/best_model.pt \\\n",
        "  --n-folds 3 \\\n",
        "  --epochs 20 \\\n",
        "  --lr 1e-3 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° PERFORMANCE NOTE:\")\n",
        "print(\"  - n_folds=3 used for Colab (good speed/robustness tradeoff)\")\n",
        "print(\"  - For production with powerful hardware, use n_folds=5\")\n",
        "print(\"  - 3-fold OOF typically achieves 95-98% of 5-fold performance\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7OP1__A_DzJ"
      },
      "outputs": [],
      "source": [
        "---\n",
        "## üÜï v1.7 Safety Features - Quick Access\n",
        "\n",
        "**NEW in v1.7:** Advanced safety features are now available! These are **OPTIONAL** and fully backward compatible.\n",
        "\n",
        "### ‚ú® One-Click Executable Cells (New!)\n",
        "\n",
        "**Ready to use v1.7 features? Just run these cells:**\n",
        "\n",
        "1. **Cell 27:** Run Unit Tests (14 tests, ~30 seconds)\n",
        "2. **Cell 28:** Test LR Finder Safety (quick validation, ~2-3 minutes)\n",
        "3. **Cell 29:** Full Training with v1.7 Features (complete safety suite)\n",
        "4. **Cell 30:** Inspect Enhanced Metadata (view LR analysis & triple weighting info)\n",
        "\n",
        "**All features are ready to run - just click and go!** No copy-pasting needed.\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ Complete Documentation & Examples\n",
        "\n",
        "Below you'll find detailed documentation for each feature. The code blocks show what's happening in Cells 27-30 above.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 1. Run Unit Tests (Cell 27)\n",
        "\n",
        "Verify all safety features are working correctly:\n",
        "\n",
        "**What Cell 27 does:**\n",
        "```python\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "!python -m pytest tests/test_lr_finder.py -v\n",
        "```\n",
        "\n",
        "**Expected:** All 14 tests should PASS (cache, LR curve analysis, validation, integration)\n",
        "\n",
        "---\n",
        "\n",
        "### üîç 2. Test LR Finder Safety (Cell 28)\n",
        "\n",
        "Test the LR Finder with safety validation on a small subset:\n",
        "\n",
        "**What Cell 28 does:**\n",
        "- Runs LR Finder on 64 samples\n",
        "- Validates suggested learning rate\n",
        "- Applies safety cap (5e-4 max)\n",
        "- Uses fallback (1e-5) for low confidence\n",
        "- Duration: ~2-3 minutes\n",
        "\n",
        "**What to check:**\n",
        "- LR Finder runs successfully\n",
        "- Safety validation applies 5e-4 cap if needed\n",
        "- Falls back to 1e-5 for low confidence/divergent curves\n",
        "- Cache is saved for future runs\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ 3. Full Training with Safety Features (Cell 29)\n",
        "\n",
        "Run full training with LR Finder and all safety features enabled:\n",
        "\n",
        "**What Cell 29 does:**\n",
        "- Auto-detects optimal learning rate with LR Finder\n",
        "- Caches LR for 168 hours (skip on reruns)\n",
        "- Enables triple weighting (sampler + class weights + focal loss)\n",
        "- Auto-adjusts weight multiplier (1.5 ‚Üí 1.2) to prevent overcorrection\n",
        "- Saves enhanced metadata (seed, git commit, LR analysis)\n",
        "- Uses adaptive GPU configuration\n",
        "\n",
        "**Features enabled:**\n",
        "- ‚úÖ LR Finder with safety validation\n",
        "- ‚úÖ LR Caching (168-hour expiry)\n",
        "- ‚úÖ Triple weighting auto-adjustment\n",
        "- ‚úÖ Enhanced checkpoint metadata\n",
        "\n",
        "**Notes:**\n",
        "- First run: LR Finder takes 5-10 min, then cached\n",
        "- Subsequent runs: Uses cache (instant)\n",
        "- To force recompute: Add `--force-find-lr` flag\n",
        "- To change cache expiry: Add `--lr-cache-max-age 336` (hours)\n",
        "\n",
        "---\n",
        "\n",
        "### üìä 4. Inspect Enhanced Metadata (Cell 30)\n",
        "\n",
        "After training, inspect the enhanced checkpoint metadata:\n",
        "\n",
        "**What Cell 30 shows:**\n",
        "- Training configuration (seed, git commit, timestamp)\n",
        "- LR Finder analysis (suggested LR, confidence, fallback status)\n",
        "- Triple weighting adjustments (original vs adjusted multiplier)\n",
        "- Training results (best epoch, F1, accuracy)\n",
        "\n",
        "**Run this after Cell 29 completes to see all v1.7 metadata!**\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ New CLI Flags Reference\n",
        "\n",
        "**LR Finder Flags:**\n",
        "- `--find-lr`: Enable LR Finder (auto-detects optimal learning rate)\n",
        "- `--force-find-lr`: Force LR Finder to run even if cache exists\n",
        "- `--lr-cache-max-age HOURS`: Cache expiry in hours (default: 168 = 7 days)\n",
        "\n",
        "**Weighting Flags (Triple Weighting):**\n",
        "- `--use-weighted-sampler`: Enable WeightedRandomSampler\n",
        "- `--weight-multiplier FLOAT`: Class weight multiplier (default: 1.0)\n",
        "- `--focal-loss`: Enable focal loss\n",
        "- When all 3 enabled: Auto-adjusts multiplier by 20% (e.g., 1.5 ‚Üí 1.2)\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "```bash\n",
        "# LR Finder only (no weighting)\n",
        "--find-lr\n",
        "\n",
        "# LR Finder + cache for 24 hours\n",
        "--find-lr --lr-cache-max-age 24\n",
        "\n",
        "# LR Finder + force recompute (ignore cache)\n",
        "--find-lr --force-find-lr\n",
        "\n",
        "# Triple weighting with auto-adjustment\n",
        "--use-weighted-sampler --weight-multiplier 1.5 --focal-loss\n",
        "# (Auto-adjusts: 1.5 ‚Üí 1.2, logs original value)\n",
        "\n",
        "# Full v1.7 safety features (used in Cell 29)\n",
        "--find-lr \\\n",
        "--use-weighted-sampler \\\n",
        "--weight-multiplier 1.5 \\\n",
        "--focal-loss\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ Documentation\n",
        "\n",
        "For complete details, see:\n",
        "- **LR Finder Safety:** `docs/TRAINING_QUICK_START.md` (Section: LR Finder Safety & Caching)\n",
        "- **Triple Weighting:** `docs/TRAINING_QUICK_START.md` (Section: Triple Weighting Auto-Adjustment)\n",
        "- **Ablation Testing:** `docs/TRAINING_QUICK_START.md` (Section: Ablation Testing)\n",
        "- **Unit Tests:** `tests/test_lr_finder.py`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Important Notes\n",
        "\n",
        "1. **Backward Compatibility:** All existing cells (1-25) work unchanged. v1.7 features are opt-in.\n",
        "2. **Cache Location:** LR Finder cache is saved in `~/.cache/streamguard/lr_finder/`\n",
        "3. **Ablation Testing:** To run ablation tests (7 weighting combinations), see `training/test_ablations.py` - run manually outside notebook\n",
        "4. **Default Behavior:** Without `--find-lr`, training uses the `--lr` value (default: 2e-5)\n",
        "5. **LR Finder Duration:** First run takes 5-10 min, subsequent runs use cache (instant)\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ Quick Start with v1.7\n",
        "\n",
        "1. **Test the features:** Run Cell 27 (unit tests) and Cell 28 (LR Finder quick test)\n",
        "2. **Full training:** Run Cell 29 (includes all safety features)\n",
        "3. **Inspect results:** Run Cell 30 (view enhanced metadata)\n",
        "4. **Continue existing workflow:** All original cells (7, 9, 11) still work!\n",
        "\n",
        "**You're all set! v1.7 features are ready to use with one-click cells.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY8F1pbQ_DzJ"
      },
      "source": [
        "---\n",
        "## Training Complete! üéâ\n",
        "\n",
        "Your models are now saved in Google Drive at:\n",
        "- `My Drive/streamguard/models/transformer_phase1/`\n",
        "- `My Drive/streamguard/models/gnn_phase1/`\n",
        "- `My Drive/streamguard/models/fusion_phase1/`\n",
        "\n",
        "**Critical Fixes Applied:**\n",
        "- ‚úÖ Runtime PyTorch/CUDA detection\n",
        "- ‚úÖ Robust tree-sitter with fallback\n",
        "- ‚úÖ Version compatibility validation\n",
        "\n",
        "**Next Steps:**\n",
        "1. Download models from Google Drive\n",
        "2. Deploy to production (see deployment guide)\n",
        "3. Optional: Run Phase 2 with collector data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üÜï v1.7 Safety Features - Instructions\n",
        "\n",
        "**NEW in v1.7:** Advanced safety features are now available! These are **OPTIONAL** and fully backward compatible.\n",
        "\n",
        "### üß™ 1. Run Unit Tests\n",
        "\n",
        "Verify all safety features are working correctly:\n",
        "\n",
        "```python\n",
        "# Cell: Run unit tests for LR Finder safety features\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "!python -m pytest tests/test_lr_finder.py -v\n",
        "```\n",
        "\n",
        "**Expected:** All 14 tests should PASS (cache, LR curve analysis, validation, integration)\n",
        "\n",
        "---\n",
        "\n",
        "### üîç 2. Test LR Finder Safety\n",
        "\n",
        "Test the LR Finder with safety validation on a small subset:\n",
        "\n",
        "```python\n",
        "# Cell: Test LR Finder safety validation\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"Testing LR Finder with safety validation...\")\n",
        "print(\"This will run LR Finder on 64 samples and validate the suggested LR\")\n",
        "print(\"Duration: ~2-3 minutes\\n\")\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --quick-test \\\n",
        "  --find-lr \\\n",
        "  --epochs 5 \\\n",
        "  --batch-size 16 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n‚úÖ LR Finder test complete!\")\n",
        "print(\"\\nüìã Check the output for:\")\n",
        "print(\"  ‚Ä¢ LR Finder curve analysis (confidence: high/medium/low)\")\n",
        "print(\"  ‚Ä¢ Safety validation (cap applied? fallback used?)\")\n",
        "print(\"  ‚Ä¢ Suggested LR and final used LR\")\n",
        "```\n",
        "\n",
        "**What to check:**\n",
        "- LR Finder runs successfully\n",
        "- Safety validation applies 5e-4 cap if needed\n",
        "- Falls back to 1e-5 for low confidence/divergent curves\n",
        "- Cache is saved for future runs\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ 3. Full Training with Safety Features\n",
        "\n",
        "Run full training with LR Finder and all safety features enabled:\n",
        "\n",
        "```python\n",
        "# Cell: Full training with v1.7 safety features\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "# Load adaptive configuration\n",
        "config_path = Path('/tmp/gpu_training_config.json')\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "    t_config = config_data['config']['transformer']\n",
        "    config_tier = config_data['tier']\n",
        "else:\n",
        "    t_config = {'epochs': 10, 'batch_size': 32, 'max_seq_len': 512, 'patience': 2}\n",
        "    config_tier = 'OPTIMIZED (Default)'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRANSFORMER TRAINING WITH v1.7 SAFETY FEATURES\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Configuration: {config_tier}\")\n",
        "print(\"\\nSafety Features Enabled:\")\n",
        "print(\"  ‚úÖ LR Finder with safety validation\")\n",
        "print(\"  ‚úÖ LR Caching (168-hour expiry)\")\n",
        "print(\"  ‚úÖ Triple weighting auto-adjustment\")\n",
        "print(\"  ‚úÖ Enhanced checkpoint metadata\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --test-data data/processed/codexglue/test.jsonl \\\n",
        "  --output-dir /content/models/transformer_v1.7 \\\n",
        "  --find-lr \\\n",
        "  --use-weighted-sampler \\\n",
        "  --weight-multiplier 1.5 \\\n",
        "  --focal-loss \\\n",
        "  --epochs {t_config['epochs']} \\\n",
        "  --batch-size {t_config['batch_size']} \\\n",
        "  --max-seq-len {t_config['max_seq_len']} \\\n",
        "  --weight-decay 0.01 \\\n",
        "  --warmup-ratio 0.1 \\\n",
        "  --dropout 0.1 \\\n",
        "  --early-stopping-patience {t_config['patience']} \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n‚úÖ Training complete with v1.7 safety features!\")\n",
        "```\n",
        "\n",
        "**Notes:**\n",
        "- `--find-lr`: Runs LR Finder (5-10 min), caches result for 168 hours\n",
        "- Triple weighting (sampler + weights + focal) auto-adjusts multiplier 1.5 ‚Üí 1.2\n",
        "- Enhanced metadata saved in checkpoint\n",
        "- To force LR Finder to run again: add `--force-find-lr`\n",
        "- To change cache expiry: add `--lr-cache-max-age 336` (hours)\n",
        "\n",
        "---\n",
        "\n",
        "### üìä 4. Inspect Enhanced Metadata\n",
        "\n",
        "After training, inspect the enhanced checkpoint metadata:\n",
        "\n",
        "```python\n",
        "# Cell: Inspect enhanced checkpoint metadata\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "checkpoint_path = Path('/content/models/transformer_v1.7/checkpoints/best_model.pt')\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ENHANCED CHECKPOINT METADATA (v1.7)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    metadata = checkpoint.get('metadata', {})\n",
        "    \n",
        "    print(\"\\nüìã Training Configuration:\")\n",
        "    print(f\"  Seed: {metadata.get('seed', 'N/A')}\")\n",
        "    print(f\"  Git Commit: {metadata.get('git_commit', 'N/A')}\")\n",
        "    print(f\"  Timestamp: {metadata.get('timestamp', 'N/A')}\")\n",
        "    \n",
        "    if 'lr_finder_analysis' in metadata:\n",
        "        lr_analysis = metadata['lr_finder_analysis']\n",
        "        print(\"\\nüîç LR Finder Analysis:\")\n",
        "        print(f\"  Suggested LR: {lr_analysis.get('suggested_lr', 'N/A')}\")\n",
        "        print(f\"  Confidence: {lr_analysis.get('confidence', 'N/A')}\")\n",
        "        print(f\"  Used Fallback: {lr_analysis.get('used_fallback', 'N/A')}\")\n",
        "        print(f\"  Note: {lr_analysis.get('note', 'N/A')}\")\n",
        "    \n",
        "    if 'triple_weighting' in metadata:\n",
        "        tw = metadata['triple_weighting']\n",
        "        print(\"\\n‚öñÔ∏è  Triple Weighting Auto-Adjustment:\")\n",
        "        print(f\"  Enabled: {tw.get('enabled', 'N/A')}\")\n",
        "        print(f\"  Original Multiplier: {tw.get('original_mult', 'N/A')}\")\n",
        "        print(f\"  Adjusted Multiplier: {tw.get('adjusted_mult', 'N/A')}\")\n",
        "    \n",
        "    print(\"\\nüìà Training Results:\")\n",
        "    print(f\"  Best Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
        "    print(f\"  Val F1: {checkpoint.get('val_f1', 'N/A'):.4f}\")\n",
        "    print(f\"  Val Accuracy: {checkpoint.get('val_acc', 'N/A'):.4f}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"‚ùå Checkpoint not found. Run training first.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ New CLI Flags Reference\n",
        "\n",
        "**LR Finder Flags:**\n",
        "- `--find-lr`: Enable LR Finder (auto-detects optimal learning rate)\n",
        "- `--force-find-lr`: Force LR Finder to run even if cache exists\n",
        "- `--lr-cache-max-age HOURS`: Cache expiry in hours (default: 168 = 7 days)\n",
        "\n",
        "**Weighting Flags (Triple Weighting):**\n",
        "- `--use-weighted-sampler`: Enable WeightedRandomSampler\n",
        "- `--weight-multiplier FLOAT`: Class weight multiplier (default: 1.0)\n",
        "- `--focal-loss`: Enable focal loss\n",
        "- When all 3 enabled: Auto-adjusts multiplier by 20% (e.g., 1.5 ‚Üí 1.2)\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "```bash\n",
        "# LR Finder only (no weighting)\n",
        "--find-lr\n",
        "\n",
        "# LR Finder + cache for 24 hours\n",
        "--find-lr --lr-cache-max-age 24\n",
        "\n",
        "# LR Finder + force recompute (ignore cache)\n",
        "--find-lr --force-find-lr\n",
        "\n",
        "# Triple weighting with auto-adjustment\n",
        "--use-weighted-sampler --weight-multiplier 1.5 --focal-loss\n",
        "# (Auto-adjusts: 1.5 ‚Üí 1.2, logs original value)\n",
        "\n",
        "# Full v1.7 safety features\n",
        "--find-lr \\\n",
        "--use-weighted-sampler \\\n",
        "--weight-multiplier 1.5 \\\n",
        "--focal-loss\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ Documentation\n",
        "\n",
        "For complete details, see:\n",
        "- **LR Finder Safety:** `docs/TRAINING_QUICK_START.md` (Section: LR Finder Safety & Caching)\n",
        "- **Triple Weighting:** `docs/TRAINING_QUICK_START.md` (Section: Triple Weighting Auto-Adjustment)\n",
        "- **Ablation Testing:** `docs/TRAINING_QUICK_START.md` (Section: Ablation Testing)\n",
        "- **Unit Tests:** `tests/test_lr_finder.py`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Important Notes\n",
        "\n",
        "1. **Backward Compatibility:** All existing cells work unchanged. v1.7 features are opt-in via flags.\n",
        "2. **Cache Location:** LR Finder cache is saved in `~/.cache/streamguard/lr_finder/`\n",
        "3. **Ablation Testing:** To run ablation tests (7 weighting combinations), see `training/test_ablations.py` - run manually outside notebook\n",
        "4. **Default Behavior:** Without `--find-lr`, training uses the `--lr` value (default: 2e-5)\n",
        "5. **LR Finder Duration:** First run takes 5-10 min, subsequent runs use cache (instant)\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ You're all set! Use the cells above to try v1.7 safety features, or continue with existing workflow.**"
      ],
      "metadata": {
        "id": "ufOuVOEFL8Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 27: Unit Tests for v1.7 Safety Features\n",
        "# Verifies all LR Finder safety features are working correctly (14 tests)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RUNNING UNIT TESTS FOR v1.7 SAFETY FEATURES\")\n",
        "print(\"=\"*70)\n",
        "print(\"Testing: LR cache, curve analysis, validation, integration\")\n",
        "print(\"Expected: All 14 tests should PASS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python -m pytest tests/test_lr_finder.py -v\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ Unit tests complete!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "Px3Z8fr4L8Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 28: LR Finder Safety Quick Test\n",
        "# Quick test of LR Finder with safety validation on small subset (2-3 min)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/streamguard')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LR FINDER SAFETY VALIDATION TEST\")\n",
        "print(\"=\"*70)\n",
        "print(\"Testing LR Finder with safety validation on 64 samples\")\n",
        "print(\"Duration: ~2-3 minutes\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python training/train_transformer.py \\\n",
        "  --train-data data/processed/codexglue/train.jsonl \\\n",
        "  --val-data data/processed/codexglue/valid.jsonl \\\n",
        "  --quick-test \\\n",
        "  --find-lr \\\n",
        "  --epochs 5 \\\n",
        "  --batch-size 16 \\\n",
        "  --seed 42\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ LR Finder test complete!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìã Check the output above for:\")\n",
        "print(\"  ‚Ä¢ LR Finder curve analysis (confidence: high/medium/low)\")\n",
        "print(\"  ‚Ä¢ Safety validation (cap applied? fallback used?)\")\n",
        "print(\"  ‚Ä¢ Suggested LR and final used LR\")\n",
        "print(\"  ‚Ä¢ Cache saved for future runs\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "WO8H3B_iL8Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0dd79a2-dd1f-4fc4-92c0-29c13fef09b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LR FINDER SAFETY VALIDATION TEST\n",
            "======================================================================\n",
            "Testing LR Finder with safety validation on 64 samples\n",
            "Duration: ~2-3 minutes\n",
            "======================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "[!] boto3 not available. S3 checkpointing disabled.\n",
            "[+] Random seed set to 42\n",
            "[+] Using device: cuda\n",
            "[+] Experiment config saved: models/transformer/exp_config.json\n",
            "[*] Loading tokenizer: microsoft/codebert-base\n",
            "[*] Loading dataset from data/processed/codexglue/train.jsonl\n",
            "[+] Loaded 21854 samples\n",
            "    Vulnerable: 10018 (45.8%)\n",
            "    Safe: 11836 (54.2%)\n",
            "[*] Loading dataset from data/processed/codexglue/valid.jsonl\n",
            "[+] Loaded 2732 samples\n",
            "    Vulnerable: 1187 (43.4%)\n",
            "    Safe: 1545 (56.6%)\n",
            "[*] Quick test mode: using 500 train, 100 val samples\n",
            "\n",
            "[*] Calculating class weights for balanced training...\n",
            "[*] Quick test mode: class weights (1.0 vs 1.6593)\n",
            "    Base inverse-freq=1.1062, multiplier=1.5\n",
            "    Class distribution: Safe=274, Vulnerable=226\n",
            "    Class weights: Safe=1.0000, Vulnerable=1.6593\n",
            "\n",
            "[*] Quick test mode: dropout set to 0.0 (disabled)\n",
            "[*] Initializing model: microsoft/codebert-base\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "Downloading pytorch_model.bin: 100% 499M/499M [00:01<00:00, 280MB/s]\n",
            "[*] Quick test mode: using default LR=1.50e-05\n",
            "[*] Final LR: 1.50e-05\n",
            "[*] Discriminative LR:\n",
            "    Encoder (76 params): LR=1.50e-06, WD=0.01\n",
            "    Head (2 params): LR=1.50e-05, WD=0.01\n",
            "    No-decay (125 params): LR=1.50e-05, WD=0.0\n",
            "    Total trainable params: 203\n",
            "[*] Scheduler config:\n",
            "    Total steps: 160\n",
            "    Warmup steps: 16 (10.0%)\n",
            "    Warmup ratio: 0.1\n",
            "[+] Loss: CrossEntropyLoss with class_weights and label_smoothing=0.0\n",
            "[+] Mixed precision training disabled\n",
            "\n",
            "======================================================================\n",
            "RUNNING LR FINDER\n",
            "======================================================================\n",
            "\n",
            "[*] Running LR Finder...\n",
            "    Start LR: 1.00e-07\n",
            "    End LR: 1.00e+00\n",
            "    Iterations: 100\n",
            "    Iter 10/100: LR=4.27e-07, Loss=0.7029\n",
            "    Iter 20/100: LR=2.14e-06, Loss=0.6996\n",
            "    Iter 30/100: LR=1.07e-05, Loss=0.6969\n",
            "    Iter 40/100: LR=5.37e-05, Loss=0.6903\n",
            "    Iter 50/100: LR=2.69e-04, Loss=0.6985\n",
            "    Iter 60/100: LR=1.35e-03, Loss=0.7043\n",
            "    Iter 70/100: LR=6.76e-03, Loss=0.7280\n",
            "    Iter 80/100: LR=3.39e-02, Loss=0.7718\n",
            "    Iter 90/100: LR=1.70e-01, Loss=0.8846\n",
            "[!] Loss diverged at LR=2.34e-01, stopping early\n",
            "[+] LR Finder complete. Tested 92 learning rates\n",
            "\n",
            "[*] LR Finder Results:\n",
            "    Raw suggestion: 2.34e-01\n",
            "    Confidence: medium\n",
            "    Slope magnitude: 3.1224\n",
            "    SNR: 1.85\n",
            "    [!] Divergence detected after minimum\n",
            "    Final LR: 1.00e-05 (fallback_due_to_divergence_after_min)\n",
            "[!] WARNING: Used conservative fallback (1e-5) due to poor curve quality\n",
            "    Reasons: divergence_after_min\n",
            "[+] LR Finder results cached (key: ddbfb5fca280...)\n",
            "[+] LR Finder plot saved to: models/transformer/lr_finder_plot.png\n",
            "[*] Applying suggested LR: 1.00e-05\n",
            "[+] Optimizer and scheduler rebuilt with suggested LR\n",
            "======================================================================\n",
            "\n",
            "[!] S3 checkpointing disabled\n",
            "\n",
            "======================================================================\n",
            "TRAINING START\n",
            "======================================================================\n",
            "[*] Quick test mode: increased patience to 6 (3x normal)\n",
            "\n",
            "Epoch 1/5\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6902\n",
            "Val Loss: 0.7102\n",
            "Val Accuracy: 0.5400\n",
            "Val Precision: 0.5400\n",
            "Val Recall: 1.0000\n",
            "Val F1 (vulnerable): 0.7013\n",
            "Predictions: Vulnerable=100/54, Safe=0/46\n",
            "/content/streamguard/training/train_transformer.py:1380: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'timestamp': datetime.utcnow().isoformat(),\n",
            "[+] Saved checkpoint: models/transformer/checkpoints/checkpoint_epoch_1.pt\n",
            "[+] Saved best model: models/transformer/checkpoints/best_model.pt\n",
            "[+] New best model! F1: 0.7013\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6862\n",
            "Val Loss: 0.7165\n",
            "Val Accuracy: 0.5400\n",
            "Val Precision: 0.5400\n",
            "Val Recall: 1.0000\n",
            "Val F1 (vulnerable): 0.7013\n",
            "Predictions: Vulnerable=100/54, Safe=0/46\n",
            "[!] COLLAPSE SIGNAL: Zero safe predictions\n",
            "/content/streamguard/training/train_transformer.py:1380: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'timestamp': datetime.utcnow().isoformat(),\n",
            "[+] Saved checkpoint: models/transformer/checkpoints/checkpoint_epoch_2.pt\n",
            "[*] No improvement. Patience: 1/6\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 0.6811\n",
            "Val Loss: 0.7186\n",
            "Val Accuracy: 0.5400\n",
            "Val Precision: 0.5400\n",
            "Val Recall: 1.0000\n",
            "Val F1 (vulnerable): 0.7013\n",
            "Predictions: Vulnerable=100/54, Safe=0/46\n",
            "[!] COLLAPSE SIGNAL: Zero safe predictions\n",
            "/content/streamguard/training/train_transformer.py:1380: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'timestamp': datetime.utcnow().isoformat(),\n",
            "[+] Saved checkpoint: models/transformer/checkpoints/checkpoint_epoch_3.pt\n",
            "[*] No improvement. Patience: 2/6\n",
            "\n",
            "======================================================================\n",
            "[!] CRITICAL: Collapse detected for 2 consecutive epochs\n",
            "[!] Collapse history: [2, 3]\n",
            "\n",
            "[!] STOPPING TRAINING. Recommended fixes:\n",
            "    1. Add: --use-weighted-sampler\n",
            "    2. Try: --lr-override 2e-5\n",
            "    3. Increase: --weight-multiplier 2.0\n",
            "    4. Try: --focal-loss\n",
            "======================================================================\n",
            "\n",
            "[+] Saved diagnostic report: models/transformer/collapse_report.json\n",
            "\n",
            "======================================================================\n",
            "‚úÖ LR Finder test complete!\n",
            "======================================================================\n",
            "\n",
            "üìã Check the output above for:\n",
            "  ‚Ä¢ LR Finder curve analysis (confidence: high/medium/low)\n",
            "  ‚Ä¢ Safety validation (cap applied? fallback used?)\n",
            "  ‚Ä¢ Suggested LR and final used LR\n",
            "  ‚Ä¢ Cache saved for future runs\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}