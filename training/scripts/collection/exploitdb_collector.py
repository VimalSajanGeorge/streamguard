"""
Exploit Database (ExploitDB) Collector for StreamGuard.

Collects exploit code samples from ExploitDB, which contains 50,000+ exploits
with proof-of-concept code for various vulnerabilities.

Data Source: https://www.exploit-db.com/
CSV Database: https://gitlab.com/exploit-database/exploitdb
"""

import json
import csv
import requests
import re
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from collections import defaultdict
import io

import sys
sys.path.insert(0, str(Path(__file__).parent))

from base_collector import BaseCollector
from checkpoint_manager import CheckpointManager


class ExploitDBCollector(BaseCollector):
    """Collector for Exploit Database (ExploitDB) exploits."""

    # ExploitDB CSV database (updated daily)
    EXPLOITDB_CSV_URL = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"

    # ExploitDB raw exploit URL templates (try multiple formats)
    EXPLOIT_RAW_URLS = [
        "https://gitlab.com/exploit-database/exploitdb/-/raw/main/{file_path}",
        "https://raw.githubusercontent.com/offensive-security/exploitdb/master/{file_path}",
    ]

    # Supported platforms (for filtering)
    PLATFORMS = [
        "linux",
        "windows",
        "multiple",
        "unix",
        "bsd",
        "osx",
        "android",
        "ios"
    ]

    # Code file extensions we want
    CODE_EXTENSIONS = [
        ".py", ".c", ".cpp", ".java", ".js", ".rb", ".php",
        ".pl", ".sh", ".bat", ".ps1", ".go", ".rs"
    ]

    def __init__(self, output_dir: str = "data/raw/exploitdb", cache_enabled: bool = True, resume: bool = False):
        """
        Initialize ExploitDB Collector.

        Args:
            output_dir: Directory to save collected data
            cache_enabled: Whether to enable caching
            resume: Whether to resume from checkpoint
        """
        super().__init__(output_dir, cache_enabled)

        # Collection statistics
        self.stats = {
            "total_exploits": 0,
            "by_platform": defaultdict(int),
            "by_type": defaultdict(int),
            "with_code": 0,
            "code_languages": defaultdict(int)
        }

        # Checkpoint manager
        self.checkpoint_manager = CheckpointManager()
        self.resume = resume
        self.checkpoint_interval = 300  # Save checkpoint every 5 minutes
        self.last_checkpoint_time = None

    def collect(self) -> List[Dict]:
        """
        Main collection method - collects ExploitDB exploits.

        Returns:
            List of collected exploit samples
        """
        print("Starting ExploitDB Collection")
        print(f"Target: 10,000 exploit code samples")

        return self.collect_all_exploits(target_samples=10000)

    def collect_all_exploits(self, target_samples: int = 10000) -> List[Dict]:
        """
        Collect exploits from ExploitDB database.

        Args:
            target_samples: Target number of samples to collect

        Returns:
            List of collected exploit samples
        """
        all_samples = []
        start_index = 0

        # Check for existing checkpoint
        if self.resume:
            checkpoint = self.checkpoint_manager.load_checkpoint("exploitdb")
            if checkpoint:
                print("\n[+] Found existing checkpoint, resuming collection...")
                all_samples = checkpoint.get("samples", [])
                start_index = checkpoint.get("state", {}).get("last_processed_index", 0) + 1
                self.samples_collected = len(all_samples)
                print(f"[+] Resuming with {len(all_samples)} samples already collected")
                print(f"[+] Starting from index {start_index}")
            else:
                print("\n[!] No checkpoint found, starting fresh collection...")

        print("\n" + "="*60)
        print("Fetching ExploitDB CSV database...")
        print("="*60)

        # Fetch and parse CSV database
        exploit_entries = self._fetch_exploit_database()

        if not exploit_entries:
            print("ERROR: Failed to fetch exploit database")
            return []

        print(f"Found {len(exploit_entries)} exploit entries in database")

        # Filter for code-based exploits
        code_exploits = [
            e for e in exploit_entries
            if any(e['file_path'].endswith(ext) for ext in self.CODE_EXTENSIONS)
        ]

        print(f"Filtered to {len(code_exploits)} code-based exploits")

        # Process exploits
        successful = 0
        failed = 0
        import time
        self.last_checkpoint_time = time.time()

        for idx, exploit_entry in enumerate(code_exploits[start_index:start_index + target_samples], start=start_index):
            if idx % 100 == 0 and idx > 0:
                success_rate = (successful / idx) * 100 if idx > 0 else 0
                print(f"Processing {idx}/{min(len(code_exploits), target_samples)} - Success: {successful} ({success_rate:.1f}%), Failed: {failed}")

            try:
                sample = self._process_exploit_entry(exploit_entry)

                if sample:
                    all_samples.append(sample)
                    self.samples_collected += 1
                    successful += 1
                else:
                    failed += 1

                # Save checkpoint periodically (every 5 minutes)
                current_time = time.time()
                if current_time - self.last_checkpoint_time >= self.checkpoint_interval:
                    self._save_checkpoint(all_samples, idx, target_samples)
                    self.last_checkpoint_time = current_time

                # Save intermediate results
                if len(all_samples) % 2000 == 0:
                    self._save_intermediate_results(all_samples)

                # Rate limiting
                if idx % 10 == 0:
                    time.sleep(0.5)  # Be nice to GitLab

            except Exception as e:
                error_msg = f"Error processing exploit {exploit_entry.get('id')}: {str(e)}"
                self.log_error(error_msg, {"exploit_id": exploit_entry.get('id')})
                failed += 1

        # Final success rate
        total_processed = successful + failed
        success_rate = (successful / total_processed) * 100 if total_processed > 0 else 0
        print(f"\nFinal: {successful}/{total_processed} successful ({success_rate:.1f}%)")

        # Deduplicate samples
        unique_samples = self.deduplicate_samples(all_samples, key="exploit_id")
        print(f"Total unique samples collected: {len(unique_samples)}")

        # Save final results
        if len(unique_samples) > 0:
            output_file = self.save_samples(unique_samples, "exploitdb_exploits.jsonl")
            print(f"\nSaved to: {output_file}")
        else:
            print("\nWARNING: No samples to save!")

        # Print statistics
        self._print_statistics()

        # Delete checkpoint after successful completion
        if self.checkpoint_manager.checkpoint_exists("exploitdb"):
            self.checkpoint_manager.delete_checkpoint("exploitdb")
            print("\n[+] Checkpoint deleted (collection completed)")

        return unique_samples

    def _fetch_exploit_database(self) -> List[Dict]:
        """
        Fetch the ExploitDB CSV database.

        Returns:
            List of exploit entries
        """
        # Check cache
        cache_key = self.make_cache_key("exploitdb_csv", datetime.now().strftime("%Y%m%d"))
        cached_result = self.load_cache(cache_key)
        if cached_result:
            print("Using cached database")
            return cached_result

        try:
            print(f"Downloading from {self.EXPLOITDB_CSV_URL}...")
            response = requests.get(self.EXPLOITDB_CSV_URL, timeout=60)
            response.raise_for_status()

            # Parse CSV
            csv_content = response.text
            reader = csv.DictReader(io.StringIO(csv_content))

            exploits = []
            for row in reader:
                exploit = {
                    "id": row.get("id", ""),
                    "file_path": row.get("file", ""),
                    "description": row.get("description", ""),
                    "date": row.get("date", ""),
                    "author": row.get("author", ""),
                    "platform": row.get("platform", ""),
                    "type": row.get("type", ""),
                    "port": row.get("port", ""),
                    "codes": row.get("codes", "")  # CVE codes
                }
                exploits.append(exploit)

            print(f"Parsed {len(exploits)} exploits from CSV")

            # Save to cache
            self.save_cache(cache_key, exploits)

            return exploits

        except Exception as e:
            error_msg = f"Failed to fetch exploit database: {str(e)}"
            print(f"ERROR: {error_msg}")
            self.log_error(error_msg)
            return []

    def _fetch_exploit_code(self, file_path: str) -> Optional[str]:
        """
        Fetch the actual exploit code from multiple sources with retry logic.

        Args:
            file_path: Path to exploit file

        Returns:
            Exploit code or None
        """
        # Check cache
        cache_key = self.make_cache_key("exploit_code", file_path)
        cached_code = self.load_cache(cache_key)
        if cached_code:
            return cached_code.get("code")

        # Try multiple URL formats
        for url_template in self.EXPLOIT_RAW_URLS:
            try:
                url = url_template.format(file_path=file_path)
                response = requests.get(url, timeout=30)
                response.raise_for_status()

                code = response.text

                # Validate code
                if not self.validate_code(code, min_length=50):
                    continue  # Try next URL

                # Save to cache (include which URL worked)
                self.save_cache(cache_key, {"code": code, "url_used": url_template})

                return code

            except requests.exceptions.HTTPError as e:
                # 404 is expected for some files, try next URL
                if e.response.status_code == 404:
                    continue
                else:
                    self.log_error(f"HTTP {e.response.status_code} for {file_path}: {str(e)}")
                    continue

            except Exception as e:
                # Try next URL on any error
                continue

        # All URLs failed
        self.log_error(f"Failed to fetch code for {file_path} from all sources")
        return None

    def _process_exploit_entry(self, exploit_entry: Dict) -> Optional[Dict]:
        """
        Process a single exploit entry and fetch its code.

        Args:
            exploit_entry: Exploit metadata from CSV

        Returns:
            Processed sample or None
        """
        exploit_id = exploit_entry.get("id")
        file_path = exploit_entry.get("file_path")
        description = exploit_entry.get("description", "")
        date = exploit_entry.get("date")
        author = exploit_entry.get("author", "")
        platform = exploit_entry.get("platform", "").lower()
        exploit_type = exploit_entry.get("type", "")
        cve_codes = exploit_entry.get("codes", "")

        # Fetch exploit code
        exploit_code = self._fetch_exploit_code(file_path)

        if not exploit_code:
            return None

        # Extract programming language from file extension
        language = self._get_language_from_path(file_path)

        # Generate "fixed" version (add security comments)
        fixed_code = self._generate_fixed_version(exploit_code, description)

        # Update statistics
        self.stats["total_exploits"] += 1
        self.stats["by_platform"][platform] += 1
        self.stats["by_type"][exploit_type] += 1
        self.stats["with_code"] += 1
        self.stats["code_languages"][language] += 1

        # Extract vulnerability type
        vuln_type = self.extract_vulnerability_type(description)

        # Parse CVE codes
        cve_list = self._parse_cve_codes(cve_codes)

        # Create sample
        sample = {
            "exploit_id": f"EDB-{exploit_id}",
            "description": description,
            "vulnerable_code": exploit_code,
            "fixed_code": fixed_code,
            "ecosystem": platform,
            "severity": "HIGH",  # Exploits are by definition high severity
            "published_at": date,
            "source": "exploitdb",
            "metadata": {
                "author": author,
                "platform": platform,
                "exploit_type": exploit_type,
                "vulnerability_type": vuln_type,
                "language": language,
                "cve_codes": cve_list,
                "file_path": file_path
            }
        }

        return sample

    def _get_language_from_path(self, file_path: str) -> str:
        """Get programming language from file extension."""
        ext_map = {
            ".py": "Python",
            ".c": "C",
            ".cpp": "C++",
            ".java": "Java",
            ".js": "JavaScript",
            ".rb": "Ruby",
            ".php": "PHP",
            ".pl": "Perl",
            ".sh": "Bash",
            ".bat": "Batch",
            ".ps1": "PowerShell",
            ".go": "Go",
            ".rs": "Rust"
        }

        for ext, lang in ext_map.items():
            if file_path.endswith(ext):
                return lang

        return "Unknown"

    def _parse_cve_codes(self, codes_str: str) -> List[str]:
        """Parse CVE codes from string."""
        if not codes_str:
            return []

        # Extract CVE-YYYY-NNNNN pattern
        cve_pattern = r'CVE-\d{4}-\d{4,}'
        cves = re.findall(cve_pattern, codes_str)

        return cves

    def _generate_fixed_version(self, exploit_code: str, description: str) -> str:
        """
        Generate a "fixed" version by adding security comments.

        This is a synthetic fix - adds comments about what's wrong.
        Real fixes would require deeper analysis.

        Args:
            exploit_code: Original exploit code
            description: Exploit description

        Returns:
            Modified code with security comments
        """
        header_comment = f"""
# ==================================================================
# SECURITY WARNING: This code demonstrates a vulnerability
# ==================================================================
# Original: Exploit code from ExploitDB
# Vulnerability: {description[:100]}...
#
# This code should NOT be used in production. Key issues:
# - Lacks input validation
# - No security checks
# - Vulnerable to attacks
# ==================================================================

"""
        return header_comment + exploit_code

    def _save_checkpoint(self, samples: List[Dict], last_processed_index: int, target_samples: int):
        """Save checkpoint for resuming collection."""
        try:
            state = {
                "last_processed_index": last_processed_index,
                "target_samples": target_samples
            }
            checkpoint_file = self.checkpoint_manager.save_checkpoint("exploitdb", state, samples)
            print(f"[+] Checkpoint saved: {checkpoint_file}")
        except Exception as e:
            print(f"[!] Warning: Failed to save checkpoint: {str(e)}")

    def _save_intermediate_results(self, samples: List[Dict]):
        """Save intermediate results."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"exploitdb_intermediate_{timestamp}.jsonl"
        self.save_samples(samples, filename)
        print(f"\nSaved intermediate results: {filename}")

    def _print_statistics(self):
        """Print collection statistics."""
        print("\n" + "="*60)
        print("COLLECTION STATISTICS")
        print("="*60)
        print(f"Total Exploits: {self.stats['total_exploits']}")
        print(f"With Code: {self.stats['with_code']}")

        print("\nBy Platform:")
        for platform, count in sorted(self.stats['by_platform'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {platform}: {count}")

        print("\nBy Language:")
        for lang, count in sorted(self.stats['code_languages'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {lang}: {count}")

        print("\nBy Type:")
        for exploit_type, count in sorted(self.stats['by_type'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {exploit_type}: {count}")

        print("\nErrors:", len(self.errors))
        if self.errors:
            print("\nLast 5 errors:")
            for error in self.errors[-5:]:
                print(f"  - {error['error'][:80]}...")


def main():
    """Main entry point for the collector."""
    import argparse

    parser = argparse.ArgumentParser(
        description="ExploitDB Collector"
    )
    parser.add_argument(
        "--output-dir",
        default="data/raw/exploitdb",
        help="Output directory for collected data"
    )
    parser.add_argument(
        "--target-samples",
        type=int,
        default=10000,
        help="Target number of samples to collect"
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Disable caching"
    )
    parser.add_argument(
        "--platform",
        choices=ExploitDBCollector.PLATFORMS,
        help="Filter by platform"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from checkpoint if available"
    )

    args = parser.parse_args()

    # Initialize collector
    collector = ExploitDBCollector(
        output_dir=args.output_dir,
        cache_enabled=not args.no_cache,
        resume=args.resume
    )

    # Collect data
    try:
        samples = collector.collect_all_exploits(target_samples=args.target_samples)
        print(f"\nCollection complete! Collected {len(samples)} samples.")

    except KeyboardInterrupt:
        print("\n\nCollection interrupted by user.")
        print(f"Collected {collector.samples_collected} samples before interruption.")

    except Exception as e:
        print(f"\n\nCollection failed with error: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # Print final statistics
        stats = collector.get_stats()
        print(f"\nFinal statistics: {json.dumps(stats, indent=2)}")


if __name__ == "__main__":
    main()
